{"traceEvents": [{"ph": "M", "pid": 908864, "tid": 908864, "name": "process_name", "args": {"name": "Process-1"}}, {"ph": "M", "pid": 908864, "tid": 908865, "name": "thread_name", "args": {"name": "Thread-1 (buffered_writer)"}}, {"ph": "M", "pid": 908864, "tid": 908864, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 908864, "tid": 908864, "ts": 183992516929.017, "ph": "X", "dur": 0.80151810807513, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516927.194, "ph": "X", "dur": 3.5066417228286935, "name": "_GeneratorContextManagerBase.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:108)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516924.99, "ph": "X", "dur": 6.011385810563476, "name": "contextmanager.<locals>.helper (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:303)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516939.607, "ph": "X", "dur": 0.6812903918638604, "name": "_RedirectStream.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:397)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516941.922, "ph": "X", "dur": 0.320607243230052, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516942.483, "ph": "X", "dur": 0.400759054037565, "name": "list.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516943.435, "ph": "X", "dur": 0.7914991317241908, "name": "builtins.setattr", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516940.83, "ph": "X", "dur": 3.5767745572852676, "name": "_RedirectStream.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:402)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516934.909, "ph": "X", "dur": 9.768501942165647, "name": "capture_stdout (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/capture.py:16)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516934.287, "ph": "X", "dur": 10.51992516848608, "name": "builtins.next", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516932.023, "ph": "X", "dur": 12.834308705553019, "name": "_GeneratorContextManager.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:136)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516950.468, "ph": "X", "dur": 1.2122961384636342, "name": "math.isnan", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516952.161, "ph": "X", "dur": 0.37070212498474764, "name": "_asyncio.get_running_loop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516955.347, "ph": "X", "dur": 0.1202277162112695, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516953.063, "ph": "X", "dur": 2.8654272363685895, "name": "BaseEventLoop.create_future (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:458)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516957.982, "ph": "X", "dur": 1.5529413343955643, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516957.682, "ph": "X", "dur": 1.913624483029373, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516962.571, "ph": "X", "dur": 0.11020873986033036, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516966.038, "ph": "X", "dur": 0.7113473209166778, "name": "_contextvars.copy_context", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516968.652, "ph": "X", "dur": 0.080151810807513, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516965.446, "ph": "X", "dur": 3.8773438478134414, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516963.823, "ph": "X", "dur": 5.8410632125975095, "name": "TimerHandle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:113)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516970.406, "ph": "X", "dur": 0.5710816520035301, "name": "_heapq.heappush", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516962.21, "ph": "X", "dur": 9.007059739494272, "name": "BaseEventLoop.call_at (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:801)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516957.081, "ph": "X", "dur": 14.54755366156361, "name": "BaseEventLoop.call_later (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:777)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516949.216, "ph": "X", "dur": 23.27408206323159, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516947.362, "ph": "X", "dur": 25.227782451664716, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516946.681, "ph": "X", "dur": 26.079395441494544, "name": "gato_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:6)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516976.457, "ph": "X", "dur": 0.6913093682147996, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516977.8, "ph": "X", "dur": 1.3826187364295992, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516977.72, "ph": "X", "dur": 1.5329033816936861, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516981.577, "ph": "X", "dur": 0.6612524391619823, "name": "math.ceil", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516982.779, "ph": "X", "dur": 0.13024669256220864, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517002.897, "ph": "X", "dur": 0.7714611790223127, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517002.416, "ph": "X", "dur": 1.5429223580446254, "name": "Condition._acquire_restore (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:315)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517005.372, "ph": "X", "dur": 0.21039850336972163, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517007.426, "ph": "X", "dur": 0.7614422026713735, "name": "_add_output_to_buffer (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:56)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517008.418, "ph": "X", "dur": 0.10018976350939125, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517009.76, "ph": "X", "dur": 0.38072110133568676, "name": "_can_merge_outputs (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:52)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517008.788, "ph": "X", "dur": 2.083947080995338, "name": "_add_output_to_buffer (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:56)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517011.193, "ph": "X", "dur": 0.14026566891314776, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517012.005, "ph": "X", "dur": 0.1703225979659651, "name": "_can_merge_outputs (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:52)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517011.704, "ph": "X", "dur": 1.0219355877957907, "name": "_add_output_to_buffer (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:56)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517012.886, "ph": "X", "dur": 0.10018976350939125, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517013.568, "ph": "X", "dur": 0.09017078715845213, "name": "_can_merge_outputs (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:52)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517013.337, "ph": "X", "dur": 0.5811006283544692, "name": "_add_output_to_buffer (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:56)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517015.05, "ph": "X", "dur": 1.9737383411350076, "name": "time.time", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517024.308, "ph": "X", "dur": 0.8315750371279473, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517024.077, "ph": "X", "dur": 1.3024669256220862, "name": "Condition._is_owned (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:318)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517025.61, "ph": "X", "dur": 0.641214486460104, "name": "_thread.allocate_lock", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517026.512, "ph": "X", "dur": 0.3105882668791129, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517027.063, "ph": "X", "dur": 0.2003795270187825, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517027.875, "ph": "X", "dur": 0.1703225979659651, "name": "_thread.lock.release", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517027.754, "ph": "X", "dur": 0.4308159830903824, "name": "Condition._release_save (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:312)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517028.636, "ph": "X", "dur": 10073.078823234197, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527104.711, "ph": "X", "dur": 1.6230741688521382, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527104.57, "ph": "X", "dur": 2.1140040100481556, "name": "Condition._acquire_restore (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:315)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527107.556, "ph": "X", "dur": 0.7414042499694953, "name": "collections.deque.remove", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992517023.536, "ph": "X", "dur": 10084.901215328304, "name": "Condition.wait (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:327)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527110.381, "ph": "X", "dur": 2.013814246538764, "name": "time.time", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527115.411, "ph": "X", "dur": 0.15028464526408686, "name": "_thread.lock.__exit__", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527114.83, "ph": "X", "dur": 0.8415940134788865, "name": "Condition.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:306)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527116.633, "ph": "X", "dur": 0.45085393579226063, "name": "dict.items", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527130.239, "ph": "X", "dur": 1.5329033816936861, "name": "time.time", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527129.678, "ph": "X", "dur": 2.1841368445047293, "name": "CellOutput.<lambda> (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/cell_output.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527128.295, "ph": "X", "dur": 3.847286918760624, "name": "__create_fn__.<locals>.__init__ (<string>:2)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527137.713, "ph": "X", "dur": 1.4126756654824166, "name": "time.time", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527137.603, "ph": "X", "dur": 1.6230741688521382, "name": "CellOp.<lambda> (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:139)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527141.961, "ph": "X", "dur": 1.3725997600786601, "name": "_contextvars.ContextVar.get", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527140.508, "ph": "X", "dur": 4.658824003186694, "name": "CellOp.__post_init__ (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:141)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527135.058, "ph": "X", "dur": 10.329564617818237, "name": "__create_fn__.<locals>.__init__ (<string>:2)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527154.114, "ph": "X", "dur": 0.8516129898298257, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527153.192, "ph": "X", "dur": 1.9637193647840683, "name": "_is_dataclass_instance (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1326)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527159.524, "ph": "X", "dur": 0.160303621615026, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527161.558, "ph": "X", "dur": 0.3005692905281737, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527162.69, "ph": "X", "dur": 0.22041747972066073, "name": "dict.values", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527165.075, "ph": "X", "dur": 0.7013283445657387, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.047, "ph": "X", "dur": 0.2003795270187825, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.327, "ph": "X", "dur": 0.050094881754695626, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.447, "ph": "X", "dur": 0.050094881754695626, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.558, "ph": "X", "dur": 0.1202277162112695, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.748, "ph": "X", "dur": 0.050094881754695626, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.848, "ph": "X", "dur": 0.050094881754695626, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527166.958, "ph": "X", "dur": 0.1202277162112695, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527167.139, "ph": "X", "dur": 0.09017078715845213, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527161.097, "ph": "X", "dur": 6.933131634849874, "name": "fields (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1308)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527168.792, "ph": "X", "dur": 0.22041747972066073, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527169.293, "ph": "X", "dur": 0.3606831486338085, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527170.505, "ph": "X", "dur": 0.09017078715845213, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527170.756, "ph": "X", "dur": 0.19036055066784338, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527171.287, "ph": "X", "dur": 0.080151810807513, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527171.848, "ph": "X", "dur": 0.2705123614753564, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527172.509, "ph": "X", "dur": 0.22041747972066073, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527172.96, "ph": "X", "dur": 0.09017078715845213, "name": "dict.values", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527178.5, "ph": "X", "dur": 0.3005692905281737, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527178.891, "ph": "X", "dur": 0.09017078715845213, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527179.051, "ph": "X", "dur": 0.07013283445657388, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527179.192, "ph": "X", "dur": 0.13024669256220864, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527179.372, "ph": "X", "dur": 0.050094881754695626, "name": "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527172.349, "ph": "X", "dur": 7.424061476045891, "name": "fields (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1308)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527180.053, "ph": "X", "dur": 0.1703225979659651, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527180.805, "ph": "X", "dur": 0.2705123614753564, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527181.947, "ph": "X", "dur": 0.5710816520035301, "name": "builtins.issubclass", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527182.668, "ph": "X", "dur": 0.10018976350939125, "name": "builtins.issubclass", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527182.899, "ph": "X", "dur": 0.06011385810563475, "name": "builtins.issubclass", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527186.205, "ph": "X", "dur": 0.37070212498474764, "name": "builtins.id", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527187.377, "ph": "X", "dur": 0.5410247229507128, "name": "dict.get", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527188.329, "ph": "X", "dur": 0.160303621615026, "name": "builtins.issubclass", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527188.69, "ph": "X", "dur": 0.5209867702488346, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527189.872, "ph": "X", "dur": 0.2905503141772346, "name": "Enum.__deepcopy__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py:1303)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527185.694, "ph": "X", "dur": 4.608729121431997, "name": "deepcopy (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/copy.py:119)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527180.414, "ph": "X", "dur": 10.219355877957907, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527190.974, "ph": "X", "dur": 0.2003795270187825, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527191.325, "ph": "X", "dur": 0.18034157431690426, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527191.725, "ph": "X", "dur": 0.15028464526408686, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527191.976, "ph": "X", "dur": 0.09017078715845213, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527192.337, "ph": "X", "dur": 0.15028464526408686, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527192.577, "ph": "X", "dur": 0.09017078715845213, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527171.467, "ph": "X", "dur": 21.41055246195691, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527193.098, "ph": "X", "dur": 0.10018976350939125, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527193.298, "ph": "X", "dur": 0.2003795270187825, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527193.659, "ph": "X", "dur": 0.09017078715845213, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527193.849, "ph": "X", "dur": 0.09017078715845213, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527194.1, "ph": "X", "dur": 0.06011385810563475, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527194.25, "ph": "X", "dur": 0.10018976350939125, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527194.711, "ph": "X", "dur": 0.06011385810563475, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527194.861, "ph": "X", "dur": 0.11020873986033036, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527195.122, "ph": "X", "dur": 0.15028464526408686, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527195.352, "ph": "X", "dur": 0.15028464526408686, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527158.663, "ph": "X", "dur": 37.04015556942194, "name": "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527152.07, "ph": "X", "dur": 43.72281279549834, "name": "asdict (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1338)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527196.274, "ph": "X", "dur": 0.15028464526408686, "name": "cast (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/typing.py:2366)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527148.834, "ph": "X", "dur": 47.8606500284362, "name": "serialize (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:63)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527147.973, "ph": "X", "dur": 48.792414829073536, "name": "Op.serialize (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:106)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527201.233, "ph": "X", "dur": 0.6211765337582258, "name": "_ConnectionBase._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:135)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527202.456, "ph": "X", "dur": 0.1703225979659651, "name": "_ConnectionBase._check_writable (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:143)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527216.482, "ph": "X", "dur": 0.6311955101091649, "name": "dict.copy", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527217.514, "ph": "X", "dur": 1.9737383411350076, "name": "dict.update", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527207.956, "ph": "X", "dur": 11.662088472493142, "name": "ForkingPickler.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py:38)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527224.658, "ph": "X", "dur": 0.6913093682147996, "name": "Enum.__reduce_ex__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py:1300)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527220.43, "ph": "X", "dur": 19.266491522855937, "name": "ForkingPickler.dump", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527241.43, "ph": "X", "dur": 1.272409996569269, "name": "_io.BytesIO.getbuffer", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527204.67, "ph": "X", "dur": 38.16228092072713, "name": "ForkingPickler.dumps (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py:48)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527244.335, "ph": "X", "dur": 0.45085393579226063, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527245.658, "ph": "X", "dur": 2.083947080995338, "name": "_struct.pack", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527249.916, "ph": "X", "dur": 0.09017078715845213, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527250.206, "ph": "X", "dur": 32.95241321823878, "name": "posix.write", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527249.755, "ph": "X", "dur": 33.934272900630816, "name": "Connection._send (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:381)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527243.954, "ph": "X", "dur": 39.96569666389617, "name": "Connection._send_bytes (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:406)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527200.552, "ph": "X", "dur": 84.07924953708114, "name": "_ConnectionBase.send (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:202)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527284.922, "ph": "X", "dur": 0.23043645607159988, "name": "_thread.lock.__exit__", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527198.168, "ph": "X", "dur": 87.11499937141569, "name": "ThreadSafeStream.write (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/streams.py:122)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527146.72, "ph": "X", "dur": 138.91310710577096, "name": "Op.broadcast (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:86)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527120.981, "ph": "X", "dur": 165.32312876684648, "name": "_write_console_output (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:33)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527288.248, "ph": "X", "dur": 0.3606831486338085, "name": "_thread.lock.__enter__", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527288.058, "ph": "X", "dur": 0.6512334628110431, "name": "Condition.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:303)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527288.97, "ph": "X", "dur": 1.60303621615026, "name": "time.time", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527291.935, "ph": "X", "dur": 0.480910864845078, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527291.785, "ph": "X", "dur": 0.7914991317241908, "name": "Condition._is_owned (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:318)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527292.997, "ph": "X", "dur": 0.480910864845078, "name": "_thread.allocate_lock", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527293.678, "ph": "X", "dur": 0.14026566891314776, "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527293.979, "ph": "X", "dur": 0.1202277162112695, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527294.42, "ph": "X", "dur": 0.1202277162112695, "name": "_thread.lock.release", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527294.32, "ph": "X", "dur": 0.3005692905281737, "name": "Condition._release_save (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:312)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516983.23, "ph": "X", "dur": 1000442.8596898034, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516981.016, "ph": "X", "dur": 1000449.1215500227, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517436.089, "ph": "X", "dur": 0.5109677938978954, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517439.285, "ph": "X", "dur": 2.3043645607159986, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517437.792, "ph": "X", "dur": 4.027628493077528, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517443.683, "ph": "X", "dur": 0.9117268479354603, "name": "_heapq.heappop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517445.517, "ph": "X", "dur": 0.7614422026713735, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517448.192, "ph": "X", "dur": 0.42079700673944326, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517451.708, "ph": "X", "dur": 0.21039850336972163, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517460.174, "ph": "X", "dur": 0.18034157431690426, "name": "_asyncio.Future.cancelled", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517465.324, "ph": "X", "dur": 0.2003795270187825, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517469.332, "ph": "X", "dur": 0.06011385810563475, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517468.31, "ph": "X", "dur": 1.5930172397993208, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517471.115, "ph": "X", "dur": 0.1703225979659651, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517466.216, "ph": "X", "dur": 5.199848726137406, "name": "BaseEventLoop._call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:848)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517464.583, "ph": "X", "dur": 6.903074705797057, "name": "BaseEventLoop.call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:819)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517461.517, "ph": "X", "dur": 10.269450759712603, "name": "_asyncio.Future.set_result", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517459.874, "ph": "X", "dur": 12.223151148145734, "name": "_set_result_unless_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:312)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517458.381, "ph": "X", "dur": 14.026566891314774, "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517454.504, "ph": "X", "dur": 18.19446105330545, "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183992516976.287, "ph": "X", "dur": 1000496.5914599736, "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517473.69, "ph": "X", "dur": 0.3105882668791129, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517476.145, "ph": "X", "dur": 0.39074007768662583, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517482.206, "ph": "X", "dur": 2.795294401912016, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517475.223, "ph": "X", "dur": 10.07909020904476, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517485.623, "ph": "X", "dur": 0.11020873986033036, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517486.083, "ph": "X", "dur": 1.1822392094108167, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517485.973, "ph": "X", "dur": 1.3525618073767818, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517487.556, "ph": "X", "dur": 0.10018976350939125, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517488.187, "ph": "X", "dur": 0.10018976350939125, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517503.536, "ph": "X", "dur": 0.13024669256220864, "name": "BaseEventLoop._timer_handle_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1942)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517507.334, "ph": "X", "dur": 0.080151810807513, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517506.823, "ph": "X", "dur": 0.8315750371279473, "name": "Handle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:73)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517502.835, "ph": "X", "dur": 4.949374317363928, "name": "TimerHandle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:157)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517495.341, "ph": "X", "dur": 12.513701462322969, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517494.279, "ph": "X", "dur": 15.319014840585922, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517513.505, "ph": "X", "dur": 0.9918786587429733, "name": "math.isnan", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517514.838, "ph": "X", "dur": 0.3606831486338085, "name": "_asyncio.get_running_loop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517519.336, "ph": "X", "dur": 0.06011385810563475, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517515.649, "ph": "X", "dur": 4.378292665360397, "name": "BaseEventLoop.create_future (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:458)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517521.701, "ph": "X", "dur": 0.6311955101091649, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517521.581, "ph": "X", "dur": 0.80151810807513, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517524.697, "ph": "X", "dur": 0.1202277162112695, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517527.301, "ph": "X", "dur": 0.80151810807513, "name": "_contextvars.copy_context", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517530.077, "ph": "X", "dur": 0.09017078715845213, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517526.821, "ph": "X", "dur": 3.697002273496537, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517525.718, "ph": "X", "dur": 5.129715891680832, "name": "TimerHandle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:113)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517531.409, "ph": "X", "dur": 0.5811006283544692, "name": "_heapq.heappush", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517524.546, "ph": "X", "dur": 7.90497234089097, "name": "BaseEventLoop.call_at (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:801)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517521.17, "ph": "X", "dur": 11.712183354247836, "name": "BaseEventLoop.call_later (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:777)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517512.373, "ph": "X", "dur": 21.540799154519117, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517510.83, "ph": "X", "dur": 23.14383537066938, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517493.958, "ph": "X", "dur": 40.115981309160254, "name": "gato_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:6)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517493.167, "ph": "X", "dur": 41.017689180744775, "name": "<module> (/tmp/marimo_908864/__marimo__cell_bkHC_.py:1)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517492.706, "ph": "X", "dur": 41.54869492734455, "name": "DefaultExecutor.execute_cell_async (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py:101)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517488.989, "ph": "X", "dur": 46.71848672442914, "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517488.528, "ph": "X", "dur": 47.38975813994206, "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517473.59, "ph": "X", "dur": 63.4301392777956, "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517537.791, "ph": "X", "dur": 0.15028464526408686, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517538.883, "ph": "X", "dur": 1.2223151148145732, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517538.753, "ph": "X", "dur": 1.3926377127805383, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517541.488, "ph": "X", "dur": 0.561062675652591, "name": "math.ceil", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517542.731, "ph": "X", "dur": 0.1202277162112695, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517543.182, "ph": "X", "dur": 1000753.3477669191, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517541.078, "ph": "X", "dur": 1000758.5075397397, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518303.452, "ph": "X", "dur": 0.23043645607159988, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518306.198, "ph": "X", "dur": 2.5348010167875987, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518304.865, "ph": "X", "dur": 4.147856209288798, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518311.227, "ph": "X", "dur": 0.8215560607770083, "name": "_heapq.heappop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518317.99, "ph": "X", "dur": 1.0319545641467298, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518320.555, "ph": "X", "dur": 0.5410247229507128, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518323.621, "ph": "X", "dur": 0.21039850336972163, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518330.393, "ph": "X", "dur": 0.240455432422539, "name": "_asyncio.Future.cancelled", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518335.403, "ph": "X", "dur": 0.240455432422539, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518339.05, "ph": "X", "dur": 0.06011385810563475, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518338.208, "ph": "X", "dur": 0.9818596823920341, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518340.142, "ph": "X", "dur": 0.160303621615026, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518336.154, "ph": "X", "dur": 4.318178807254762, "name": "BaseEventLoop._call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:848)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518334.862, "ph": "X", "dur": 5.750892425439058, "name": "BaseEventLoop.call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:819)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518331.706, "ph": "X", "dur": 9.177382337460237, "name": "_asyncio.Future.set_result", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518330.023, "ph": "X", "dur": 11.151120678595245, "name": "_set_result_unless_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:312)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518329.021, "ph": "X", "dur": 12.373435793409818, "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518325.805, "ph": "X", "dur": 15.829982634483816, "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183993517537.491, "ph": "X", "dur": 1000804.6749827649, "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518342.947, "ph": "X", "dur": 0.22041747972066073, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518344.991, "ph": "X", "dur": 0.33062621958099114, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518345.662, "ph": "X", "dur": 3.43650888837212, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518344.129, "ph": "X", "dur": 5.219886678839284, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518349.65, "ph": "X", "dur": 0.09017078715845213, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518350.071, "ph": "X", "dur": 1.2523720438673906, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518349.971, "ph": "X", "dur": 1.4026566891314773, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518351.554, "ph": "X", "dur": 0.10018976350939125, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518352.125, "ph": "X", "dur": 0.11020873986033036, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518364.468, "ph": "X", "dur": 0.19036055066784338, "name": "BaseEventLoop._timer_handle_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1942)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518367.644, "ph": "X", "dur": 0.06011385810563475, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518367.153, "ph": "X", "dur": 0.7614422026713735, "name": "Handle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:73)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518364.067, "ph": "X", "dur": 3.9775336113228326, "name": "TimerHandle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:157)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518357.835, "ph": "X", "dur": 10.279469736063541, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518357.164, "ph": "X", "dur": 12.173056266391036, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518373.906, "ph": "X", "dur": 0.2905503141772346, "name": "_thread.get_ident", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518375.309, "ph": "X", "dur": 3.536698651881511, "name": "builtins.print", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518372.674, "ph": "X", "dur": 6.542391557163249, "name": "print_override (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/print_override.py:19)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518356.834, "ph": "X", "dur": 22.592791671367728, "name": "gato_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:6)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518384.416, "ph": "X", "dur": 1.1622012567089384, "name": "math.isnan", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518385.999, "ph": "X", "dur": 0.320607243230052, "name": "_asyncio.get_running_loop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518389.315, "ph": "X", "dur": 0.07013283445657388, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518386.77, "ph": "X", "dur": 3.1058826687911285, "name": "BaseEventLoop.create_future (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:458)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518391.269, "ph": "X", "dur": 1.3124859019730255, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518391.209, "ph": "X", "dur": 1.4327136181842948, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518394.745, "ph": "X", "dur": 0.10018976350939125, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518397.14, "ph": "X", "dur": 0.7113473209166778, "name": "_contextvars.copy_context", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518399.404, "ph": "X", "dur": 0.0400759054037565, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518396.268, "ph": "X", "dur": 3.536698651881511, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518395.567, "ph": "X", "dur": 4.488501405220728, "name": "TimerHandle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:113)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518400.466, "ph": "X", "dur": 0.480910864845078, "name": "_heapq.heappush", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518394.555, "ph": "X", "dur": 6.512334628110431, "name": "BaseEventLoop.call_at (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:801)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518390.728, "ph": "X", "dur": 12.974574374466167, "name": "BaseEventLoop.call_later (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:777)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518383.264, "ph": "X", "dur": 21.580875059922874, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518382.151, "ph": "X", "dur": 22.76311426933369, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518381.731, "ph": "X", "dur": 23.234006157827828, "name": "yo_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:11)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518356.343, "ph": "X", "dur": 48.74231994731884, "name": "<module> (/tmp/marimo_908864/__marimo__cell_bkHC_.py:1)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518355.862, "ph": "X", "dur": 49.29336364662049, "name": "DefaultExecutor.execute_cell_async (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py:101)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518352.916, "ph": "X", "dur": 53.35104906875084, "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518352.485, "ph": "X", "dur": 53.922130720754375, "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518342.637, "ph": "X", "dur": 64.68251132166299, "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518407.83, "ph": "X", "dur": 0.080151810807513, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518408.441, "ph": "X", "dur": 1.2022771621126949, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518408.381, "ph": "X", "dur": 1.3124859019730255, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518410.736, "ph": "X", "dur": 0.6512334628110431, "name": "math.ceil", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518412.068, "ph": "X", "dur": 0.080151810807513, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518412.359, "ph": "X", "dur": 1001022.5276045398, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518410.385, "ph": "X", "dur": 1001027.5972065732, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519442.19, "ph": "X", "dur": 0.33062621958099114, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519444.915, "ph": "X", "dur": 2.554838969489477, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519443.673, "ph": "X", "dur": 4.027628493077528, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519450.005, "ph": "X", "dur": 0.9217458242863995, "name": "_heapq.heappop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519451.868, "ph": "X", "dur": 0.721366297267617, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519454.523, "ph": "X", "dur": 0.561062675652591, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519457.649, "ph": "X", "dur": 0.21039850336972163, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519464.562, "ph": "X", "dur": 0.240455432422539, "name": "_asyncio.Future.cancelled", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519468.76, "ph": "X", "dur": 0.2003795270187825, "name": "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519472.477, "ph": "X", "dur": 0.09017078715845213, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519471.716, "ph": "X", "dur": 1.0219355877957907, "name": "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519473.85, "ph": "X", "dur": 0.14026566891314776, "name": "collections.deque.append", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519469.632, "ph": "X", "dur": 4.518558334273545, "name": "BaseEventLoop._call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:848)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519468.32, "ph": "X", "dur": 5.921215023405023, "name": "BaseEventLoop.call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:819)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519465.925, "ph": "X", "dur": 8.596281709105769, "name": "_asyncio.Future.set_result", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519464.212, "ph": "X", "dur": 10.620114931995474, "name": "_set_result_unless_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:312)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519463.1, "ph": "X", "dur": 12.052828550179767, "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519459.954, "ph": "X", "dur": 15.439242556797192, "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183994518407.68, "ph": "X", "dur": 1001067.9235863858, "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519476.615, "ph": "X", "dur": 0.240455432422539, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519479.17, "ph": "X", "dur": 0.38072110133568676, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519479.972, "ph": "X", "dur": 3.5968125099871457, "name": "select.epoll.poll", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519478.339, "ph": "X", "dur": 5.4903990403146405, "name": "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519484.14, "ph": "X", "dur": 0.10018976350939125, "name": "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519484.56, "ph": "X", "dur": 1.132144327656121, "name": "time.monotonic", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519484.45, "ph": "X", "dur": 1.3225048783239646, "name": "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519485.963, "ph": "X", "dur": 0.080151810807513, "name": "builtins.len", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519486.504, "ph": "X", "dur": 0.11020873986033036, "name": "collections.deque.popleft", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519501.743, "ph": "X", "dur": 0.11020873986033036, "name": "BaseEventLoop._timer_handle_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1942)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519504.628, "ph": "X", "dur": 0.09017078715845213, "name": "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519504.198, "ph": "X", "dur": 5.680759590982484, "name": "Handle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:73)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519500.941, "ph": "X", "dur": 9.067173597599908, "name": "TimerHandle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:157)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519494.94, "ph": "X", "dur": 15.148692242619957, "name": "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519494.539, "ph": "X", "dur": 16.851918222279608, "name": "siesta_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:3)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519513.936, "ph": "X", "dur": 0.3606831486338085, "name": "_thread.get_ident", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519515.188, "ph": "X", "dur": 4.408349594413215, "name": "builtins.print", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519512.924, "ph": "X", "dur": 7.023302422008326, "name": "print_override (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/print_override.py:19)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519494.229, "ph": "X", "dur": 25.939129772581392, "name": "yo_async (/tmp/marimo_908864/__marimo__cell_bkHC_.py:11)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519527.732, "ph": "X", "dur": 1.1622012567089384, "name": "list.pop", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519529.015, "ph": "X", "dur": 1.0720304695504863, "name": "builtins.setattr", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519525.979, "ph": "X", "dur": 4.187932114692554, "name": "_RedirectStream.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:407)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519525.057, "ph": "X", "dur": 5.941252976106901, "name": "capture_stdout (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/capture.py:16)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519524.125, "ph": "X", "dur": 8.746566354369856, "name": "builtins.next", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519523.715, "ph": "X", "dur": 11.281367371157454, "name": "_GeneratorContextManager.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:145)", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527294.811, "ph": "B", "name": "_thread.lock.acquire", "cat": "FEE"}, {"pid": 908864, "tid": 908865, "ts": 183992527291.434, "ph": "B", "name": "Condition.wait (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:327)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519493.818, "ph": "B", "name": "<module> (/tmp/marimo_908864/__marimo__cell_bkHC_.py:1)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519492.926, "ph": "B", "name": "DefaultExecutor.execute_cell_async (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py:101)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519489.83, "ph": "B", "name": "_contextvars.Context.run", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519488.648, "ph": "B", "name": "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)", "cat": "FEE"}, {"pid": 908864, "tid": 908864, "ts": 183995519476.325, "ph": "B", "name": "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1751213996778879189}, "file_info": {"files": {"/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py": ["\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\nimport abc\nimport os\nimport sys\nimport _collections_abc\nfrom collections import deque\nfrom functools import wraps\nfrom types import MethodType, GenericAlias\n\n__all__ = [\"asynccontextmanager\", \"contextmanager\", \"closing\", \"nullcontext\",\n           \"AbstractContextManager\", \"AbstractAsyncContextManager\",\n           \"AsyncExitStack\", \"ContextDecorator\", \"ExitStack\",\n           \"redirect_stdout\", \"redirect_stderr\", \"suppress\", \"aclosing\",\n           \"chdir\"]\n\n\nclass AbstractContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    __slots__ = ()\n\n    def __enter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractContextManager:\n            return _collections_abc._check_methods(C, \"__enter__\", \"__exit__\")\n        return NotImplemented\n\n\nclass AbstractAsyncContextManager(abc.ABC):\n\n    \"\"\"An abstract base class for asynchronous context managers.\"\"\"\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    __slots__ = ()\n\n    async def __aenter__(self):\n        \"\"\"Return `self` upon entering the runtime context.\"\"\"\n        return self\n\n    @abc.abstractmethod\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        \"\"\"Raise any exception triggered within the runtime context.\"\"\"\n        return None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AbstractAsyncContextManager:\n            return _collections_abc._check_methods(C, \"__aenter__\",\n                                                   \"__aexit__\")\n        return NotImplemented\n\n\nclass ContextDecorator(object):\n    \"A base class or mixin that enables context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n\n        Allows an otherwise one-shot context manager like\n        _GeneratorContextManager to support use as\n        a decorator via implicit recreation.\n\n        This is a private interface just for _GeneratorContextManager.\n        See issue #11647 for details.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        def inner(*args, **kwds):\n            with self._recreate_cm():\n                return func(*args, **kwds)\n        return inner\n\n\nclass AsyncContextDecorator(object):\n    \"A base class or mixin that enables async context managers to work as decorators.\"\n\n    def _recreate_cm(self):\n        \"\"\"Return a recreated instance of self.\n        \"\"\"\n        return self\n\n    def __call__(self, func):\n        @wraps(func)\n        async def inner(*args, **kwds):\n            async with self._recreate_cm():\n                return await func(*args, **kwds)\n        return inner\n\n\nclass _GeneratorContextManagerBase:\n    \"\"\"Shared functionality for @contextmanager and @asynccontextmanager.\"\"\"\n\n    def __init__(self, func, args, kwds):\n        self.gen = func(*args, **kwds)\n        self.func, self.args, self.kwds = func, args, kwds\n        # Issue 19330: ensure context manager instances have good docstrings\n        doc = getattr(func, \"__doc__\", None)\n        if doc is None:\n            doc = type(self).__doc__\n        self.__doc__ = doc\n        # Unfortunately, this still doesn't provide good help output when\n        # inspecting the created context manager instances, since pydoc\n        # currently bypasses the instance docstring and shows the docstring\n        # for the class instead.\n        # See http://bugs.python.org/issue19404 for more details.\n\n    def _recreate_cm(self):\n        # _GCMB instances are one-shot context managers, so the\n        # CM must be recreated each time a decorated function is\n        # called\n        return self.__class__(self.func, self.args, self.kwds)\n\n\nclass _GeneratorContextManager(\n    _GeneratorContextManagerBase,\n    AbstractContextManager,\n    ContextDecorator,\n):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def __enter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return next(self.gen)\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    def __exit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                next(self.gen)\n            except StopIteration:\n                return False\n            else:\n                try:\n                    raise RuntimeError(\"generator didn't stop\")\n                finally:\n                    self.gen.close()\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = typ()\n            try:\n                self.gen.throw(value)\n            except StopIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    exc.__traceback__ = traceback\n                    return False\n                # Avoid suppressing if a StopIteration exception\n                # was passed to throw() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actually Stop(Async)Iteration (see\n                # issue29692).\n                if (\n                    isinstance(value, StopIteration)\n                    and exc.__cause__ is value\n                ):\n                    value.__traceback__ = traceback\n                    return False\n                raise\n            except BaseException as exc:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                if exc is not value:\n                    raise\n                exc.__traceback__ = traceback\n                return False\n            try:\n                raise RuntimeError(\"generator didn't stop after throw()\")\n            finally:\n                self.gen.close()\n\nclass _AsyncGeneratorContextManager(\n    _GeneratorContextManagerBase,\n    AbstractAsyncContextManager,\n    AsyncContextDecorator,\n):\n    \"\"\"Helper for @asynccontextmanager decorator.\"\"\"\n\n    async def __aenter__(self):\n        # do not keep args and kwds alive unnecessarily\n        # they are only needed for recreation, which is not possible anymore\n        del self.args, self.kwds, self.func\n        try:\n            return await anext(self.gen)\n        except StopAsyncIteration:\n            raise RuntimeError(\"generator didn't yield\") from None\n\n    async def __aexit__(self, typ, value, traceback):\n        if typ is None:\n            try:\n                await anext(self.gen)\n            except StopAsyncIteration:\n                return False\n            else:\n                try:\n                    raise RuntimeError(\"generator didn't stop\")\n                finally:\n                    await self.gen.aclose()\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = typ()\n            try:\n                await self.gen.athrow(value)\n            except StopAsyncIteration as exc:\n                # Suppress StopIteration *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed.\n                return exc is not value\n            except RuntimeError as exc:\n                # Don't re-raise the passed in exception. (issue27122)\n                if exc is value:\n                    exc.__traceback__ = traceback\n                    return False\n                # Avoid suppressing if a Stop(Async)Iteration exception\n                # was passed to athrow() and later wrapped into a RuntimeError\n                # (see PEP 479 for sync generators; async generators also\n                # have this behavior). But do this only if the exception wrapped\n                # by the RuntimeError is actually Stop(Async)Iteration (see\n                # issue29692).\n                if (\n                    isinstance(value, (StopIteration, StopAsyncIteration))\n                    and exc.__cause__ is value\n                ):\n                    value.__traceback__ = traceback\n                    return False\n                raise\n            except BaseException as exc:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                if exc is not value:\n                    raise\n                exc.__traceback__ = traceback\n                return False\n            try:\n                raise RuntimeError(\"generator didn't stop after athrow()\")\n            finally:\n                await self.gen.aclose()\n\n\ndef contextmanager(func):\n    \"\"\"@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _GeneratorContextManager(func, args, kwds)\n    return helper\n\n\ndef asynccontextmanager(func):\n    \"\"\"@asynccontextmanager decorator.\n\n    Typical usage:\n\n        @asynccontextmanager\n        async def some_async_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        async with some_async_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return _AsyncGeneratorContextManager(func, args, kwds)\n    return helper\n\n\nclass closing(AbstractContextManager):\n    \"\"\"Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    def __enter__(self):\n        return self.thing\n    def __exit__(self, *exc_info):\n        self.thing.close()\n\n\nclass aclosing(AbstractAsyncContextManager):\n    \"\"\"Async context manager for safely finalizing an asynchronously cleaned-up\n    resource such as an async generator, calling its ``aclose()`` method.\n\n    Code like this:\n\n        async with aclosing(<module>.fetch(<arguments>)) as agen:\n            <block>\n\n    is equivalent to this:\n\n        agen = <module>.fetch(<arguments>)\n        try:\n            <block>\n        finally:\n            await agen.aclose()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    async def __aenter__(self):\n        return self.thing\n    async def __aexit__(self, *exc_info):\n        await self.thing.aclose()\n\n\nclass _RedirectStream(AbstractContextManager):\n\n    _stream = None\n\n    def __init__(self, new_target):\n        self._new_target = new_target\n        # We use a list of old targets to make this CM re-entrant\n        self._old_targets = []\n\n    def __enter__(self):\n        self._old_targets.append(getattr(sys, self._stream))\n        setattr(sys, self._stream, self._new_target)\n        return self._new_target\n\n    def __exit__(self, exctype, excinst, exctb):\n        setattr(sys, self._stream, self._old_targets.pop())\n\n\nclass redirect_stdout(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stdout to another file.\n\n        # How to send help() to stderr\n        with redirect_stdout(sys.stderr):\n            help(dir)\n\n        # How to write help() to a file\n        with open('help.txt', 'w') as f:\n            with redirect_stdout(f):\n                help(pow)\n    \"\"\"\n\n    _stream = \"stdout\"\n\n\nclass redirect_stderr(_RedirectStream):\n    \"\"\"Context manager for temporarily redirecting stderr to another file.\"\"\"\n\n    _stream = \"stderr\"\n\n\nclass suppress(AbstractContextManager):\n    \"\"\"Context manager to suppress specified exceptions\n\n    After the exception is suppressed, execution proceeds with the next\n    statement following the with statement.\n\n         with suppress(FileNotFoundError):\n             os.remove(somefile)\n         # Execution still resumes here if the file was already removed\n    \"\"\"\n\n    def __init__(self, *exceptions):\n        self._exceptions = exceptions\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exctype, excinst, exctb):\n        # Unlike isinstance and issubclass, CPython exception handling\n        # currently only looks at the concrete type hierarchy (ignoring\n        # the instance and subclass checking hooks). While Guido considers\n        # that a bug rather than a feature, it's a fairly hard one to fix\n        # due to various internal implementation details. suppress provides\n        # the simpler issubclass based semantics, rather than trying to\n        # exactly reproduce the limitations of the CPython interpreter.\n        #\n        # See http://bugs.python.org/issue12029 for more details\n        if exctype is None:\n            return\n        if issubclass(exctype, self._exceptions):\n            return True\n        if issubclass(exctype, BaseExceptionGroup):\n            match, rest = excinst.split(self._exceptions)\n            if rest is None:\n                return True\n            raise rest\n        return False\n\n\nclass _BaseExitStack:\n    \"\"\"A base class for ExitStack and AsyncExitStack.\"\"\"\n\n    @staticmethod\n    def _create_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_cb_wrapper(callback, /, *args, **kwds):\n        def _exit_wrapper(exc_type, exc, tb):\n            callback(*args, **kwds)\n        return _exit_wrapper\n\n    def __init__(self):\n        self._exit_callbacks = deque()\n\n    def pop_all(self):\n        \"\"\"Preserve the context stack by transferring it to a new instance.\"\"\"\n        new_stack = type(self)()\n        new_stack._exit_callbacks = self._exit_callbacks\n        self._exit_callbacks = deque()\n        return new_stack\n\n    def push(self, exit):\n        \"\"\"Registers a callback with the standard __exit__ method signature.\n\n        Can suppress exceptions the same way __exit__ method can.\n        Also accepts any object with an __exit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        # We use an unbound method rather than a bound method to follow\n        # the standard lookup behaviour for special methods.\n        _cb_type = type(exit)\n\n        try:\n            exit_method = _cb_type.__exit__\n        except AttributeError:\n            # Not a context manager, so assume it's a callable.\n            self._push_exit_callback(exit)\n        else:\n            self._push_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator.\n\n    def enter_context(self, cm):\n        \"\"\"Enters the supplied context manager.\n\n        If successful, also pushes its __exit__ method as a callback and\n        returns the result of the __enter__ method.\n        \"\"\"\n        # We look up the special methods on the type to match the with\n        # statement.\n        cls = type(cm)\n        try:\n            _enter = cls.__enter__\n            _exit = cls.__exit__\n        except AttributeError:\n            raise TypeError(f\"'{cls.__module__}.{cls.__qualname__}' object does \"\n                            f\"not support the context manager protocol\") from None\n        result = _enter(cm)\n        self._push_cm_exit(cm, _exit)\n        return result\n\n    def callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary callback and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper)\n        return callback  # Allow use as a decorator\n\n    def _push_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register callbacks to __exit__ methods.\"\"\"\n        _exit_wrapper = self._create_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, True)\n\n    def _push_exit_callback(self, callback, is_sync=True):\n        self._exit_callbacks.append((is_sync, callback))\n\n\n# Inspired by discussions on http://bugs.python.org/issue13585\nclass ExitStack(_BaseExitStack, AbstractContextManager):\n    \"\"\"Context manager for dynamic management of a stack of exit callbacks.\n\n    For example:\n        with ExitStack() as stack:\n            files = [stack.enter_context(open(fname)) for fname in filenames]\n            # All opened files will automatically be closed at the end of\n            # the with statement, even if attempts to open files later\n            # in the list raise an exception.\n    \"\"\"\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_details):\n        exc = exc_details[1]\n        received_exc = exc is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exception()\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is None or exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            assert is_sync\n            try:\n                if exc is None:\n                    exc_details = None, None, None\n                else:\n                    exc_details = type(exc), exc, exc.__traceback__\n                if cb(*exc_details):\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc = None\n            except BaseException as new_exc:\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc, exc)\n                pending_raise = True\n                exc = new_exc\n\n        if pending_raise:\n            try:\n                # bare \"raise exc\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc.__context__\n                raise exc\n            except BaseException:\n                exc.__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n    def close(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        self.__exit__(None, None, None)\n\n\n# Inspired by discussions on https://bugs.python.org/issue29302\nclass AsyncExitStack(_BaseExitStack, AbstractAsyncContextManager):\n    \"\"\"Async context manager for dynamic management of a stack of exit\n    callbacks.\n\n    For example:\n        async with AsyncExitStack() as stack:\n            connections = [await stack.enter_async_context(get_connection())\n                for i in range(5)]\n            # All opened connections will automatically be released at the\n            # end of the async with statement, even if attempts to open a\n            # connection later in the list raise an exception.\n    \"\"\"\n\n    @staticmethod\n    def _create_async_exit_wrapper(cm, cm_exit):\n        return MethodType(cm_exit, cm)\n\n    @staticmethod\n    def _create_async_cb_wrapper(callback, /, *args, **kwds):\n        async def _exit_wrapper(exc_type, exc, tb):\n            await callback(*args, **kwds)\n        return _exit_wrapper\n\n    async def enter_async_context(self, cm):\n        \"\"\"Enters the supplied async context manager.\n\n        If successful, also pushes its __aexit__ method as a callback and\n        returns the result of the __aenter__ method.\n        \"\"\"\n        cls = type(cm)\n        try:\n            _enter = cls.__aenter__\n            _exit = cls.__aexit__\n        except AttributeError:\n            raise TypeError(f\"'{cls.__module__}.{cls.__qualname__}' object does \"\n                            f\"not support the asynchronous context manager protocol\"\n                           ) from None\n        result = await _enter(cm)\n        self._push_async_cm_exit(cm, _exit)\n        return result\n\n    def push_async_exit(self, exit):\n        \"\"\"Registers a coroutine function with the standard __aexit__ method\n        signature.\n\n        Can suppress exceptions the same way __aexit__ method can.\n        Also accepts any object with an __aexit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n        _cb_type = type(exit)\n        try:\n            exit_method = _cb_type.__aexit__\n        except AttributeError:\n            # Not an async context manager, so assume it's a coroutine function\n            self._push_exit_callback(exit, False)\n        else:\n            self._push_async_cm_exit(exit, exit_method)\n        return exit  # Allow use as a decorator\n\n    def push_async_callback(self, callback, /, *args, **kwds):\n        \"\"\"Registers an arbitrary coroutine function and arguments.\n\n        Cannot suppress exceptions.\n        \"\"\"\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\n\n        # We changed the signature, so using @wraps is not appropriate, but\n        # setting __wrapped__ may still help with introspection.\n        _exit_wrapper.__wrapped__ = callback\n        self._push_exit_callback(_exit_wrapper, False)\n        return callback  # Allow use as a decorator\n\n    async def aclose(self):\n        \"\"\"Immediately unwind the context stack.\"\"\"\n        await self.__aexit__(None, None, None)\n\n    def _push_async_cm_exit(self, cm, cm_exit):\n        \"\"\"Helper to correctly register coroutine function to __aexit__\n        method.\"\"\"\n        _exit_wrapper = self._create_async_exit_wrapper(cm, cm_exit)\n        self._push_exit_callback(_exit_wrapper, False)\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc_details):\n        exc = exc_details[1]\n        received_exc = exc is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exception()\n        def _fix_exception_context(new_exc, old_exc):\n            # Context may not be correct, so find the end of the chain\n            while 1:\n                exc_context = new_exc.__context__\n                if exc_context is None or exc_context is old_exc:\n                    # Context is already set correctly (see issue 20317)\n                    return\n                if exc_context is frame_exc:\n                    break\n                new_exc = exc_context\n            # Change the end of the chain to point to the exception\n            # we expect it to reference\n            new_exc.__context__ = old_exc\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            is_sync, cb = self._exit_callbacks.pop()\n            try:\n                if exc is None:\n                    exc_details = None, None, None\n                else:\n                    exc_details = type(exc), exc, exc.__traceback__\n                if is_sync:\n                    cb_suppress = cb(*exc_details)\n                else:\n                    cb_suppress = await cb(*exc_details)\n\n                if cb_suppress:\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc = None\n            except BaseException as new_exc:\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc, exc)\n                pending_raise = True\n                exc = new_exc\n\n        if pending_raise:\n            try:\n                # bare \"raise exc\" replaces our carefully\n                # set-up context\n                fixed_ctx = exc.__context__\n                raise exc\n            except BaseException:\n                exc.__context__ = fixed_ctx\n                raise\n        return received_exc and suppressed_exc\n\n\nclass nullcontext(AbstractContextManager, AbstractAsyncContextManager):\n    \"\"\"Context manager that does no additional processing.\n\n    Used as a stand-in for a normal context manager, when a particular\n    block of code is only sometimes used with a normal context manager:\n\n    cm = optional_cm if condition else nullcontext()\n    with cm:\n        # Perform operation, using optional_cm if condition is True\n    \"\"\"\n\n    def __init__(self, enter_result=None):\n        self.enter_result = enter_result\n\n    def __enter__(self):\n        return self.enter_result\n\n    def __exit__(self, *excinfo):\n        pass\n\n    async def __aenter__(self):\n        return self.enter_result\n\n    async def __aexit__(self, *excinfo):\n        pass\n\n\nclass chdir(AbstractContextManager):\n    \"\"\"Non thread-safe context manager to change the current working directory.\"\"\"\n\n    def __init__(self, path):\n        self.path = path\n        self._old_cwd = []\n\n    def __enter__(self):\n        self._old_cwd.append(os.getcwd())\n        os.chdir(self.path)\n\n    def __exit__(self, *excinfo):\n        os.chdir(self._old_cwd.pop())\n", 814], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/capture.py": ["# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport contextlib\nimport io\nimport sys\nfrom typing import TYPE_CHECKING\n\nfrom marimo._plugins.stateless.plain_text import plain_text\nfrom marimo._runtime.output import _output\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n\n\n@contextlib.contextmanager\ndef capture_stdout() -> Iterator[io.StringIO]:\n    \"\"\"Capture standard output.\n\n    Use this context manager to capture print statements and\n    other output sent to standard output.\n\n    Examples:\n        ```python\n        with mo.capture_stdout() as buffer:\n            print(\"Hello!\")\n        output = buffer.getvalue()\n        ```\n    \"\"\"\n    with contextlib.redirect_stdout(io.StringIO()) as buffer:\n        yield buffer\n\n\n@contextlib.contextmanager\ndef capture_stderr() -> Iterator[io.StringIO]:\n    \"\"\"Capture standard error.\n\n    Use this context manager to capture output sent to standard error.\n\n    Examples:\n        ```python\n        with mo.capture_stderr() as buffer:\n            sys.stderr.write(\"Hello!\")\n        output = buffer.getvalue()\n        ```\n    \"\"\"\n    with contextlib.redirect_stderr(io.StringIO()) as buffer:\n        yield buffer\n\n\ndef _redirect(msg: str) -> None:\n    _output.append(plain_text(msg))\n\n\n@contextlib.contextmanager\ndef redirect_stdout() -> Iterator[None]:\n    \"\"\"Redirect stdout to a cell's output area.\n\n    Examples:\n        ```python\n        with mo.redirect_stdout():\n            # These print statements will show up in the cell's output area\n            print(\"Hello!\")\n            print(\"World!\")\n        ```\n    \"\"\"\n    old_stdout_write = sys.stdout.write\n    sys.stdout.write = _redirect  # type: ignore\n    try:\n        yield\n    finally:\n        sys.stdout.write = old_stdout_write  # type: ignore\n\n\n@contextlib.contextmanager\ndef redirect_stderr() -> Iterator[None]:\n    \"\"\"Redirect `stderr` to a cell's output area.\n\n    Examples:\n        ```python\n        with mo.redirect_stderr():\n            # These messages will show up in the cell's output area\n            sys.stderr.write(\"Hello!\")\n            sys.stderr.write(\"World!\")\n        ```\n    \"\"\"\n    old_stderr_write = sys.stderr.write\n    sys.stderr.write = _redirect  # type: ignore\n    try:\n        yield\n    finally:\n        sys.stderr.write = old_stderr_write  # type: ignore\n", 92], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport errno\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import timeouts\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop','Server',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout, ssl_shutdown_timeout=None):\n        self._loop = loop\n        self._sockets = sockets\n        # Weak references so we don't break Transport's ability to\n        # detect abandoned transports\n        self._clients = weakref.WeakSet()\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._ssl_shutdown_timeout = ssl_shutdown_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self, transport):\n        assert self._sockets is not None\n        self._clients.add(transport)\n\n    def _detach(self, transport):\n        self._clients.discard(transport)\n        if len(self._clients) == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout,\n                self._ssl_shutdown_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if len(self._clients) == 0:\n            self._wakeup()\n\n    def close_clients(self):\n        for transport in self._clients.copy():\n            transport.close()\n\n    def abort_clients(self):\n        for transport in self._clients.copy():\n            transport.abort()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        \"\"\"Wait until server is closed and all connections are dropped.\n\n        - If the server is not closed, wait.\n        - If it is closed, but there are still active connections, wait.\n\n        Anyone waiting here will be unblocked once both conditions\n        (server is closed and all connections have been dropped)\n        have become true, in either order.\n\n        Historical note: In 3.11 and before, this was broken, returning\n        immediately if the server was already closed, even if there\n        were still active connections. An attempted fix in 3.12.0 was\n        still broken, returning immediately if the server was still\n        open and there were no active connections. Hopefully in 3.12.1\n        we have it right.\n        \"\"\"\n        # Waiters are unblocked by self._wakeup(), which is called\n        # from two places: self.close() and self._detach(), but only\n        # when both conditions have become true. To signal that this\n        # has happened, self._wakeup() sets self._waiters to None.\n        if self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # The preserved state of async generator hooks.\n        self._old_agen_hooks = None\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n        # Set to True when `loop.shutdown_default_executor` is called.\n        self._executor_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None, context=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name, context=context)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            if context is None:\n                # Use legacy API if context is not needed\n                task = self._task_factory(self, coro)\n            else:\n                task = self._task_factory(self, coro, context=context)\n\n            task.set_name(name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _check_default_executor(self):\n        if self._executor_shutdown_called:\n            raise RuntimeError('Executor shutdown has been called')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    async def shutdown_default_executor(self, timeout=None):\n        \"\"\"Schedule the shutdown of the default executor.\n\n        The timeout parameter specifies the amount of time the executor will\n        be given to finish joining. The default value is None, which means\n        that the executor will be given an unlimited amount of time.\n        \"\"\"\n        self._executor_shutdown_called = True\n        if self._default_executor is None:\n            return\n        future = self.create_future()\n        thread = threading.Thread(target=self._do_shutdown, args=(future,))\n        thread.start()\n        try:\n            async with timeouts.timeout(timeout):\n                await future\n        except TimeoutError:\n            warnings.warn(\"The executor did not finishing joining \"\n                          f\"its threads within {timeout} seconds.\",\n                          RuntimeWarning, stacklevel=2)\n            self._default_executor.shutdown(wait=False)\n        else:\n            thread.join()\n\n    def _do_shutdown(self, future):\n        try:\n            self._default_executor.shutdown(wait=True)\n            if not self.is_closed():\n                self.call_soon_threadsafe(futures._set_result_unless_cancelled,\n                                          future, None)\n        except Exception as ex:\n            if not self.is_closed() and not future.cancelled():\n                self.call_soon_threadsafe(future.set_exception, ex)\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def _run_forever_setup(self):\n        \"\"\"Prepare the run loop to process events.\n\n        This method exists so that custom custom event loop subclasses (e.g., event loops\n        that integrate a GUI event loop with Python's event loop) have access to all the\n        loop setup logic.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n\n        self._old_agen_hooks = sys.get_asyncgen_hooks()\n        self._thread_id = threading.get_ident()\n        sys.set_asyncgen_hooks(\n            firstiter=self._asyncgen_firstiter_hook,\n            finalizer=self._asyncgen_finalizer_hook\n        )\n\n        events._set_running_loop(self)\n\n    def _run_forever_cleanup(self):\n        \"\"\"Clean up after an event loop finishes the looping over events.\n\n        This method exists so that custom custom event loop subclasses (e.g., event loops\n        that integrate a GUI event loop with Python's event loop) have access to all the\n        loop cleanup logic.\n        \"\"\"\n        self._stopping = False\n        self._thread_id = None\n        events._set_running_loop(None)\n        self._set_coroutine_origin_tracking(False)\n        # Restore any pre-existing async generator hooks.\n        if self._old_agen_hooks is not None:\n            sys.set_asyncgen_hooks(*self._old_agen_hooks)\n            self._old_agen_hooks = None\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        try:\n            self._run_forever_setup()\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._run_forever_cleanup()\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        self._executor_shutdown_called = True\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it is undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        if delay is None:\n            raise TypeError('delay must not be None')\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        if when is None:\n            raise TypeError(\"when cannot be None\")\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            # Only check when the default executor is being used\n            self._check_default_executor()\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor(\n                    thread_name_prefix='asyncio'\n                )\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            raise TypeError('executor must be ThreadPoolExecutor instance')\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        _check_ssl_socket(sock)\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            f\"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for lfamily, _, _, _, laddr in local_addr_infos:\n                    # skip local addresses of different family\n                    if lfamily != family:\n                        continue\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: {str(exc).lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    if my_exceptions:\n                        raise my_exceptions.pop()\n                    else:\n                        raise OSError(f\"no matching local address with {family=} found\")\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n        finally:\n            exceptions = my_exceptions = None\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None,\n            all_errors=False):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                try:\n                    if all_errors:\n                        raise ExceptionGroup(\"create_connection failed\", exceptions)\n                    if len(exceptions) == 1:\n                        raise exceptions[0]\n                    else:\n                        # If they all have the same str(), raise one.\n                        model = str(exceptions[0])\n                        if all(str(exc) == model for exc in exceptions):\n                            raise exceptions[0]\n                        # Raise a combined exception so the user can see all\n                        # the various error messages.\n                        raise OSError('Multiple exceptions: {}'.format(\n                            ', '.join(str(exc) for exc in exceptions)))\n                finally:\n                    exceptions = None\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout,\n                ssl_shutdown_timeout=ssl_shutdown_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type == socket.SOCK_STREAM:\n                raise ValueError(\n                    f'A datagram socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        if not (isinstance(addr, tuple) and len(addr) == 2):\n                            raise TypeError('2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            keep_alive=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == \"posix\" and sys.platform != \"cygwin\"\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if keep_alive:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_KEEPALIVE, True)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        msg = ('error while attempting '\n                               'to bind on address %r: %s'\n                               % (sa, str(err).lower()))\n                        if err.errno == errno.EADDRNOTAVAIL:\n                            # Assume the family is not enabled (bpo-30945)\n                            sockets.pop()\n                            sock.close()\n                            if self._debug:\n                                logger.warning(msg)\n                            continue\n                        raise OSError(err.errno, msg) from None\n\n                if not sockets:\n                    raise OSError('could not bind on any address out of %r'\n                                  % ([info[4] for info in infos],))\n\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout,\n                        ssl_shutdown_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if ssl_shutdown_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_shutdown_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                ctx = None\n                thing = context.get(\"task\")\n                if thing is None:\n                    # Even though Futures don't have a context,\n                    # Task is a subclass of Future,\n                    # and sometimes the 'future' key holds a Task.\n                    thing = context.get(\"future\")\n                if thing is None:\n                    # Handles also have a context.\n                    thing = context.get(\"handle\")\n                if thing is not None and hasattr(thing, \"get_context\"):\n                    ctx = thing.get_context()\n                if ctx is not None and hasattr(ctx, \"run\"):\n                    ctx.run(self._exception_handler, self, context)\n                else:\n                    self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _ready.\"\"\"\n        if not handle._cancelled:\n            self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            timeout = self._scheduled[0]._when - self.time()\n            if timeout > MAXIMUM_SELECT_TIMEOUT:\n                timeout = MAXIMUM_SELECT_TIMEOUT\n            elif timeout < 0:\n                timeout = 0\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n        # Needed to break cycles when an exception occurs.\n        event_list = None\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 2052], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n# Contains code from https://github.com/MagicStack/uvloop/tree/v0.16.0\n# SPDX-License-Identifier: PSF-2.0 AND (MIT OR Apache-2.0)\n# SPDX-FileCopyrightText: Copyright (c) 2015-2021 MagicStack Inc.  http://magic.io\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args,\n                debug=self._loop.get_debug()))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def get_context(self):\n        return self._context\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args,\n                debug=self._loop.get_debug())\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def close_clients(self):\n        \"\"\"Close all active connections.\"\"\"\n        raise NotImplementedError\n\n    def abort_clients(self):\n        \"\"\"Close all active connections immediately.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args, context=None):\n        return self.call_later(0, callback, *args, context=context)\n\n    def call_later(self, delay, callback, *args, context=None):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args, context=None):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None, context=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            keep_alive=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        keep_alive set to True keeps connections active by enabling the\n        periodic transmission of messages.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL shutdown procedure\n        before aborting the connection. Default is 30s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None,\n                        ssl_shutdown_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file system path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        ssl_shutdown_timeout is the time in seconds that an SSL server\n        will wait for the SSL shutdown to finish (defaults to 30s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None,\n            ssl_shutdown_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio, but use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_recvfrom(self, sock, bufsize):\n        raise NotImplementedError\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_sendto(self, sock, data, address):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the AbstractEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                threading.current_thread() is threading.main_thread()):\n            stacklevel = 2\n            try:\n                f = sys._getframe(1)\n            except AttributeError:\n                pass\n            else:\n                # Move up the call stack so that the warning is attached\n                # to the line outside asyncio itself.\n                while f:\n                    module = f.f_globals.get('__name__')\n                    if not (module == 'asyncio' or module.startswith('asyncio.')):\n                        break\n                    f = f.f_back\n                    stacklevel += 1\n            import warnings\n            warnings.warn('There is no current event loop',\n                          DeprecationWarning, stacklevel=stacklevel)\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        if loop is not None and not isinstance(loop, AbstractEventLoop):\n            raise TypeError(f\"loop must be an instance of AbstractEventLoop or None, not '{type(loop).__name__}'\")\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    if policy is not None and not isinstance(policy, AbstractEventLoopPolicy):\n        raise TypeError(f\"policy must be an instance of AbstractEventLoopPolicy or None, not '{type(policy).__name__}'\")\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n\n\nif hasattr(os, 'fork'):\n    def on_fork():\n        # Reset the loop and wakeupfd in the forked child process.\n        if _event_loop_policy is not None:\n            _event_loop_policy._local = BaseDefaultEventLoopPolicy._Local()\n        _set_running_loop(None)\n        signal.set_wakeup_fd(-1)\n\n    os.register_at_fork(after_in_child=on_fork)\n", 882], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    'create_eager_task_factory', 'eager_task_factory',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport math\nimport types\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import queues\nfrom . import timeouts\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # capturing the set of eager tasks first, so if an eager task \"graduates\"\n    # to a regular task in another thread, we don't risk missing it.\n    eager_tasks = list(_eager_tasks)\n    # Looping over the WeakSet isn't safe as it can be updated from another\n    # thread, therefore we cast it to list prior to filtering. The list cast\n    # itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur).\n    # See issues 34970 and 36607 for details.\n    scheduled_tasks = None\n    i = 0\n    while True:\n        try:\n            scheduled_tasks = list(_scheduled_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in itertools.chain(scheduled_tasks, eager_tasks)\n            if futures._get_loop(t) is loop and not t.done()}\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    # _fut_waiter is either None or a Future.  The Future\n    # can be either done() or not done().\n    # The task can be in any of 3 states:\n    #\n    # - 1: _fut_waiter is not None and not _fut_waiter.done():\n    #      __step() is *not* scheduled and the Task is waiting for _fut_waiter.\n    # - 2: (_fut_waiter is None or _fut_waiter.done()) and __step() is scheduled:\n    #       the Task is waiting for __step() to be executed.\n    # - 3:  _fut_waiter is None and __step() is *not* scheduled:\n    #       the Task is currently executing (in __step()).\n    #\n    # * In state 1, one of the callbacks of __fut_waiter must be __wakeup().\n    # * The transition from 1 to 2 happens when _fut_waiter becomes done(),\n    #   as it schedules __wakeup() to be called (which calls __step() so\n    #   we way that __step() is scheduled).\n    # * It transitions from 2 to 3 when __step() is executed, and it clears\n    #   _fut_waiter to None.\n\n    # If False, don't log a message if the task is destroyed while its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None, context=None,\n                 eager_start=False):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._num_cancels_requested = 0\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        if context is None:\n            self._context = contextvars.copy_context()\n        else:\n            self._context = context\n\n        if eager_start and self._loop.is_running():\n            self.__eager_start()\n        else:\n            self._loop.call_soon(self.__step, context=self._context)\n            _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __repr__(self):\n        return base_tasks._task_repr(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_context(self):\n        return self._context\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n\n        This also increases the task's count of cancellation requests.\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        self._num_cancels_requested += 1\n        # These two lines are controversial.  See discussion starting at\n        # https://github.com/python/cpython/pull/31394#issuecomment-1053545331\n        # Also remember that this is duplicated in _asynciomodule.c.\n        # if self._num_cancels_requested > 1:\n        #     return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def cancelling(self):\n        \"\"\"Return the count of the task's cancellation requests.\n\n        This count is incremented when .cancel() is called\n        and may be decremented using .uncancel().\n        \"\"\"\n        return self._num_cancels_requested\n\n    def uncancel(self):\n        \"\"\"Decrement the task's count of cancellation requests.\n\n        This should be called by the party that called `cancel()` on the task\n        beforehand.\n\n        Returns the remaining number of cancellation requests.\n        \"\"\"\n        if self._num_cancels_requested > 0:\n            self._num_cancels_requested -= 1\n            if self._num_cancels_requested == 0:\n                self._must_cancel = False\n        return self._num_cancels_requested\n\n    def __eager_start(self):\n        prev_task = _swap_current_task(self._loop, self)\n        try:\n            _register_eager_task(self)\n            try:\n                self._context.run(self.__step_run_and_handle_result, None)\n            finally:\n                _unregister_eager_task(self)\n        finally:\n            try:\n                curtask = _swap_current_task(self._loop, prev_task)\n                assert curtask is self\n            finally:\n                if self.done():\n                    self._coro = None\n                    self = None  # Needed to break cycles when an exception occurs.\n                else:\n                    _register_task(self)\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        try:\n            self.__step_run_and_handle_result(exc)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __step_run_and_handle_result(self, exc):\n        coro = self._coro\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None, context=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    if context is None:\n        # Use legacy API if context is not needed\n        task = loop.create_task(coro, name=name)\n    else:\n        task = loop.create_task(coro, name=name, context=context)\n\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures or Tasks given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of Tasks/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        raise TypeError(\"Passing coroutines is forbidden, use tasks explicitly.\")\n\n    loop = events.get_running_loop()\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    If the task suppresses the cancellation and returns a value instead,\n    that value is returned.\n\n    This function is a coroutine.\n    \"\"\"\n    # The special case for timeout <= 0 is for the following case:\n    #\n    # async def test_waitfor():\n    #     func_started = False\n    #\n    #     async def func():\n    #         nonlocal func_started\n    #         func_started = True\n    #\n    #     try:\n    #         await asyncio.wait_for(func(), 0)\n    #     except asyncio.TimeoutError:\n    #         assert not func_started\n    #     else:\n    #         assert False\n    #\n    # asyncio.run(test_waitfor())\n\n\n    if timeout is not None and timeout <= 0:\n        fut = ensure_future(fut)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise TimeoutError from exc\n\n    async with timeouts.timeout(timeout):\n        return await fut\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    loop = events.get_running_loop()\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\nclass _AsCompletedIterator:\n    \"\"\"Iterator of awaitables representing tasks of asyncio.as_completed.\n\n    As an asynchronous iterator, iteration yields futures as they finish. As a\n    plain iterator, new coroutines are yielded that will return or raise the\n    result of the next underlying future to complete.\n    \"\"\"\n    def __init__(self, aws, timeout):\n        self._done = queues.Queue()\n        self._timeout_handle = None\n\n        loop = events.get_event_loop()\n        todo = {ensure_future(aw, loop=loop) for aw in set(aws)}\n        for f in todo:\n            f.add_done_callback(self._handle_completion)\n        if todo and timeout is not None:\n            self._timeout_handle = (\n                loop.call_later(timeout, self._handle_timeout)\n            )\n        self._todo = todo\n        self._todo_left = len(todo)\n\n    def __aiter__(self):\n        return self\n\n    def __iter__(self):\n        return self\n\n    async def __anext__(self):\n        if not self._todo_left:\n            raise StopAsyncIteration\n        assert self._todo_left > 0\n        self._todo_left -= 1\n        return await self._wait_for_one()\n\n    def __next__(self):\n        if not self._todo_left:\n            raise StopIteration\n        assert self._todo_left > 0\n        self._todo_left -= 1\n        return self._wait_for_one(resolve=True)\n\n    def _handle_timeout(self):\n        for f in self._todo:\n            f.remove_done_callback(self._handle_completion)\n            self._done.put_nowait(None)  # Sentinel for _wait_for_one().\n        self._todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _handle_completion(self, f):\n        if not self._todo:\n            return  # _handle_timeout() was here first.\n        self._todo.remove(f)\n        self._done.put_nowait(f)\n        if not self._todo and self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n\n    async def _wait_for_one(self, resolve=False):\n        # Wait for the next future to be done and return it unless resolve is\n        # set, in which case return either the result of the future or raise\n        # an exception.\n        f = await self._done.get()\n        if f is None:\n            # Dummy value from _handle_timeout().\n            raise exceptions.TimeoutError\n        return f.result() if resolve else f\n\n\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Create an iterator of awaitables or their results in completion order.\n\n    Run the supplied awaitables concurrently. The returned object can be\n    iterated to obtain the results of the awaitables as they finish.\n\n    The object returned can be iterated as an asynchronous iterator or a plain\n    iterator. When asynchronous iteration is used, the originally-supplied\n    awaitables are yielded if they are tasks or futures. This makes it easy to\n    correlate previously-scheduled tasks with their results:\n\n        ipv4_connect = create_task(open_connection(\"127.0.0.1\", 80))\n        ipv6_connect = create_task(open_connection(\"::1\", 80))\n        tasks = [ipv4_connect, ipv6_connect]\n\n        async for earliest_connect in as_completed(tasks):\n            # earliest_connect is done. The result can be obtained by\n            # awaiting it or calling earliest_connect.result()\n            reader, writer = await earliest_connect\n\n            if earliest_connect is ipv6_connect:\n                print(\"IPv6 connection established.\")\n            else:\n                print(\"IPv4 connection established.\")\n\n    During asynchronous iteration, implicitly-created tasks will be yielded for\n    supplied awaitables that aren't tasks or futures.\n\n    When used as a plain iterator, each iteration yields a new coroutine that\n    returns the result or raises the exception of the next completed awaitable.\n    This pattern is compatible with Python versions older than 3.13:\n\n        ipv4_connect = create_task(open_connection(\"127.0.0.1\", 80))\n        ipv6_connect = create_task(open_connection(\"::1\", 80))\n        tasks = [ipv4_connect, ipv6_connect]\n\n        for next_connect in as_completed(tasks):\n            # next_connect is not one of the original task objects. It must be\n            # awaited to obtain the result value or raise the exception of the\n            # awaitable that finishes next.\n            reader, writer = await next_connect\n\n    A TimeoutError is raised if the timeout occurs before all awaitables are\n    done. This is raised by the async for loop during asynchronous iteration or\n    by the coroutines yielded during plain iteration.\n    \"\"\"\n    if inspect.isawaitable(fs):\n        raise TypeError(\n            f\"expects an iterable of awaitables, not {type(fs).__name__}\"\n        )\n\n    return _AsCompletedIterator(fs, timeout)\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    if math.isnan(delay):\n        raise ValueError(\"Invalid delay: NaN (not a number)\")\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    should_close = True\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            async def _wrap_awaitable(awaitable):\n                return await awaitable\n\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            should_close = False\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events.get_event_loop()\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError:\n        if should_close:\n            coro_or_future.close()\n        raise\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events.get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    done_futs = []\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            if fut.done():\n                done_futs.append(fut)\n            else:\n                fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    # Run done callbacks after GatheringFuture created so any post-processing\n    # can be performed at this point\n    # optimization: in the special case that *all* futures finished eagerly,\n    # this will effectively complete the gather eagerly, with the last\n    # callback setting the result (or exception) on outer before returning it\n    for fut in done_futs:\n        _done_callback(fut)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\ndef create_eager_task_factory(custom_task_constructor):\n    \"\"\"Create a function suitable for use as a task factory on an event-loop.\n\n        Example usage:\n\n            loop.set_task_factory(\n                asyncio.create_eager_task_factory(my_task_constructor))\n\n        Now, tasks created will be started immediately (rather than being first\n        scheduled to an event loop). The constructor argument can be any callable\n        that returns a Task-compatible object and has a signature compatible\n        with `Task.__init__`; it must have the `eager_start` keyword argument.\n\n        Most applications will use `Task` for `custom_task_constructor` and in\n        this case there's no need to call `create_eager_task_factory()`\n        directly. Instead the  global `eager_task_factory` instance can be\n        used. E.g. `loop.set_task_factory(asyncio.eager_task_factory)`.\n        \"\"\"\n\n    def factory(loop, coro, *, name=None, context=None):\n        return custom_task_constructor(\n            coro, loop=loop, name=name, context=context, eager_start=True)\n\n    return factory\n\n\neager_task_factory = create_eager_task_factory(Task)\n\n\n# Collectively these two sets hold references to the complete set of active\n# tasks. Eagerly executed tasks use a faster regular set as an optimization\n# but may graduate to a WeakSet if the task blocks on IO.\n_scheduled_tasks = weakref.WeakSet()\n_eager_tasks = set()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register an asyncio Task scheduled to run on an event loop.\"\"\"\n    _scheduled_tasks.add(task)\n\n\ndef _register_eager_task(task):\n    \"\"\"Register an asyncio Task about to be eagerly executed.\"\"\"\n    _eager_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _swap_current_task(loop, task):\n    prev_task = _current_tasks.get(loop)\n    if task is None:\n        del _current_tasks[loop]\n    else:\n        _current_tasks[loop] = task\n    return prev_task\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a completed, scheduled Task.\"\"\"\n    _scheduled_tasks.discard(task)\n\n\ndef _unregister_eager_task(task):\n    \"\"\"Unregister a task which finished its first eager step.\"\"\"\n    _eager_tasks.discard(task)\n\n\n_py_current_task = current_task\n_py_register_task = _register_task\n_py_register_eager_task = _register_eager_task\n_py_unregister_task = _unregister_task\n_py_unregister_eager_task = _unregister_eager_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n_py_swap_current_task = _swap_current_task\n\n\ntry:\n    from _asyncio import (_register_task, _register_eager_task,\n                          _unregister_task, _unregister_eager_task,\n                          _enter_task, _leave_task, _swap_current_task,\n                          _scheduled_tasks, _eager_tasks, _current_tasks,\n                          current_task)\nexcept ImportError:\n    pass\nelse:\n    _c_current_task = current_task\n    _c_register_task = _register_task\n    _c_register_eager_task = _register_eager_task\n    _c_unregister_task = _unregister_task\n    _c_unregister_eager_task = _unregister_eager_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n    _c_swap_current_task = _swap_current_task\n", 1118], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport warnings\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile',\n           'setprofile_all_threads','settrace_all_threads']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_joinable_thread = _thread.start_joinable_thread\n_daemon_threads_allowed = _thread.daemon_threads_allowed\n_allocate_lock = _thread.allocate_lock\n_LockType = _thread.LockType\n_thread_shutdown = _thread._shutdown\n_make_thread_handle = _thread._make_thread_handle\n_ThreadHandle = _thread._ThreadHandle\nget_ident = _thread.get_ident\n_get_main_thread_ident = _thread._get_main_thread_ident\n_is_main_interpreter = _thread._is_main_interpreter\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef setprofile_all_threads(func):\n    \"\"\"Set a profile function for all threads started from the threading module\n    and all Python threads that are currently executing.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n    \"\"\"\n    setprofile(func)\n    _sys._setprofileallthreads(func)\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef settrace_all_threads(func):\n    \"\"\"Set a trace function for all threads started from the threading module\n    and all Python threads that are currently executing.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n    \"\"\"\n    settrace(func)\n    _sys._settraceallthreads(func)\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _LockType\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if args or kwargs:\n        warnings.warn(\n            'Passing arguments to RLock is deprecated and will be removed in 3.15',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n    # Internal method used for reentrancy checks\n\n    def _recursion_count(self):\n        if self._owner != get_ident():\n            return 0\n        return self._count\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        if hasattr(lock, '_release_save'):\n            self._release_save = lock._release_save\n        if hasattr(lock, '_acquire_restore'):\n            self._acquire_restore = lock._acquire_restore\n        if hasattr(lock, '_is_owned'):\n            self._is_owned = lock._is_owned\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            self._cond.notify(n)\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        super().__init__(value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            self._cond.notify(n)\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._after_fork()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        if parties < 1:\n            raise ValueError(\"parties must be > 0\")\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            if daemon and not _daemon_threads_allowed():\n                raise RuntimeError('daemon threads are disabled in this (sub)interpreter')\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._handle = _ThreadHandle()\n        self._started = Event()\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _after_fork(self, new_ident=None):\n        # Private!  Called by threading._after_fork().\n        self._started._at_fork_reinit()\n        if new_ident is not None:\n            # This thread is alive.\n            self._ident = new_ident\n            assert self._handle.ident == new_ident\n        else:\n            # Otherwise, the thread is dead, Jim.  _PyThread_AfterFork()\n            # already marked our handle done.\n            pass\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        if self._handle.is_done():\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            # Start joinable thread\n            _start_joinable_thread(self._bootstrap, handle=self._handle,\n                                   daemon=self.daemon)\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()  # Will set ident and native_id\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating-point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        # the behavior of a negative timeout isn't documented, but\n        # historically .join(timeout=x) for x<0 has acted as if timeout=0\n        if timeout is not None:\n            timeout = max(timeout, 0)\n\n        self._handle.join(timeout)\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._started.is_set() and not self._handle.is_done()\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if daemonic and not _daemon_threads_allowed():\n            raise RuntimeError('daemon threads are disabled in this interpreter')\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._started.set()\n        self._ident = _get_main_thread_ident()\n        self._handle = _make_thread_handle(self._ident)\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Helper thread-local instance to detect when a _DummyThread\n# is collected. Not a part of the public API.\n_thread_local_info = local()\n\n\nclass _DeleteDummyThreadOnDel:\n    '''\n    Helper class to remove a dummy thread from threading._active on __del__.\n    '''\n\n    def __init__(self, dummy_thread):\n        self._dummy_thread = dummy_thread\n        self._tident = dummy_thread.ident\n        # Put the thread on a thread local variable so that when\n        # the related thread finishes this instance is collected.\n        #\n        # Note: no other references to this instance may be created.\n        # If any client code creates a reference to this instance,\n        # the related _DummyThread will be kept forever!\n        _thread_local_info._track_dummy_thread_ref = self\n\n    def __del__(self):\n        with _active_limbo_lock:\n            if _active.get(self._tident) is self._dummy_thread:\n                _active.pop(self._tident, None)\n\n\n# Dummy thread class to represent threads not started here.\n# These should be added to `_active` and removed automatically\n# when they die, although they can't be waited for.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"),\n                        daemon=_daemon_threads_allowed())\n        self._started.set()\n        self._set_ident()\n        self._handle = _make_thread_handle(self._ident)\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n        _DeleteDummyThreadOnDel(self)\n\n    def is_alive(self):\n        if not self._handle.is_done() and self._started.is_set():\n            return True\n        raise RuntimeError(\"thread is not alive\")\n\n    def join(self, timeout=None):\n        raise RuntimeError(\"cannot join a dummy thread\")\n\n    def _after_fork(self, new_ident=None):\n        if new_ident is not None:\n            self.__class__ = _MainThread\n            self._name = 'MainThread'\n            self._daemonic = False\n        Thread._after_fork(self, new_ident=new_ident)\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    # NOTE: if the logic in here ever changes, update Modules/posixmodule.c\n    # warn_about_fork_with_threads() to match.\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    _threading_atexits.append(lambda: func(*arg, **kwargs))\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure: other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it. We can't wait for it to be marked as done\n    # normally - that won't happen until the interpreter is nearly dead. So\n    # mark it done here.\n    if _main_thread._handle.is_done() and _is_main_interpreter():\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    if _is_main_interpreter():\n        _main_thread._handle._set_done()\n\n    # Wait for all non-daemon threads to exit.\n    _thread_shutdown()\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    # XXX Figure this out for subinterpreters.  (See gh-75698.)\n    return _main_thread\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # This is the one and only active thread.\n                ident = get_ident()\n                thread._after_fork(new_ident=ident)\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._after_fork()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1599], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py": ["# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Literal\n\nfrom marimo._messaging.cell_output import CellChannel, CellOutput\nfrom marimo._messaging.mimetypes import ConsoleMimeType\nfrom marimo._types.ids import CellId_t\n\nif TYPE_CHECKING:\n    from collections import deque\n    from threading import Condition\n    from typing import Optional\n\n    from marimo._messaging.types import Stream\n\nStreamT = Literal[CellChannel.STDERR, CellChannel.STDOUT, CellChannel.STDIN]\n\n# Flush console outputs every 10ms\nTIMEOUT_S = 0.01\n\n\n@dataclass\nclass ConsoleMsg:\n    stream: StreamT\n    cell_id: CellId_t\n    data: str\n    mimetype: ConsoleMimeType\n\n\ndef _write_console_output(\n    stream: Stream,\n    stream_type: StreamT,\n    cell_id: CellId_t,\n    data: str,\n    mimetype: ConsoleMimeType,\n) -> None:\n    from marimo._messaging.ops import CellOp\n\n    CellOp(\n        cell_id=cell_id,\n        console=CellOutput(\n            channel=stream_type,\n            mimetype=mimetype,\n            data=data,\n        ),\n    ).broadcast(stream)\n\n\ndef _can_merge_outputs(first: ConsoleMsg, second: ConsoleMsg) -> bool:\n    return first.stream == second.stream and first.mimetype == second.mimetype\n\n\ndef _add_output_to_buffer(\n    console_output: ConsoleMsg,\n    outputs_buffered_per_cell: dict[CellId_t, list[ConsoleMsg]],\n) -> None:\n    cell_id = console_output.cell_id\n    buffer = (\n        outputs_buffered_per_cell[cell_id]\n        if cell_id in outputs_buffered_per_cell\n        else None\n    )\n    if buffer and _can_merge_outputs(buffer[-1], console_output):\n        buffer[-1].data += console_output.data\n    elif buffer:\n        buffer.append(console_output)\n    else:\n        outputs_buffered_per_cell[cell_id] = [console_output]\n\n\ndef buffered_writer(\n    msg_queue: deque[ConsoleMsg | None],\n    stream: Stream,\n    cv: Condition,\n) -> None:\n    \"\"\"\n    Writes standard out and standard error to frontend in batches\n\n    Buffers console messages, writing them out in batches. A condition\n    variable is used to synchronize access to `msg_queue`, and to obtain\n    notifications when messages have been added. (A deque + condition variable\n    was noticeably faster than the builtin queue.Queue in testing.)\n\n    A `None` passed to `msg_queue` signals the writer should terminate.\n    \"\"\"\n\n    # only have a non-None timer when there's at least one output buffered\n    #\n    # when the timer expires, all buffered outputs are flushed\n    timer: Optional[float] = None\n\n    outputs_buffered_per_cell: dict[CellId_t, list[ConsoleMsg]] = {}\n    while True:\n        with cv:\n            # We wait for messages until the timer (if any) expires\n            while timer is None or timer > 0:\n                time_started_waiting = time.time()\n                # if the timer is set or if the message queue is empty, wait;\n                # otherwise, no timer is set but we received a message, so\n                # process it\n                if timer is not None or not msg_queue:\n                    cv.wait(timeout=timer)\n                while msg_queue:\n                    msg = msg_queue.popleft()\n                    if msg is None:\n                        return\n                    _add_output_to_buffer(msg, outputs_buffered_per_cell)\n                if outputs_buffered_per_cell and timer is None:\n                    # start the timeout timer\n                    timer = TIMEOUT_S\n                elif timer is not None:\n                    time_waited = time.time() - time_started_waiting\n                    timer -= time_waited\n\n        # the timer has expired: flush the outputs\n        for cell_id, buffer in outputs_buffered_per_cell.items():\n            for output in buffer:\n                _write_console_output(\n                    stream,\n                    output.stream,\n                    cell_id,\n                    output.data,\n                    output.mimetype,\n                )\n        outputs_buffered_per_cell = {}\n        timer = None\n", 129], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/cell_output.py": ["# Copyright 2024 Marimo. All rights reserved.\n\"\"\"Specification of a cell's visual output\"\"\"\n\nfrom __future__ import annotations\n\nimport time\nfrom dataclasses import asdict, dataclass, field\nfrom enum import Enum\nfrom typing import Any, Union\n\nfrom marimo._messaging.errors import Error\nfrom marimo._messaging.mimetypes import ConsoleMimeType, KnownMimeType\n\n\nclass CellChannel(str, Enum):\n    \"\"\"The channel of a cell's output.\"\"\"\n\n    STDOUT = \"stdout\"\n    STDERR = \"stderr\"\n    STDIN = \"stdin\"\n    PDB = \"pdb\"\n    OUTPUT = \"output\"\n    MARIMO_ERROR = \"marimo-error\"\n    MEDIA = \"media\"\n\n    def __repr__(self) -> str:\n        return self.value\n\n\n@dataclass\nclass CellOutput:\n    # descriptive name about the kind of output: e.g., stdout, stderr, ...\n    channel: CellChannel\n    mimetype: KnownMimeType\n    data: Union[str, list[Error], dict[str, Any]]\n    timestamp: float = field(default_factory=lambda: time.time())\n\n    def __repr__(self) -> str:\n        return f\"CellOutput(channel={self.channel}, mimetype={self.mimetype}, timestamp={self.timestamp})\"\n\n    def asdict(self) -> dict[str, Any]:\n        return asdict(self)\n\n    @staticmethod\n    def stdout(\n        data: str, mimetype: ConsoleMimeType = \"text/plain\"\n    ) -> CellOutput:\n        return CellOutput(\n            channel=CellChannel.STDOUT,\n            mimetype=mimetype,\n            data=data,\n        )\n\n    @staticmethod\n    def stderr(\n        data: str, mimetype: ConsoleMimeType = \"text/plain\"\n    ) -> CellOutput:\n        return CellOutput(\n            channel=CellChannel.STDERR,\n            mimetype=mimetype,\n            data=data,\n        )\n\n    @staticmethod\n    def stdin(data: str) -> CellOutput:\n        return CellOutput(\n            channel=CellChannel.STDIN, mimetype=\"text/plain\", data=data\n        )\n\n    @staticmethod\n    def empty() -> CellOutput:\n        return CellOutput(\n            channel=CellChannel.OUTPUT,\n            mimetype=\"text/plain\",\n            data=\"\",\n        )\n\n    @staticmethod\n    def errors(data: list[Error]) -> CellOutput:\n        return CellOutput(\n            channel=CellChannel.MARIMO_ERROR,\n            mimetype=\"application/vnd.marimo+error\",\n            data=data,\n        )\n", 84], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py": ["# Copyright 2024 Marimo. All rights reserved.\n\"\"\"Message Types\n\nMessages that the kernel sends to the frontend.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport sys\nimport time\nfrom collections.abc import Sequence  # noqa: TC003\nfrom dataclasses import asdict, dataclass, field\nfrom types import ModuleType\nfrom typing import (\n    Any,\n    ClassVar,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import uuid4\n\nfrom marimo import _loggers as loggers\nfrom marimo._ast.app_config import _AppConfig\nfrom marimo._ast.cell import CellConfig, RuntimeStateType\nfrom marimo._ast.toplevel import TopLevelHints, TopLevelStatus\nfrom marimo._data.models import (\n    ColumnStats,\n    DataSourceConnection,\n    DataTable,\n    DataTableSource,\n)\nfrom marimo._dependencies.dependencies import DependencyManager\nfrom marimo._messaging.cell_output import CellChannel, CellOutput\nfrom marimo._messaging.completion_option import CompletionOption\nfrom marimo._messaging.context import RUN_ID_CTX, RunId_t\nfrom marimo._messaging.errors import (\n    Error,\n    MarimoInternalError,\n    is_sensitive_error,\n)\nfrom marimo._messaging.mimetypes import KnownMimeType\nfrom marimo._messaging.streams import output_max_bytes\nfrom marimo._messaging.types import Stream\nfrom marimo._messaging.variables import get_variable_preview\nfrom marimo._output.hypertext import Html\nfrom marimo._plugins.core.json_encoder import WebComponentEncoder\nfrom marimo._plugins.core.web_component import JSONType\nfrom marimo._plugins.ui._core.ui_element import UIElement\nfrom marimo._runtime.context import get_context\nfrom marimo._runtime.context.types import ContextNotInitializedError\nfrom marimo._runtime.context.utils import get_mode\nfrom marimo._runtime.layout.layout import LayoutConfig\nfrom marimo._secrets.models import SecretKeysWithProvider\nfrom marimo._types.ids import CellId_t, RequestId, WidgetModelId\nfrom marimo._utils.platform import is_pyodide, is_windows\n\nLOGGER = loggers.marimo_logger()\n\n\ndef serialize(datacls: Any) -> dict[str, JSONType]:\n    # TODO(akshayka): maybe serialize as bytes (JSON), not objects ...,\n    # then `send_bytes` over connection ... to try to avoid pickling\n    # issues\n    try:\n        # Try to serialize as a dataclass\n        return cast(\n            dict[str, JSONType],\n            asdict(datacls),\n        )\n    except Exception:\n        # If that fails, try to serialize using the WebComponentEncoder\n        return cast(\n            dict[str, JSONType],\n            json.loads(WebComponentEncoder.json_dumps(datacls)),\n        )\n\n\n@dataclass\nclass Op:\n    name: ClassVar[str]\n\n    # TODO(akshayka): fix typing once mypy has stricter typing for asdict\n    def broadcast(self, stream: Optional[Stream] = None) -> None:\n        if stream is None:\n            try:\n                ctx = get_context()\n            except ContextNotInitializedError:\n                LOGGER.debug(\"No context initialized.\")\n                return\n            else:\n                stream = ctx.stream\n\n        try:\n            stream.write(op=self.name, data=self.serialize())\n        except Exception as e:\n            LOGGER.exception(\n                \"Error serializing op %s: %s\",\n                self.__class__.__name__,\n                e,\n            )\n            return\n\n    def serialize(self) -> dict[str, Any]:\n        return serialize(self)\n\n\n@dataclass\nclass CellOp(Op):\n    \"\"\"Op to transition a cell.\n\n    A CellOp's data has some optional fields:\n\n    output        - a CellOutput\n    console       - a CellOutput (console msg to append), or a list of\n                    CellOutputs\n    status        - execution status\n    stale_inputs  - whether the cell has stale inputs (variables, modules, ...)\n    run_id        - the run associated with this cell.\n    serialization - the serialization status of the cell\n\n    Omitting a field means that its value should be unchanged!\n\n    And one required field:\n\n    cell_id - the cell id\n    \"\"\"\n\n    name: ClassVar[str] = \"cell-op\"\n    cell_id: CellId_t\n    output: Optional[CellOutput] = None\n    console: Optional[Union[CellOutput, list[CellOutput]]] = None\n    status: Optional[RuntimeStateType] = None\n    stale_inputs: Optional[bool] = None\n    run_id: Optional[RunId_t] = None\n    serialization: Optional[str] = None\n    timestamp: float = field(default_factory=lambda: time.time())\n\n    def __post_init__(self) -> None:\n        if self.run_id is not None:\n            return\n\n        # We currently don't support tracing for replayed cell ops (previous session runs)\n        if self.status == \"idle\":\n            self.run_id = None\n            return\n\n        try:\n            self.run_id = RUN_ID_CTX.get()\n        except LookupError:\n            # Be specific about the exception we're catching\n            # The context variable hasn't been set yet\n            self.run_id = None\n        except Exception as e:\n            LOGGER.error(\"Error getting run id: %s\", str(e))\n            self.run_id = None\n\n    @staticmethod\n    def maybe_truncate_output(\n        mimetype: KnownMimeType, data: str\n    ) -> tuple[KnownMimeType, str]:\n        if (size := sys.getsizeof(data)) > output_max_bytes():\n            from marimo._output.md import md\n            from marimo._plugins.stateless.callout import callout\n\n            text = f\"\"\"\n                <span class=\"text-error\">**Your output is too large**</span>\n\n                Your output is too large for marimo to show. It has a size\n                of {size} bytes. Did you output this object by accident?\n\n                If this limitation is a problem for you, you can configure\n                the max output size by adding (eg)\n\n                ```\n                [tool.marimo.runtime]\n                output_max_bytes = 10_000_000\n                ```\n\n                to your pyproject.toml, or with the environment variable\n                `MARIMO_OUTPUT_MAX_BYTES`:\n\n                ```\n                export MARIMO_OUTPUT_MAX_BYTES=10_000_000\n                ```\n\n                Increasing the max output size may cause performance issues.\n                If you run into problems, please reach out\n                to us on [Discord](https://marimo.io/discord?ref=app) or\n                [GitHub](https://github.com/marimo-team/marimo/issues).\n                \"\"\"\n\n            warning = callout(\n                md(text),\n                kind=\"warn\",\n            )\n            mimetype, data = warning._mime_()\n        return mimetype, data\n\n    @staticmethod\n    def broadcast_output(\n        channel: CellChannel,\n        mimetype: KnownMimeType,\n        data: str,\n        cell_id: Optional[CellId_t],\n        status: Optional[RuntimeStateType],\n        stream: Stream | None = None,\n    ) -> None:\n        mimetype, data = CellOp.maybe_truncate_output(mimetype, data)\n        cell_id = (\n            cell_id if cell_id is not None else get_context().stream.cell_id\n        )\n        assert cell_id is not None\n        CellOp(\n            cell_id=cell_id,\n            output=CellOutput(\n                channel=channel,\n                mimetype=mimetype,\n                data=data,\n            ),\n            status=status,\n        ).broadcast(stream=stream)\n\n    @staticmethod\n    def broadcast_empty_output(\n        cell_id: Optional[CellId_t],\n        status: Optional[RuntimeStateType],\n        stream: Stream | None = None,\n    ) -> None:\n        cell_id = (\n            cell_id if cell_id is not None else get_context().stream.cell_id\n        )\n        assert cell_id is not None\n        CellOp(\n            cell_id=cell_id,\n            output=CellOutput.empty(),\n            status=status,\n        ).broadcast(stream=stream)\n\n    @staticmethod\n    def broadcast_console_output(\n        channel: CellChannel,\n        mimetype: KnownMimeType,\n        data: str,\n        cell_id: Optional[CellId_t],\n        status: Optional[RuntimeStateType],\n        stream: Stream | None = None,\n    ) -> None:\n        mimetype, data = CellOp.maybe_truncate_output(mimetype, data)\n        cell_id = (\n            cell_id if cell_id is not None else get_context().stream.cell_id\n        )\n        assert cell_id is not None\n        CellOp(\n            cell_id=cell_id,\n            console=CellOutput(\n                channel=channel,\n                mimetype=mimetype,\n                data=data,\n            ),\n            status=status,\n        ).broadcast(stream=stream)\n\n    @staticmethod\n    def broadcast_status(\n        cell_id: CellId_t,\n        status: RuntimeStateType,\n        stream: Stream | None = None,\n    ) -> None:\n        if status != \"running\":\n            CellOp(cell_id=cell_id, status=status).broadcast()\n        else:\n            # Console gets cleared on \"running\"\n            CellOp(cell_id=cell_id, console=[], status=status).broadcast(\n                stream=stream\n            )\n\n    @staticmethod\n    def broadcast_error(\n        data: Sequence[Error],\n        clear_console: bool,\n        cell_id: CellId_t,\n    ) -> None:\n        console: Optional[list[CellOutput]] = [] if clear_console else None\n\n        # In run mode, we don't want to broadcast the error. Instead we want to print the error to the console\n        # and then broadcast a new error such that the data is hidden.\n        safe_errors: list[Error] = []\n        if get_mode() == \"run\":\n            for error in data:\n                # Skip non-sensitive errors\n                if not is_sensitive_error(error):\n                    safe_errors.append(error)\n                    continue\n\n                error_id = uuid4()\n                LOGGER.error(\n                    f\"(error_id={error_id}) {error.describe()}\",\n                    extra={\"error_id\": error_id},\n                )\n                safe_errors.append(MarimoInternalError(error_id=str(error_id)))\n        else:\n            safe_errors = list(data)\n\n        CellOp(\n            cell_id=cell_id,\n            output=CellOutput.errors(safe_errors),\n            console=console,\n            status=None,\n        ).broadcast()\n\n    @staticmethod\n    def broadcast_stale(\n        cell_id: CellId_t, stale: bool, stream: Stream | None = None\n    ) -> None:\n        CellOp(cell_id=cell_id, stale_inputs=stale).broadcast(stream)\n\n    @staticmethod\n    def broadcast_serialization(\n        cell_id: CellId_t,\n        serialization: TopLevelStatus,\n        stream: Stream | None = None,\n    ) -> None:\n        status: Optional[TopLevelHints] = serialization.hint\n        CellOp(cell_id=cell_id, serialization=str(status)).broadcast(stream)\n\n\n@dataclass\nclass HumanReadableStatus:\n    \"\"\"Human-readable status.\"\"\"\n\n    code: Literal[\"ok\", \"error\"]\n    title: Union[str, None] = None\n    message: Union[str, None] = None\n\n\n@dataclass\nclass FunctionCallResult(Op):\n    \"\"\"Result of calling a function.\"\"\"\n\n    name: ClassVar[str] = \"function-call-result\"\n\n    function_call_id: RequestId\n    return_value: JSONType\n    status: HumanReadableStatus\n\n    def __post_init__(self) -> None:\n        # We want to serialize the return_value using our custom JSON encoder\n        try:\n            self.return_value = json.loads(\n                WebComponentEncoder.json_dumps(self.return_value)\n            )\n        except Exception as e:\n            LOGGER.exception(\n                \"Error serializing function call result %s: %s\",\n                self.__class__.__name__,\n                e,\n            )\n\n    def serialize(self) -> dict[str, Any]:\n        try:\n            return serialize(self)\n        except Exception as e:\n            LOGGER.exception(\n                \"Error serializing function call result %s: %s\",\n                self.__class__.__name__,\n                e,\n            )\n            return serialize(\n                FunctionCallResult(\n                    function_call_id=self.function_call_id,\n                    return_value=None,\n                    status=HumanReadableStatus(\n                        code=\"error\",\n                        title=\"Error calling function\",\n                        message=\"Failed to serialize function call result\",\n                    ),\n                )\n            )\n\n\n@dataclass\nclass RemoveUIElements(Op):\n    \"\"\"Invalidate UI elements for a given cell.\"\"\"\n\n    name: ClassVar[str] = \"remove-ui-elements\"\n    cell_id: CellId_t\n\n\n@dataclass\nclass SendUIElementMessage(Op):\n    \"\"\"Send a message to a UI element.\"\"\"\n\n    name: ClassVar[str] = \"send-ui-element-message\"\n    ui_element: Optional[str]\n    model_id: Optional[WidgetModelId]\n    message: dict[str, Any]\n    buffers: Optional[list[str]] = None\n\n\n@dataclass\nclass Interrupted(Op):\n    \"\"\"Written when the kernel is interrupted by the user.\"\"\"\n\n    name: ClassVar[str] = \"interrupted\"\n\n\n@dataclass\nclass CompletedRun(Op):\n    \"\"\"Written on run completion (of submitted cells and their descendants.\"\"\"\n\n    name: ClassVar[str] = \"completed-run\"\n\n\n@dataclass\nclass KernelCapabilities:\n    terminal: bool = False\n    pylsp: bool = False\n\n    def __post_init__(self) -> None:\n        # Only available in mac/linux\n        self.terminal = not is_windows() and not is_pyodide()\n        self.pylsp = DependencyManager.pylsp.has()\n\n\n@dataclass\nclass KernelReady(Op):\n    \"\"\"Kernel is ready for execution.\"\"\"\n\n    name: ClassVar[str] = \"kernel-ready\"\n    cell_ids: tuple[CellId_t, ...]\n    codes: tuple[str, ...]\n    names: tuple[str, ...]\n    layout: Optional[LayoutConfig]\n    configs: tuple[CellConfig, ...]\n    # Whether the kernel was resumed from a previous session\n    resumed: bool\n    # If the kernel was resumed, the values of the UI elements\n    ui_values: Optional[dict[str, JSONType]]\n    # If the kernel was resumed, the last executed code for each cell\n    last_executed_code: Optional[dict[CellId_t, str]]\n    # If the kernel was resumed, the last execution time for each cell\n    last_execution_time: Optional[dict[CellId_t, float]]\n    # App config\n    app_config: _AppConfig\n    # Whether the kernel is kiosk mode\n    kiosk: bool\n    # Kernel capabilities\n    capabilities: KernelCapabilities\n\n\n@dataclass\nclass CompletionResult(Op):\n    \"\"\"Code completion result.\"\"\"\n\n    name: ClassVar[str] = \"completion-result\"\n    completion_id: str\n    prefix_length: int\n    options: list[CompletionOption]\n\n\n@dataclass\nclass Alert(Op):\n    name: ClassVar[str] = \"alert\"\n    title: str\n    # description may be HTML\n    description: str\n    variant: Optional[Literal[\"danger\"]] = None\n\n\n@dataclass\nclass MissingPackageAlert(Op):\n    name: ClassVar[str] = \"missing-package-alert\"\n    packages: list[str]\n    isolated: bool\n\n\n# package name => installation status\nPackageStatusType = dict[\n    str, Literal[\"queued\", \"installing\", \"installed\", \"failed\"]\n]\n\n\n@dataclass\nclass InstallingPackageAlert(Op):\n    name: ClassVar[str] = \"installing-package-alert\"\n    packages: PackageStatusType\n\n\n@dataclass\nclass Reconnected(Op):\n    name: ClassVar[str] = \"reconnected\"\n\n\n@dataclass\nclass Banner(Op):\n    name: ClassVar[str] = \"banner\"\n    title: str\n    # description may be HTML\n    description: str\n    variant: Optional[Literal[\"danger\"]] = None\n    action: Optional[Literal[\"restart\"]] = None\n\n\n@dataclass\nclass Reload(Op):\n    name: ClassVar[str] = \"reload\"\n\n\n@dataclass\nclass VariableDeclaration:\n    name: str\n    declared_by: list[CellId_t]\n    used_by: list[CellId_t]\n\n\n@dataclass\nclass VariableValue:\n    name: str\n    value: Optional[str]\n    datatype: Optional[str]\n\n    def __init__(\n        self, name: str, value: object, datatype: Optional[str] = None\n    ) -> None:\n        self.name = name\n\n        # Defensively try-catch attribute accesses, which could raise\n        # exceptions\n        # If datatype is already defined, don't try to infer it\n        if datatype is None:\n            try:\n                self.datatype = (\n                    type(value).__name__ if value is not None else None\n                )\n            except Exception:\n                self.datatype = datatype\n        else:\n            self.datatype = datatype\n\n        try:\n            self.value = self._format_value(value)\n        except Exception:\n            self.value = None\n\n    def _stringify(self, value: object) -> str:\n        MAX_STR_LEN = 50\n\n        if isinstance(value, str):\n            if len(value) > MAX_STR_LEN:\n                return value[:MAX_STR_LEN]\n            return value\n\n        try:\n            # str(value) can be slow for large objects\n            # or lead to large memory spikes\n            return get_variable_preview(value, max_str_len=MAX_STR_LEN)\n        except BaseException:\n            # Catch-all: some libraries like Polars have bugs and raise\n            # BaseExceptions, which shouldn't crash the kernel\n            return \"<UNKNOWN>\"\n\n    def _format_value(self, value: object) -> str:\n        resolved = value\n        if isinstance(value, UIElement):\n            resolved = value.value\n        elif isinstance(value, Html):\n            resolved = value.text\n        elif isinstance(value, ModuleType):\n            resolved = value.__name__\n        return self._stringify(resolved)\n\n\n@dataclass\nclass Variables(Op):\n    \"\"\"List of variable declarations.\"\"\"\n\n    name: ClassVar[str] = \"variables\"\n    variables: list[VariableDeclaration]\n\n\n@dataclass\nclass VariableValues(Op):\n    \"\"\"List of variables and their types/values.\"\"\"\n\n    name: ClassVar[str] = \"variable-values\"\n    variables: list[VariableValue]\n\n\n@dataclass\nclass Datasets(Op):\n    \"\"\"List of datasets.\"\"\"\n\n    name: ClassVar[str] = \"datasets\"\n    tables: list[DataTable]\n    clear_channel: Optional[DataTableSource] = None\n\n\n@dataclass\nclass SQLTablePreview(Op):\n    \"\"\"Preview of a table in a SQL database.\"\"\"\n\n    name: ClassVar[str] = \"sql-table-preview\"\n    request_id: RequestId\n    table: Optional[DataTable]\n    error: Optional[str] = None\n\n\n@dataclass\nclass SQLTableListPreview(Op):\n    \"\"\"Preview of a list of tables in a schema.\"\"\"\n\n    name: ClassVar[str] = \"sql-table-list-preview\"\n    request_id: RequestId\n    tables: list[DataTable] = field(default_factory=list)\n    error: Optional[str] = None\n\n\n@dataclass\nclass ColumnPreview:\n    chart_spec: Optional[str] = None\n    chart_code: Optional[str] = None\n    error: Optional[str] = None\n    missing_packages: Optional[list[str]] = None\n    stats: Optional[ColumnStats] = None\n\n\n# We shouldn't need to make table_name and column_name have default values.\n# We can use kw_only=True once we drop support for Python 3.9.\n@dataclass()\nclass DataColumnPreview(Op, ColumnPreview):\n    \"\"\"Preview of a column in a dataset.\"\"\"\n\n    name: ClassVar[str] = \"data-column-preview\"\n    table_name: str = \"\"\n    column_name: str = \"\"\n\n\n@dataclass\nclass DataSourceConnections(Op):\n    name: ClassVar[str] = \"data-source-connections\"\n    connections: list[DataSourceConnection]\n\n\n@dataclass\nclass QueryParamsSet(Op):\n    \"\"\"Set query parameters.\"\"\"\n\n    name: ClassVar[str] = \"query-params-set\"\n    key: str\n    value: Union[str, list[str]]\n\n\n@dataclass\nclass QueryParamsAppend(Op):\n    name: ClassVar[str] = \"query-params-append\"\n    key: str\n    value: str\n\n\n@dataclass\nclass QueryParamsDelete(Op):\n    name: ClassVar[str] = \"query-params-delete\"\n    key: str\n    # If value is None, delete all values for the key\n    # If a value is provided, only that value is deleted\n    value: Optional[str]\n\n\n@dataclass\nclass QueryParamsClear(Op):\n    # Clear all query parameters\n    name: ClassVar[str] = \"query-params-clear\"\n\n\n@dataclass\nclass FocusCell(Op):\n    name: ClassVar[str] = \"focus-cell\"\n    cell_id: CellId_t\n\n\n@dataclass\nclass UpdateCellCodes(Op):\n    name: ClassVar[str] = \"update-cell-codes\"\n    cell_ids: list[CellId_t]\n    codes: list[str]\n    # If true, this means the code was not run on the backend when updating\n    # the cell codes.\n    code_is_stale: bool\n\n\n@dataclass\nclass SecretKeysResult(Op):\n    \"\"\"Result of listing secret keys.\"\"\"\n\n    request_id: RequestId\n    name: ClassVar[str] = \"secret-keys-result\"\n    secrets: list[SecretKeysWithProvider]\n\n\n@dataclass\nclass UpdateCellIdsRequest(Op):\n    \"\"\"\n    Update the cell ID ordering of the cells in the notebook.\n\n    Right now we send the entire list of cell IDs,\n    but in the future we might want to send change-deltas.\n    \"\"\"\n\n    name: ClassVar[str] = \"update-cell-ids\"\n    cell_ids: list[CellId_t]\n\n\nMessageOperation = Union[\n    # Cell operations\n    CellOp,\n    FunctionCallResult,\n    SendUIElementMessage,\n    RemoveUIElements,\n    # Notebook operations\n    Reload,\n    Reconnected,\n    Interrupted,\n    CompletedRun,\n    KernelReady,\n    # Editor operations\n    CompletionResult,\n    # Alerts\n    Alert,\n    Banner,\n    MissingPackageAlert,\n    InstallingPackageAlert,\n    # Variables\n    Variables,\n    VariableValues,\n    # Query params\n    QueryParamsSet,\n    QueryParamsAppend,\n    QueryParamsDelete,\n    QueryParamsClear,\n    # Datasets\n    Datasets,\n    DataColumnPreview,\n    SQLTablePreview,\n    SQLTableListPreview,\n    DataSourceConnections,\n    # Secrets\n    SecretKeysResult,\n    # Kiosk specific\n    FocusCell,\n    UpdateCellCodes,\n    UpdateCellIdsRequest,\n]\n", 755], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py": ["import re\nimport sys\nimport copy\nimport types\nimport inspect\nimport keyword\nimport itertools\nimport abc\nfrom reprlib import recursive_repr\n\n\n__all__ = ['dataclass',\n           'field',\n           'Field',\n           'FrozenInstanceError',\n           'InitVar',\n           'KW_ONLY',\n           'MISSING',\n\n           # Helper functions.\n           'fields',\n           'asdict',\n           'astuple',\n           'make_dataclass',\n           'replace',\n           'is_dataclass',\n           ]\n\n# Conditions for adding methods.  The boxes indicate what action the\n# dataclass decorator takes.  For all of these tables, when I talk\n# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I'm\n# referring to the arguments to the @dataclass decorator.  When\n# checking if a dunder method already exists, I mean check for an\n# entry in the class's __dict__.  I never check to see if an attribute\n# is defined in a base class.\n\n# Key:\n# +=========+=========================================+\n# + Value   | Meaning                                 |\n# +=========+=========================================+\n# | <blank> | No action: no method is added.          |\n# +---------+-----------------------------------------+\n# | add     | Generated method is added.              |\n# +---------+-----------------------------------------+\n# | raise   | TypeError is raised.                    |\n# +---------+-----------------------------------------+\n# | None    | Attribute is set to None.               |\n# +=========+=========================================+\n\n# __init__\n#\n#   +--- init= parameter\n#   |\n#   v     |       |       |\n#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __repr__\n#\n#    +--- repr= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n\n# __setattr__\n# __delattr__\n#\n#    +--- frozen= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because not adding these methods would break the \"frozen-ness\"\n# of the class.\n\n# __eq__\n#\n#    +--- eq= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __lt__\n# __le__\n# __gt__\n# __ge__\n#\n#    +--- order= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because to allow this case would interfere with using\n# functools.total_ordering.\n\n# __hash__\n\n#    +------------------- unsafe_hash= parameter\n#    |       +----------- eq= parameter\n#    |       |       +--- frozen= parameter\n#    |       |       |\n#    v       v       v    |        |        |\n#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n# +=======+=======+=======+========+========+\n# | False | False | False |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | False | True  |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | True  | False | None   |        | <-- the default, not hashable\n# +-------+-------+-------+--------+--------+\n# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n# +-------+-------+-------+--------+--------+\n# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | False | add    | raise  | Not frozen, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | True  | add    | raise  | Frozen, so hashable\n# +=======+=======+=======+========+========+\n# For boxes that are blank, __hash__ is untouched and therefore\n# inherited from the base class.  If the base is object, then\n# id-based hashing is used.\n#\n# Note that a class may already have __hash__=None if it specified an\n# __eq__ method in the class body (not one that was created by\n# @dataclass).\n#\n# See _hash_action (below) for a coded version of this table.\n\n# __match_args__\n#\n#    +--- match_args= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __match_args__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n# __match_args__ is always added unless the class already defines it. It is a\n# tuple of __init__ parameter names; non-init fields must be matched by keyword.\n\n\n# Raised when an attempt is made to modify a frozen class.\nclass FrozenInstanceError(AttributeError): pass\n\n# A sentinel object for default values to signal that a default\n# factory will be used.  This is given a nice repr() which will appear\n# in the function signature of dataclasses' constructors.\nclass _HAS_DEFAULT_FACTORY_CLASS:\n    def __repr__(self):\n        return '<factory>'\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n\n# A sentinel object to detect if a parameter is supplied or not.  Use\n# a class to give it a better repr.\nclass _MISSING_TYPE:\n    pass\nMISSING = _MISSING_TYPE()\n\n# A sentinel object to indicate that following fields are keyword-only by\n# default.  Use a class to give it a better repr.\nclass _KW_ONLY_TYPE:\n    pass\nKW_ONLY = _KW_ONLY_TYPE()\n\n# Since most per-field metadata will be unused, create an empty\n# read-only proxy that can be shared among all fields.\n_EMPTY_METADATA = types.MappingProxyType({})\n\n# Markers for the various kinds of fields and pseudo-fields.\nclass _FIELD_BASE:\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return self.name\n_FIELD = _FIELD_BASE('_FIELD')\n_FIELD_CLASSVAR = _FIELD_BASE('_FIELD_CLASSVAR')\n_FIELD_INITVAR = _FIELD_BASE('_FIELD_INITVAR')\n\n# The name of an attribute on the class where we store the Field\n# objects.  Also used to check if a class is a Data Class.\n_FIELDS = '__dataclass_fields__'\n\n# The name of an attribute on the class that stores the parameters to\n# @dataclass.\n_PARAMS = '__dataclass_params__'\n\n# The name of the function, that if it exists, is called at the end of\n# __init__.\n_POST_INIT_NAME = '__post_init__'\n\n# String regex that string annotations for ClassVar or InitVar must match.\n# Allows \"identifier.identifier[\" or \"identifier[\".\n# https://bugs.python.org/issue33453 for details.\n_MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n\n# Atomic immutable types which don't require any recursive handling and for which deepcopy\n# returns the same object. We can provide a fast-path for these types in asdict and astuple.\n_ATOMIC_TYPES = frozenset({\n    # Common JSON Serializable types\n    types.NoneType,\n    bool,\n    int,\n    float,\n    str,\n    # Other common types\n    complex,\n    bytes,\n    # Other types that are also unaffected by deepcopy\n    types.EllipsisType,\n    types.NotImplementedType,\n    types.CodeType,\n    types.BuiltinFunctionType,\n    types.FunctionType,\n    type,\n    range,\n    property,\n})\n\n\nclass InitVar:\n    __slots__ = ('type', )\n\n    def __init__(self, type):\n        self.type = type\n\n    def __repr__(self):\n        if isinstance(self.type, type):\n            type_name = self.type.__name__\n        else:\n            # typing objects, e.g. List[int]\n            type_name = repr(self.type)\n        return f'dataclasses.InitVar[{type_name}]'\n\n    def __class_getitem__(cls, type):\n        return InitVar(type)\n\n# Instances of Field are only ever created from within this module,\n# and only from the field() function, although Field instances are\n# exposed externally as (conceptually) read-only objects.\n#\n# name and type are filled in after the fact, not in __init__.\n# They're not known at the time this class is instantiated, but it's\n# convenient if they're available later.\n#\n# When cls._FIELDS is filled in with a list of Field objects, the name\n# and type fields will have been populated.\nclass Field:\n    __slots__ = ('name',\n                 'type',\n                 'default',\n                 'default_factory',\n                 'repr',\n                 'hash',\n                 'init',\n                 'compare',\n                 'metadata',\n                 'kw_only',\n                 '_field_type',  # Private: not to be used by user code.\n                 )\n\n    def __init__(self, default, default_factory, init, repr, hash, compare,\n                 metadata, kw_only):\n        self.name = None\n        self.type = None\n        self.default = default\n        self.default_factory = default_factory\n        self.init = init\n        self.repr = repr\n        self.hash = hash\n        self.compare = compare\n        self.metadata = (_EMPTY_METADATA\n                         if metadata is None else\n                         types.MappingProxyType(metadata))\n        self.kw_only = kw_only\n        self._field_type = None\n\n    @recursive_repr()\n    def __repr__(self):\n        return ('Field('\n                f'name={self.name!r},'\n                f'type={self.type!r},'\n                f'default={self.default!r},'\n                f'default_factory={self.default_factory!r},'\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'hash={self.hash!r},'\n                f'compare={self.compare!r},'\n                f'metadata={self.metadata!r},'\n                f'kw_only={self.kw_only!r},'\n                f'_field_type={self._field_type}'\n                ')')\n\n    # This is used to support the PEP 487 __set_name__ protocol in the\n    # case where we're using a field that contains a descriptor as a\n    # default value.  For details on __set_name__, see\n    # https://peps.python.org/pep-0487/#implementation-details.\n    #\n    # Note that in _process_class, this Field object is overwritten\n    # with the default value, so the end result is a descriptor that\n    # had __set_name__ called on it at the right time.\n    def __set_name__(self, owner, name):\n        func = getattr(type(self.default), '__set_name__', None)\n        if func:\n            # There is a __set_name__ method on the descriptor, call\n            # it.\n            func(self.default, owner, name)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\nclass _DataclassParams:\n    __slots__ = ('init',\n                 'repr',\n                 'eq',\n                 'order',\n                 'unsafe_hash',\n                 'frozen',\n                 'match_args',\n                 'kw_only',\n                 'slots',\n                 'weakref_slot',\n                 )\n\n    def __init__(self,\n                 init, repr, eq, order, unsafe_hash, frozen,\n                 match_args, kw_only, slots, weakref_slot):\n        self.init = init\n        self.repr = repr\n        self.eq = eq\n        self.order = order\n        self.unsafe_hash = unsafe_hash\n        self.frozen = frozen\n        self.match_args = match_args\n        self.kw_only = kw_only\n        self.slots = slots\n        self.weakref_slot = weakref_slot\n\n    def __repr__(self):\n        return ('_DataclassParams('\n                f'init={self.init!r},'\n                f'repr={self.repr!r},'\n                f'eq={self.eq!r},'\n                f'order={self.order!r},'\n                f'unsafe_hash={self.unsafe_hash!r},'\n                f'frozen={self.frozen!r},'\n                f'match_args={self.match_args!r},'\n                f'kw_only={self.kw_only!r},'\n                f'slots={self.slots!r},'\n                f'weakref_slot={self.weakref_slot!r}'\n                ')')\n\n\n# This function is used instead of exposing Field creation directly,\n# so that a type checker can be told (via overloads) that this is a\n# function whose type depends on its parameters.\ndef field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n          hash=None, compare=True, metadata=None, kw_only=MISSING):\n    \"\"\"Return an object to identify dataclass fields.\n\n    default is the default value of the field.  default_factory is a\n    0-argument function called to initialize a field's value.  If init\n    is true, the field will be a parameter to the class's __init__()\n    function.  If repr is true, the field will be included in the\n    object's repr().  If hash is true, the field will be included in the\n    object's hash().  If compare is true, the field will be used in\n    comparison functions.  metadata, if specified, must be a mapping\n    which is stored but not otherwise examined by dataclass.  If kw_only\n    is true, the field will become a keyword-only parameter to\n    __init__().\n\n    It is an error to specify both default and default_factory.\n    \"\"\"\n\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError('cannot specify both default and default_factory')\n    return Field(default, default_factory, init, repr, hash, compare,\n                 metadata, kw_only)\n\n\ndef _fields_in_init_order(fields):\n    # Returns the fields as __init__ will output them.  It returns 2 tuples:\n    # the first for normal args, and the second for keyword args.\n\n    return (tuple(f for f in fields if f.init and not f.kw_only),\n            tuple(f for f in fields if f.init and f.kw_only)\n            )\n\n\ndef _tuple_str(obj_name, fields):\n    # Return a string representing each field of obj_name as a tuple\n    # member.  So, if fields is ['x', 'y'] and obj_name is \"self\",\n    # return \"(self.x,self.y)\".\n\n    # Special case for the 0-tuple.\n    if not fields:\n        return '()'\n    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'\n\n\nclass _FuncBuilder:\n    def __init__(self, globals):\n        self.names = []\n        self.src = []\n        self.globals = globals\n        self.locals = {}\n        self.overwrite_errors = {}\n        self.unconditional_adds = {}\n\n    def add_fn(self, name, args, body, *, locals=None, return_type=MISSING,\n               overwrite_error=False, unconditional_add=False, decorator=None):\n        if locals is not None:\n            self.locals.update(locals)\n\n        # Keep track if this method is allowed to be overwritten if it already\n        # exists in the class.  The error is method-specific, so keep it with\n        # the name.  We'll use this when we generate all of the functions in\n        # the add_fns_to_class call.  overwrite_error is either True, in which\n        # case we'll raise an error, or it's a string, in which case we'll\n        # raise an error and append this string.\n        if overwrite_error:\n            self.overwrite_errors[name] = overwrite_error\n\n        # Should this function always overwrite anything that's already in the\n        # class?  The default is to not overwrite a function that already\n        # exists.\n        if unconditional_add:\n            self.unconditional_adds[name] = True\n\n        self.names.append(name)\n\n        if return_type is not MISSING:\n            self.locals[f'__dataclass_{name}_return_type__'] = return_type\n            return_annotation = f'->__dataclass_{name}_return_type__'\n        else:\n            return_annotation = ''\n        args = ','.join(args)\n        body = '\\n'.join(body)\n\n        # Compute the text of the entire function, add it to the text we're generating.\n        self.src.append(f'{f' {decorator}\\n' if decorator else ''} def {name}({args}){return_annotation}:\\n{body}')\n\n    def add_fns_to_class(self, cls):\n        # The source to all of the functions we're generating.\n        fns_src = '\\n'.join(self.src)\n\n        # The locals they use.\n        local_vars = ','.join(self.locals.keys())\n\n        # The names of all of the functions, used for the return value of the\n        # outer function.  Need to handle the 0-tuple specially.\n        if len(self.names) == 0:\n            return_names = '()'\n        else:\n            return_names  =f'({\",\".join(self.names)},)'\n\n        # txt is the entire function we're going to execute, including the\n        # bodies of the functions we're defining.  Here's a greatly simplified\n        # version:\n        # def __create_fn__():\n        #  def __init__(self, x, y):\n        #   self.x = x\n        #   self.y = y\n        #  @recursive_repr\n        #  def __repr__(self):\n        #   return f\"cls(x={self.x!r},y={self.y!r})\"\n        # return __init__,__repr__\n\n        txt = f\"def __create_fn__({local_vars}):\\n{fns_src}\\n return {return_names}\"\n        ns = {}\n        exec(txt, self.globals, ns)\n        fns = ns['__create_fn__'](**self.locals)\n\n        # Now that we've generated the functions, assign them into cls.\n        for name, fn in zip(self.names, fns):\n            fn.__qualname__ = f\"{cls.__qualname__}.{fn.__name__}\"\n            if self.unconditional_adds.get(name, False):\n                setattr(cls, name, fn)\n            else:\n                already_exists = _set_new_attribute(cls, name, fn)\n\n                # See if it's an error to overwrite this particular function.\n                if already_exists and (msg_extra := self.overwrite_errors.get(name)):\n                    error_msg = (f'Cannot overwrite attribute {fn.__name__} '\n                                 f'in class {cls.__name__}')\n                    if not msg_extra is True:\n                        error_msg = f'{error_msg} {msg_extra}'\n\n                    raise TypeError(error_msg)\n\n\ndef _field_assign(frozen, name, value, self_name):\n    # If we're a frozen class, then assign to our fields in __init__\n    # via object.__setattr__.  Otherwise, just use a simple\n    # assignment.\n    #\n    # self_name is what \"self\" is called in this function: don't\n    # hard-code \"self\", since that might be a field name.\n    if frozen:\n        return f'  __dataclass_builtins_object__.__setattr__({self_name},{name!r},{value})'\n    return f'  {self_name}.{name}={value}'\n\n\ndef _field_init(f, frozen, globals, self_name, slots):\n    # Return the text of the line in the body of __init__ that will\n    # initialize this field.\n\n    default_name = f'__dataclass_dflt_{f.name}__'\n    if f.default_factory is not MISSING:\n        if f.init:\n            # This field has a default factory.  If a parameter is\n            # given, use it.  If not, call the factory.\n            globals[default_name] = f.default_factory\n            value = (f'{default_name}() '\n                     f'if {f.name} is __dataclass_HAS_DEFAULT_FACTORY__ '\n                     f'else {f.name}')\n        else:\n            # This is a field that's not in the __init__ params, but\n            # has a default factory function.  It needs to be\n            # initialized here by calling the factory function,\n            # because there's no other way to initialize it.\n\n            # For a field initialized with a default=defaultvalue, the\n            # class dict just has the default value\n            # (cls.fieldname=defaultvalue).  But that won't work for a\n            # default factory, the factory must be called in __init__\n            # and we must assign that to self.fieldname.  We can't\n            # fall back to the class dict's value, both because it's\n            # not set, and because it might be different per-class\n            # (which, after all, is why we have a factory function!).\n\n            globals[default_name] = f.default_factory\n            value = f'{default_name}()'\n    else:\n        # No default factory.\n        if f.init:\n            if f.default is MISSING:\n                # There's no default, just do an assignment.\n                value = f.name\n            elif f.default is not MISSING:\n                globals[default_name] = f.default\n                value = f.name\n        else:\n            # If the class has slots, then initialize this field.\n            if slots and f.default is not MISSING:\n                globals[default_name] = f.default\n                value = default_name\n            else:\n                # This field does not need initialization: reading from it will\n                # just use the class attribute that contains the default.\n                # Signify that to the caller by returning None.\n                return None\n\n    # Only test this now, so that we can create variables for the\n    # default.  However, return None to signify that we're not going\n    # to actually do the assignment statement for InitVars.\n    if f._field_type is _FIELD_INITVAR:\n        return None\n\n    # Now, actually generate the field assignment.\n    return _field_assign(frozen, f.name, value, self_name)\n\n\ndef _init_param(f):\n    # Return the __init__ parameter string for this field.  For\n    # example, the equivalent of 'x:int=3' (except instead of 'int',\n    # reference a variable set to int, and instead of '3', reference a\n    # variable set to 3).\n    if f.default is MISSING and f.default_factory is MISSING:\n        # There's no default, and no default_factory, just output the\n        # variable name and type.\n        default = ''\n    elif f.default is not MISSING:\n        # There's a default, this will be the name that's used to look\n        # it up.\n        default = f'=__dataclass_dflt_{f.name}__'\n    elif f.default_factory is not MISSING:\n        # There's a factory function.  Set a marker.\n        default = '=__dataclass_HAS_DEFAULT_FACTORY__'\n    return f'{f.name}:__dataclass_type_{f.name}__{default}'\n\n\ndef _init_fn(fields, std_fields, kw_only_fields, frozen, has_post_init,\n             self_name, func_builder, slots):\n    # fields contains both real fields and InitVar pseudo-fields.\n\n    # Make sure we don't have fields without defaults following fields\n    # with defaults.  This actually would be caught when exec-ing the\n    # function source code, but catching it here gives a better error\n    # message, and future-proofs us in case we build up the function\n    # using ast.\n\n    seen_default = None\n    for f in std_fields:\n        # Only consider the non-kw-only fields in the __init__ call.\n        if f.init:\n            if not (f.default is MISSING and f.default_factory is MISSING):\n                seen_default = f\n            elif seen_default:\n                raise TypeError(f'non-default argument {f.name!r} '\n                                f'follows default argument {seen_default.name!r}')\n\n    locals = {**{f'__dataclass_type_{f.name}__': f.type for f in fields},\n              **{'__dataclass_HAS_DEFAULT_FACTORY__': _HAS_DEFAULT_FACTORY,\n                 '__dataclass_builtins_object__': object,\n                 }\n              }\n\n    body_lines = []\n    for f in fields:\n        line = _field_init(f, frozen, locals, self_name, slots)\n        # line is None means that this field doesn't require\n        # initialization (it's a pseudo-field).  Just skip it.\n        if line:\n            body_lines.append(line)\n\n    # Does this class have a post-init function?\n    if has_post_init:\n        params_str = ','.join(f.name for f in fields\n                              if f._field_type is _FIELD_INITVAR)\n        body_lines.append(f'  {self_name}.{_POST_INIT_NAME}({params_str})')\n\n    # If no body lines, use 'pass'.\n    if not body_lines:\n        body_lines = ['  pass']\n\n    _init_params = [_init_param(f) for f in std_fields]\n    if kw_only_fields:\n        # Add the keyword-only args.  Because the * can only be added if\n        # there's at least one keyword-only arg, there needs to be a test here\n        # (instead of just concatenting the lists together).\n        _init_params += ['*']\n        _init_params += [_init_param(f) for f in kw_only_fields]\n    func_builder.add_fn('__init__',\n                        [self_name] + _init_params,\n                        body_lines,\n                        locals=locals,\n                        return_type=None)\n\n\ndef _frozen_get_del_attr(cls, fields, func_builder):\n    locals = {'cls': cls,\n              'FrozenInstanceError': FrozenInstanceError}\n    condition = 'type(self) is cls'\n    if fields:\n        condition += ' or name in {' + ', '.join(repr(f.name) for f in fields) + '}'\n\n    func_builder.add_fn('__setattr__',\n                        ('self', 'name', 'value'),\n                        (f'  if {condition}:',\n                          '   raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\n                         f'  super(cls, self).__setattr__(name, value)'),\n                        locals=locals,\n                        overwrite_error=True)\n    func_builder.add_fn('__delattr__',\n                        ('self', 'name'),\n                        (f'  if {condition}:',\n                          '   raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\n                         f'  super(cls, self).__delattr__(name)'),\n                        locals=locals,\n                        overwrite_error=True)\n\n\ndef _is_classvar(a_type, typing):\n    # This test uses a typing internal class, but it's the best way to\n    # test if this is a ClassVar.\n    return (a_type is typing.ClassVar\n            or (type(a_type) is typing._GenericAlias\n                and a_type.__origin__ is typing.ClassVar))\n\n\ndef _is_initvar(a_type, dataclasses):\n    # The module we're checking against is the module we're\n    # currently in (dataclasses.py).\n    return (a_type is dataclasses.InitVar\n            or type(a_type) is dataclasses.InitVar)\n\ndef _is_kw_only(a_type, dataclasses):\n    return a_type is dataclasses.KW_ONLY\n\n\ndef _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n    # Given a type annotation string, does it refer to a_type in\n    # a_module?  For example, when checking that annotation denotes a\n    # ClassVar, then a_module is typing, and a_type is\n    # typing.ClassVar.\n\n    # It's possible to look up a_module given a_type, but it involves\n    # looking in sys.modules (again!), and seems like a waste since\n    # the caller already knows a_module.\n\n    # - annotation is a string type annotation\n    # - cls is the class that this annotation was found in\n    # - a_module is the module we want to match\n    # - a_type is the type in that module we want to match\n    # - is_type_predicate is a function called with (obj, a_module)\n    #   that determines if obj is of the desired type.\n\n    # Since this test does not do a local namespace lookup (and\n    # instead only a module (global) lookup), there are some things it\n    # gets wrong.\n\n    # With string annotations, cv0 will be detected as a ClassVar:\n    #   CV = ClassVar\n    #   @dataclass\n    #   class C0:\n    #     cv0: CV\n\n    # But in this example cv1 will not be detected as a ClassVar:\n    #   @dataclass\n    #   class C1:\n    #     CV = ClassVar\n    #     cv1: CV\n\n    # In C1, the code in this function (_is_type) will look up \"CV\" in\n    # the module and not find it, so it will not consider cv1 as a\n    # ClassVar.  This is a fairly obscure corner case, and the best\n    # way to fix it would be to eval() the string \"CV\" with the\n    # correct global and local namespaces.  However that would involve\n    # a eval() penalty for every single field of every dataclass\n    # that's defined.  It was judged not worth it.\n\n    match = _MODULE_IDENTIFIER_RE.match(annotation)\n    if match:\n        ns = None\n        module_name = match.group(1)\n        if not module_name:\n            # No module name, assume the class's module did\n            # \"from dataclasses import InitVar\".\n            ns = sys.modules.get(cls.__module__).__dict__\n        else:\n            # Look up module_name in the class's module.\n            module = sys.modules.get(cls.__module__)\n            if module and module.__dict__.get(module_name) is a_module:\n                ns = sys.modules.get(a_type.__module__).__dict__\n        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n            return True\n    return False\n\n\ndef _get_field(cls, a_name, a_type, default_kw_only):\n    # Return a Field object for this field name and type.  ClassVars and\n    # InitVars are also returned, but marked as such (see f._field_type).\n    # default_kw_only is the value of kw_only to use if there isn't a field()\n    # that defines it.\n\n    # If the default value isn't derived from Field, then it's only a\n    # normal default value.  Convert it to a Field().\n    default = getattr(cls, a_name, MISSING)\n    if isinstance(default, Field):\n        f = default\n    else:\n        if isinstance(default, types.MemberDescriptorType):\n            # This is a field in __slots__, so it has no default value.\n            default = MISSING\n        f = field(default=default)\n\n    # Only at this point do we know the name and the type.  Set them.\n    f.name = a_name\n    f.type = a_type\n\n    # Assume it's a normal field until proven otherwise.  We're next\n    # going to decide if it's a ClassVar or InitVar, everything else\n    # is just a normal field.\n    f._field_type = _FIELD\n\n    # In addition to checking for actual types here, also check for\n    # string annotations.  get_type_hints() won't always work for us\n    # (see https://github.com/python/typing/issues/508 for example),\n    # plus it's expensive and would require an eval for every string\n    # annotation.  So, make a best effort to see if this is a ClassVar\n    # or InitVar using regex's and checking that the thing referenced\n    # is actually of the correct type.\n\n    # For the complete discussion, see https://bugs.python.org/issue33453\n\n    # If typing has not been imported, then it's impossible for any\n    # annotation to be a ClassVar.  So, only look for ClassVar if\n    # typing has been imported by any module (not necessarily cls's\n    # module).\n    typing = sys.modules.get('typing')\n    if typing:\n        if (_is_classvar(a_type, typing)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, typing, typing.ClassVar,\n                             _is_classvar))):\n            f._field_type = _FIELD_CLASSVAR\n\n    # If the type is InitVar, or if it's a matching string annotation,\n    # then it's an InitVar.\n    if f._field_type is _FIELD:\n        # The module we're checking against is the module we're\n        # currently in (dataclasses.py).\n        dataclasses = sys.modules[__name__]\n        if (_is_initvar(a_type, dataclasses)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n                             _is_initvar))):\n            f._field_type = _FIELD_INITVAR\n\n    # Validations for individual fields.  This is delayed until now,\n    # instead of in the Field() constructor, since only here do we\n    # know the field name, which allows for better error reporting.\n\n    # Special restrictions for ClassVar and InitVar.\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n        if f.default_factory is not MISSING:\n            raise TypeError(f'field {f.name} cannot have a '\n                            'default factory')\n        # Should I check for other field settings? default_factory\n        # seems the most serious to check for.  Maybe add others.  For\n        # example, how about init=False (or really,\n        # init=<not-the-default-init-value>)?  It makes no sense for\n        # ClassVar and InitVar to specify init=<anything>.\n\n    # kw_only validation and assignment.\n    if f._field_type in (_FIELD, _FIELD_INITVAR):\n        # For real and InitVar fields, if kw_only wasn't specified use the\n        # default value.\n        if f.kw_only is MISSING:\n            f.kw_only = default_kw_only\n    else:\n        # Make sure kw_only isn't set for ClassVars\n        assert f._field_type is _FIELD_CLASSVAR\n        if f.kw_only is not MISSING:\n            raise TypeError(f'field {f.name} is a ClassVar but specifies '\n                            'kw_only')\n\n    # For real fields, disallow mutable defaults.  Use unhashable as a proxy\n    # indicator for mutability.  Read the __hash__ attribute from the class,\n    # not the instance.\n    if f._field_type is _FIELD and f.default.__class__.__hash__ is None:\n        raise ValueError(f'mutable default {type(f.default)} for field '\n                         f'{f.name} is not allowed: use default_factory')\n\n    return f\n\ndef _set_new_attribute(cls, name, value):\n    # Never overwrites an existing attribute.  Returns True if the\n    # attribute already exists.\n    if name in cls.__dict__:\n        return True\n    setattr(cls, name, value)\n    return False\n\n\n# Decide if/how we're going to create a hash function.  Key is\n# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n# take.  The common case is to do nothing, so instead of providing a\n# function that is a no-op, use None to signify that.\n\ndef _hash_set_none(cls, fields, func_builder):\n    # It's sort of a hack that I'm setting this here, instead of at\n    # func_builder.add_fns_to_class time, but since this is an exceptional case\n    # (it's not setting an attribute to a function, but to a scalar value),\n    # just do it directly here.  I might come to regret this.\n    cls.__hash__ = None\n\ndef _hash_add(cls, fields, func_builder):\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n    self_tuple = _tuple_str('self', flds)\n    func_builder.add_fn('__hash__',\n                        ('self',),\n                        [f'  return hash({self_tuple})'],\n                        unconditional_add=True)\n\ndef _hash_exception(cls, fields, func_builder):\n    # Raise an exception.\n    raise TypeError(f'Cannot overwrite attribute __hash__ '\n                    f'in class {cls.__name__}')\n\n#\n#                +-------------------------------------- unsafe_hash?\n#                |      +------------------------------- eq?\n#                |      |      +------------------------ frozen?\n#                |      |      |      +----------------  has-explicit-hash?\n#                |      |      |      |\n#                |      |      |      |        +-------  action\n#                |      |      |      |        |\n#                v      v      v      v        v\n_hash_action = {(False, False, False, False): None,\n                (False, False, False, True ): None,\n                (False, False, True,  False): None,\n                (False, False, True,  True ): None,\n                (False, True,  False, False): _hash_set_none,\n                (False, True,  False, True ): None,\n                (False, True,  True,  False): _hash_add,\n                (False, True,  True,  True ): None,\n                (True,  False, False, False): _hash_add,\n                (True,  False, False, True ): _hash_exception,\n                (True,  False, True,  False): _hash_add,\n                (True,  False, True,  True ): _hash_exception,\n                (True,  True,  False, False): _hash_add,\n                (True,  True,  False, True ): _hash_exception,\n                (True,  True,  True,  False): _hash_add,\n                (True,  True,  True,  True ): _hash_exception,\n                }\n# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n# version of this table.\n\n\ndef _process_class(cls, init, repr, eq, order, unsafe_hash, frozen,\n                   match_args, kw_only, slots, weakref_slot):\n    # Now that dicts retain insertion order, there's no reason to use\n    # an ordered dict.  I am leveraging that ordering here, because\n    # derived class fields overwrite base class fields, but the order\n    # is defined by the base class, which is found first.\n    fields = {}\n\n    if cls.__module__ in sys.modules:\n        globals = sys.modules[cls.__module__].__dict__\n    else:\n        # Theoretically this can happen if someone writes\n        # a custom string to cls.__module__.  In which case\n        # such dataclass won't be fully introspectable\n        # (w.r.t. typing.get_type_hints) but will still function\n        # correctly.\n        globals = {}\n\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n                                           unsafe_hash, frozen,\n                                           match_args, kw_only,\n                                           slots, weakref_slot))\n\n    # Find our base classes in reverse MRO order, and exclude\n    # ourselves.  In reversed order so that more derived classes\n    # override earlier field definitions in base classes.  As long as\n    # we're iterating over them, see if all or any of them are frozen.\n    any_frozen_base = False\n    # By default `all_frozen_bases` is `None` to represent a case,\n    # where some dataclasses does not have any bases with `_FIELDS`\n    all_frozen_bases = None\n    has_dataclass_bases = False\n    for b in cls.__mro__[-1:0:-1]:\n        # Only process classes that have been processed by our\n        # decorator.  That is, they have a _FIELDS attribute.\n        base_fields = getattr(b, _FIELDS, None)\n        if base_fields is not None:\n            has_dataclass_bases = True\n            for f in base_fields.values():\n                fields[f.name] = f\n            if all_frozen_bases is None:\n                all_frozen_bases = True\n            current_frozen = getattr(b, _PARAMS).frozen\n            all_frozen_bases = all_frozen_bases and current_frozen\n            any_frozen_base = any_frozen_base or current_frozen\n\n    # Annotations defined specifically in this class (not in base classes).\n    #\n    # Fields are found from cls_annotations, which is guaranteed to be\n    # ordered.  Default values are from class attributes, if a field\n    # has a default.  If the default value is a Field(), then it\n    # contains additional info beyond (and possibly including) the\n    # actual default value.  Pseudo-fields ClassVars and InitVars are\n    # included, despite the fact that they're not real fields.  That's\n    # dealt with later.\n    cls_annotations = inspect.get_annotations(cls)\n\n    # Now find fields in our class.  While doing so, validate some\n    # things, and set the default values (as class attributes) where\n    # we can.\n    cls_fields = []\n    # Get a reference to this module for the _is_kw_only() test.\n    KW_ONLY_seen = False\n    dataclasses = sys.modules[__name__]\n    for name, type in cls_annotations.items():\n        # See if this is a marker to change the value of kw_only.\n        if (_is_kw_only(type, dataclasses)\n            or (isinstance(type, str)\n                and _is_type(type, cls, dataclasses, dataclasses.KW_ONLY,\n                             _is_kw_only))):\n            # Switch the default to kw_only=True, and ignore this\n            # annotation: it's not a real field.\n            if KW_ONLY_seen:\n                raise TypeError(f'{name!r} is KW_ONLY, but KW_ONLY '\n                                'has already been specified')\n            KW_ONLY_seen = True\n            kw_only = True\n        else:\n            # Otherwise it's a field of some type.\n            cls_fields.append(_get_field(cls, name, type, kw_only))\n\n    for f in cls_fields:\n        fields[f.name] = f\n\n        # If the class attribute (which is the default value for this\n        # field) exists and is of type 'Field', replace it with the\n        # real default.  This is so that normal class introspection\n        # sees a real default value, not a Field.\n        if isinstance(getattr(cls, f.name, None), Field):\n            if f.default is MISSING:\n                # If there's no default, delete the class attribute.\n                # This happens if we specify field(repr=False), for\n                # example (that is, we specified a field object, but\n                # no default value).  Also if we're using a default\n                # factory.  The class attribute should not be set at\n                # all in the post-processed class.\n                delattr(cls, f.name)\n            else:\n                setattr(cls, f.name, f.default)\n\n    # Do we have any Field members that don't also have annotations?\n    for name, value in cls.__dict__.items():\n        if isinstance(value, Field) and not name in cls_annotations:\n            raise TypeError(f'{name!r} is a field but has no type annotation')\n\n    # Check rules that apply if we are derived from any dataclasses.\n    if has_dataclass_bases:\n        # Raise an exception if any of our bases are frozen, but we're not.\n        if any_frozen_base and not frozen:\n            raise TypeError('cannot inherit non-frozen dataclass from a '\n                            'frozen one')\n\n        # Raise an exception if we're frozen, but none of our bases are.\n        if all_frozen_bases is False and frozen:\n            raise TypeError('cannot inherit frozen dataclass from a '\n                            'non-frozen one')\n\n    # Remember all of the fields on our class (including bases).  This\n    # also marks this class as being a dataclass.\n    setattr(cls, _FIELDS, fields)\n\n    # Was this class defined with an explicit __hash__?  Note that if\n    # __eq__ is defined in this class, then python will automatically\n    # set __hash__ to None.  This is a heuristic, as it's possible\n    # that such a __hash__ == None was not auto-generated, but it's\n    # close enough.\n    class_hash = cls.__dict__.get('__hash__', MISSING)\n    has_explicit_hash = not (class_hash is MISSING or\n                             (class_hash is None and '__eq__' in cls.__dict__))\n\n    # If we're generating ordering methods, we must be generating the\n    # eq methods.\n    if order and not eq:\n        raise ValueError('eq must be true if order is true')\n\n    # Include InitVars and regular fields (so, not ClassVars).  This is\n    # initialized here, outside of the \"if init:\" test, because std_init_fields\n    # is used with match_args, below.\n    all_init_fields = [f for f in fields.values()\n                       if f._field_type in (_FIELD, _FIELD_INITVAR)]\n    (std_init_fields,\n     kw_only_init_fields) = _fields_in_init_order(all_init_fields)\n\n    func_builder = _FuncBuilder(globals)\n\n    if init:\n        # Does this class have a post-init function?\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\n\n        _init_fn(all_init_fields,\n                 std_init_fields,\n                 kw_only_init_fields,\n                 frozen,\n                 has_post_init,\n                 # The name to use for the \"self\"\n                 # param in __init__.  Use \"self\"\n                 # if possible.\n                 '__dataclass_self__' if 'self' in fields\n                 else 'self',\n                 func_builder,\n                 slots,\n                 )\n\n    _set_new_attribute(cls, '__replace__', _replace)\n\n    # Get the fields as a list, and include only real fields.  This is\n    # used in all of the following methods.\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n\n    if repr:\n        flds = [f for f in field_list if f.repr]\n        func_builder.add_fn('__repr__',\n                            ('self',),\n                            ['  return f\"{self.__class__.__qualname__}(' +\n                             ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\n                                        for f in flds]) + ')\"'],\n                            locals={'__dataclasses_recursive_repr': recursive_repr},\n                            decorator=\"@__dataclasses_recursive_repr()\")\n\n    if eq:\n        # Create __eq__ method.  There's no need for a __ne__ method,\n        # since python will call __eq__ and negate it.\n        cmp_fields = (field for field in field_list if field.compare)\n        terms = [f'self.{field.name}==other.{field.name}' for field in cmp_fields]\n        field_comparisons = ' and '.join(terms) or 'True'\n        func_builder.add_fn('__eq__',\n                            ('self', 'other'),\n                            [ '  if self is other:',\n                              '   return True',\n                              '  if other.__class__ is self.__class__:',\n                             f'   return {field_comparisons}',\n                              '  return NotImplemented'])\n\n    if order:\n        # Create and set the ordering methods.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str('self', flds)\n        other_tuple = _tuple_str('other', flds)\n        for name, op in [('__lt__', '<'),\n                         ('__le__', '<='),\n                         ('__gt__', '>'),\n                         ('__ge__', '>='),\n                         ]:\n            # Create a comparison function.  If the fields in the object are\n            # named 'x' and 'y', then self_tuple is the string\n            # '(self.x,self.y)' and other_tuple is the string\n            # '(other.x,other.y)'.\n            func_builder.add_fn(name,\n                            ('self', 'other'),\n                            [ '  if other.__class__ is self.__class__:',\n                             f'   return {self_tuple}{op}{other_tuple}',\n                              '  return NotImplemented'],\n                            overwrite_error='Consider using functools.total_ordering')\n\n    if frozen:\n        _frozen_get_del_attr(cls, field_list, func_builder)\n\n    # Decide if/how we're going to create a hash function.\n    hash_action = _hash_action[bool(unsafe_hash),\n                               bool(eq),\n                               bool(frozen),\n                               has_explicit_hash]\n    if hash_action:\n        cls.__hash__ = hash_action(cls, field_list, func_builder)\n\n    # Generate the methods and add them to the class.  This needs to be done\n    # before the __doc__ logic below, since inspect will look at the __init__\n    # signature.\n    func_builder.add_fns_to_class(cls)\n\n    if not getattr(cls, '__doc__'):\n        # Create a class doc-string.\n        try:\n            # In some cases fetching a signature is not possible.\n            # But, we surely should not fail in this case.\n            text_sig = str(inspect.signature(cls)).replace(' -> None', '')\n        except (TypeError, ValueError):\n            text_sig = ''\n        cls.__doc__ = (cls.__name__ + text_sig)\n\n    if match_args:\n        # I could probably compute this once.\n        _set_new_attribute(cls, '__match_args__',\n                           tuple(f.name for f in std_init_fields))\n\n    # It's an error to specify weakref_slot if slots is False.\n    if weakref_slot and not slots:\n        raise TypeError('weakref_slot is True but slots is False')\n    if slots:\n        cls = _add_slots(cls, frozen, weakref_slot)\n\n    abc.update_abstractmethods(cls)\n\n    return cls\n\n\n# _dataclass_getstate and _dataclass_setstate are needed for pickling frozen\n# classes with slots.  These could be slightly more performant if we generated\n# the code instead of iterating over fields.  But that can be a project for\n# another day, if performance becomes an issue.\ndef _dataclass_getstate(self):\n    return [getattr(self, f.name) for f in fields(self)]\n\n\ndef _dataclass_setstate(self, state):\n    for field, value in zip(fields(self), state):\n        # use setattr because dataclass may be frozen\n        object.__setattr__(self, field.name, value)\n\n\ndef _get_slots(cls):\n    match cls.__dict__.get('__slots__'):\n        # `__dictoffset__` and `__weakrefoffset__` can tell us whether\n        # the base type has dict/weakref slots, in a way that works correctly\n        # for both Python classes and C extension types. Extension types\n        # don't use `__slots__` for slot creation\n        case None:\n            slots = []\n            if getattr(cls, '__weakrefoffset__', -1) != 0:\n                slots.append('__weakref__')\n            if getattr(cls, '__dictrefoffset__', -1) != 0:\n                slots.append('__dict__')\n            yield from slots\n        case str(slot):\n            yield slot\n        # Slots may be any iterable, but we cannot handle an iterator\n        # because it will already be (partially) consumed.\n        case iterable if not hasattr(iterable, '__next__'):\n            yield from iterable\n        case _:\n            raise TypeError(f\"Slots of '{cls.__name__}' cannot be determined\")\n\n\ndef _add_slots(cls, is_frozen, weakref_slot):\n    # Need to create a new class, since we can't set __slots__\n    #  after a class has been created.\n\n    # Make sure __slots__ isn't already set.\n    if '__slots__' in cls.__dict__:\n        raise TypeError(f'{cls.__name__} already specifies __slots__')\n\n    # Create a new dict for our new class.\n    cls_dict = dict(cls.__dict__)\n    field_names = tuple(f.name for f in fields(cls))\n    # Make sure slots don't overlap with those in base classes.\n    inherited_slots = set(\n        itertools.chain.from_iterable(map(_get_slots, cls.__mro__[1:-1]))\n    )\n    # The slots for our class.  Remove slots from our base classes.  Add\n    # '__weakref__' if weakref_slot was given, unless it is already present.\n    cls_dict[\"__slots__\"] = tuple(\n        itertools.filterfalse(\n            inherited_slots.__contains__,\n            itertools.chain(\n                # gh-93521: '__weakref__' also needs to be filtered out if\n                # already present in inherited_slots\n                field_names, ('__weakref__',) if weakref_slot else ()\n            )\n        ),\n    )\n\n    for field_name in field_names:\n        # Remove our attributes, if present. They'll still be\n        #  available in _MARKER.\n        cls_dict.pop(field_name, None)\n\n    # Remove __dict__ itself.\n    cls_dict.pop('__dict__', None)\n\n    # Clear existing `__weakref__` descriptor, it belongs to a previous type:\n    cls_dict.pop('__weakref__', None)  # gh-102069\n\n    # And finally create the class.\n    qualname = getattr(cls, '__qualname__', None)\n    cls = type(cls)(cls.__name__, cls.__bases__, cls_dict)\n    if qualname is not None:\n        cls.__qualname__ = qualname\n\n    if is_frozen:\n        # Need this for pickling frozen classes with slots.\n        if '__getstate__' not in cls_dict:\n            cls.__getstate__ = _dataclass_getstate\n        if '__setstate__' not in cls_dict:\n            cls.__setstate__ = _dataclass_setstate\n\n    return cls\n\n\ndef dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False, match_args=True,\n              kw_only=False, slots=False, weakref_slot=False):\n    \"\"\"Add dunder methods based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If repr\n    is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method is added. If frozen is true, fields may not be\n    assigned to after instance creation. If match_args is true, the\n    __match_args__ tuple is added. If kw_only is true, then by default\n    all fields are keyword-only. If slots is true, a new class with a\n    __slots__ attribute is returned.\n    \"\"\"\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash,\n                              frozen, match_args, kw_only, slots,\n                              weakref_slot)\n\n    # See if we're being called as @dataclass or @dataclass().\n    if cls is None:\n        # We're called with parens.\n        return wrap\n\n    # We're called as @dataclass without parens.\n    return wrap(cls)\n\n\ndef fields(class_or_instance):\n    \"\"\"Return a tuple describing the fields of this dataclass.\n\n    Accepts a dataclass or an instance of one. Tuple elements are of\n    type Field.\n    \"\"\"\n\n    # Might it be worth caching this, per class?\n    try:\n        fields = getattr(class_or_instance, _FIELDS)\n    except AttributeError:\n        raise TypeError('must be called with a dataclass type or instance') from None\n\n    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n    # order, so the order of the tuple is as the fields were defined.\n    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n\n\ndef _is_dataclass_instance(obj):\n    \"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\n    return hasattr(type(obj), _FIELDS)\n\n\ndef is_dataclass(obj):\n    \"\"\"Returns True if obj is a dataclass or an instance of a\n    dataclass.\"\"\"\n    cls = obj if isinstance(obj, type) else type(obj)\n    return hasattr(cls, _FIELDS)\n\n\ndef asdict(obj, *, dict_factory=dict):\n    \"\"\"Return the fields of a dataclass instance as a new dictionary mapping\n    field names to field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert asdict(c) == {'x': 1, 'y': 2}\n\n    If given, 'dict_factory' will be used instead of built-in dict.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts. Other objects are copied with 'copy.deepcopy()'.\n    \"\"\"\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"asdict() should be called on dataclass instances\")\n    return _asdict_inner(obj, dict_factory)\n\n\ndef _asdict_inner(obj, dict_factory):\n    obj_type = type(obj)\n    if obj_type in _ATOMIC_TYPES:\n        return obj\n    elif hasattr(obj_type, _FIELDS):\n        # dataclass instance: fast path for the common case\n        if dict_factory is dict:\n            return {\n                f.name: _asdict_inner(getattr(obj, f.name), dict)\n                for f in fields(obj)\n            }\n        else:\n            return dict_factory([\n                (f.name, _asdict_inner(getattr(obj, f.name), dict_factory))\n                for f in fields(obj)\n            ])\n    # handle the builtin types first for speed; subclasses handled below\n    elif obj_type is list:\n        return [_asdict_inner(v, dict_factory) for v in obj]\n    elif obj_type is dict:\n        return {\n            _asdict_inner(k, dict_factory): _asdict_inner(v, dict_factory)\n            for k, v in obj.items()\n        }\n    elif obj_type is tuple:\n        return tuple([_asdict_inner(v, dict_factory) for v in obj])\n    elif issubclass(obj_type, tuple):\n        if hasattr(obj, '_fields'):\n            # obj is a namedtuple.  Recurse into it, but the returned\n            # object is another namedtuple of the same type.  This is\n            # similar to how other list- or tuple-derived classes are\n            # treated (see below), but we just need to create them\n            # differently because a namedtuple's __init__ needs to be\n            # called differently (see bpo-34363).\n\n            # I'm not using namedtuple's _asdict()\n            # method, because:\n            # - it does not recurse in to the namedtuple fields and\n            #   convert them to dicts (using dict_factory).\n            # - I don't actually want to return a dict here.  The main\n            #   use case here is json.dumps, and it handles converting\n            #   namedtuples to lists.  Admittedly we're losing some\n            #   information here when we produce a json list instead of a\n            #   dict.  Note that if we returned dicts here instead of\n            #   namedtuples, we could no longer call asdict() on a data\n            #   structure where a namedtuple was used as a dict key.\n            return obj_type(*[_asdict_inner(v, dict_factory) for v in obj])\n        else:\n            return obj_type(_asdict_inner(v, dict_factory) for v in obj)\n    elif issubclass(obj_type, dict):\n        if hasattr(obj_type, 'default_factory'):\n            # obj is a defaultdict, which has a different constructor from\n            # dict as it requires the default_factory as its first arg.\n            result = obj_type(obj.default_factory)\n            for k, v in obj.items():\n                result[_asdict_inner(k, dict_factory)] = _asdict_inner(v, dict_factory)\n            return result\n        return obj_type((_asdict_inner(k, dict_factory),\n                         _asdict_inner(v, dict_factory))\n                        for k, v in obj.items())\n    elif issubclass(obj_type, list):\n        # Assume we can create an object of this type by passing in a\n        # generator\n        return obj_type(_asdict_inner(v, dict_factory) for v in obj)\n    else:\n        return copy.deepcopy(obj)\n\n\ndef astuple(obj, *, tuple_factory=tuple):\n    \"\"\"Return the fields of a dataclass instance as a new tuple of field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert astuple(c) == (1, 2)\n\n    If given, 'tuple_factory' will be used instead of built-in tuple.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts. Other objects are copied with 'copy.deepcopy()'.\n    \"\"\"\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"astuple() should be called on dataclass instances\")\n    return _astuple_inner(obj, tuple_factory)\n\n\ndef _astuple_inner(obj, tuple_factory):\n    if type(obj) in _ATOMIC_TYPES:\n        return obj\n    elif _is_dataclass_instance(obj):\n        return tuple_factory([\n            _astuple_inner(getattr(obj, f.name), tuple_factory)\n            for f in fields(obj)\n        ])\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple's __init__ needs to be\n        # called differently (see bpo-34363).\n        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n    elif isinstance(obj, dict):\n        obj_type = type(obj)\n        if hasattr(obj_type, 'default_factory'):\n            # obj is a defaultdict, which has a different constructor from\n            # dict as it requires the default_factory as its first arg.\n            result = obj_type(getattr(obj, 'default_factory'))\n            for k, v in obj.items():\n                result[_astuple_inner(k, tuple_factory)] = _astuple_inner(v, tuple_factory)\n            return result\n        return obj_type((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n                          for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n                   repr=True, eq=True, order=False, unsafe_hash=False,\n                   frozen=False, match_args=True, kw_only=False, slots=False,\n                   weakref_slot=False, module=None):\n    \"\"\"Return a new dynamically created dataclass.\n\n    The dataclass name will be 'cls_name'.  'fields' is an iterable\n    of either (name), (name, type) or (name, type, Field) objects. If type is\n    omitted, use the string 'typing.Any'.  Field objects are created by\n    the equivalent of calling 'field(name, type [, Field-info])'.::\n\n      C = make_dataclass('C', ['x', ('y', int), ('z', int, field(init=False))], bases=(Base,))\n\n    is equivalent to::\n\n      @dataclass\n      class C(Base):\n          x: 'typing.Any'\n          y: int\n          z: int = field(init=False)\n\n    For the bases and namespace parameters, see the builtin type() function.\n\n    The parameters init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only,\n    slots, and weakref_slot are passed to dataclass().\n\n    If module parameter is defined, the '__module__' attribute of the dataclass is\n    set to that value.\n    \"\"\"\n\n    if namespace is None:\n        namespace = {}\n\n    # While we're looking through the field names, validate that they\n    # are identifiers, are not keywords, and not duplicates.\n    seen = set()\n    annotations = {}\n    defaults = {}\n    for item in fields:\n        if isinstance(item, str):\n            name = item\n            tp = 'typing.Any'\n        elif len(item) == 2:\n            name, tp, = item\n        elif len(item) == 3:\n            name, tp, spec = item\n            defaults[name] = spec\n        else:\n            raise TypeError(f'Invalid field: {item!r}')\n\n        if not isinstance(name, str) or not name.isidentifier():\n            raise TypeError(f'Field names must be valid identifiers: {name!r}')\n        if keyword.iskeyword(name):\n            raise TypeError(f'Field names must not be keywords: {name!r}')\n        if name in seen:\n            raise TypeError(f'Field name duplicated: {name!r}')\n\n        seen.add(name)\n        annotations[name] = tp\n\n    # Update 'ns' with the user-supplied namespace plus our calculated values.\n    def exec_body_callback(ns):\n        ns.update(namespace)\n        ns.update(defaults)\n        ns['__annotations__'] = annotations\n\n    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n    # of generic dataclasses.\n    cls = types.new_class(cls_name, bases, {}, exec_body_callback)\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the dataclass is created.\n    if module is None:\n        try:\n            module = sys._getframemodulename(1) or '__main__'\n        except AttributeError:\n            try:\n                module = sys._getframe(1).f_globals.get('__name__', '__main__')\n            except (AttributeError, ValueError):\n                pass\n    if module is not None:\n        cls.__module__ = module\n\n    # Apply the normal decorator.\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n                     unsafe_hash=unsafe_hash, frozen=frozen,\n                     match_args=match_args, kw_only=kw_only, slots=slots,\n                     weakref_slot=weakref_slot)\n\n\ndef replace(obj, /, **changes):\n    \"\"\"Return a new object replacing specified fields with new values.\n\n    This is especially useful for frozen classes.  Example usage::\n\n      @dataclass(frozen=True)\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      c1 = replace(c, x=3)\n      assert c1.x == 3 and c1.y == 2\n    \"\"\"\n    if not _is_dataclass_instance(obj):\n        raise TypeError(\"replace() should be called on dataclass instances\")\n    return _replace(obj, **changes)\n\n\ndef _replace(self, /, **changes):\n    # We're going to mutate 'changes', but that's okay because it's a\n    # new dict, even if called with 'replace(self, **my_changes)'.\n\n    # It's an error to have init=False fields in 'changes'.\n    # If a field is not in 'changes', read its value from the provided 'self'.\n\n    for f in getattr(self, _FIELDS).values():\n        # Only consider normal fields or InitVars.\n        if f._field_type is _FIELD_CLASSVAR:\n            continue\n\n        if not f.init:\n            # Error if this field is specified in changes.\n            if f.name in changes:\n                raise TypeError(f'field {f.name} is declared with '\n                                f'init=False, it cannot be specified with '\n                                f'replace()')\n            continue\n\n        if f.name not in changes:\n            if f._field_type is _FIELD_INITVAR and f.default is MISSING:\n                raise TypeError(f\"InitVar {f.name!r} \"\n                                f'must be specified with replace()')\n            changes[f.name] = getattr(self, f.name)\n\n    # Create the new object, which calls __init__() and\n    # __post_init__() (if defined), using all of the init fields we've\n    # added and/or left in 'changes'.  If there are values supplied in\n    # changes that aren't fields, this will correctly raise a\n    # TypeError.\n    return self.__class__(**changes)\n", 1630], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py": ["import sys\nimport builtins as bltns\nfrom types import MappingProxyType, DynamicClassAttribute\n\n\n__all__ = [\n        'EnumType', 'EnumMeta', 'EnumDict',\n        'Enum', 'IntEnum', 'StrEnum', 'Flag', 'IntFlag', 'ReprEnum',\n        'auto', 'unique', 'property', 'verify', 'member', 'nonmember',\n        'FlagBoundary', 'STRICT', 'CONFORM', 'EJECT', 'KEEP',\n        'global_flag_repr', 'global_enum_repr', 'global_str', 'global_enum',\n        'EnumCheck', 'CONTINUOUS', 'NAMED_FLAGS', 'UNIQUE',\n        'pickle_by_global_name', 'pickle_by_enum_name',\n        ]\n\n\n# Dummy value for Enum and Flag as there are explicit checks for them\n# before they have been created.\n# This is also why there are checks in EnumType like `if Enum is not None`\nEnum = Flag = EJECT = _stdlib_enums = ReprEnum = None\n\nclass nonmember(object):\n    \"\"\"\n    Protects item from becoming an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\nclass member(object):\n    \"\"\"\n    Forces item to become an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\ndef _is_descriptor(obj):\n    \"\"\"\n    Returns True if obj is a descriptor, False otherwise.\n    \"\"\"\n    return (\n            hasattr(obj, '__get__') or\n            hasattr(obj, '__set__') or\n            hasattr(obj, '__delete__')\n            )\n\ndef _is_dunder(name):\n    \"\"\"\n    Returns True if a __dunder__ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 4 and\n            name[:2] == name[-2:] == '__' and\n            name[2] != '_' and\n            name[-3] != '_'\n            )\n\ndef _is_sunder(name):\n    \"\"\"\n    Returns True if a _sunder_ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 2 and\n            name[0] == name[-1] == '_' and\n            name[1] != '_' and\n            name[-2] != '_'\n            )\n\ndef _is_internal_class(cls_name, obj):\n    # do not use `re` as `re` imports `enum`\n    if not isinstance(obj, type):\n        return False\n    qualname = getattr(obj, '__qualname__', '')\n    s_pattern = cls_name + '.' + getattr(obj, '__name__', '')\n    e_pattern = '.' + s_pattern\n    return qualname == s_pattern or qualname.endswith(e_pattern)\n\ndef _is_private(cls_name, name):\n    # do not use `re` as `re` imports `enum`\n    pattern = '_%s__' % (cls_name, )\n    pat_len = len(pattern)\n    if (\n            len(name) > pat_len\n            and name.startswith(pattern)\n            and (name[-1] != '_' or name[-2] != '_')\n        ):\n        return True\n    else:\n        return False\n\ndef _is_single_bit(num):\n    \"\"\"\n    True if only one bit set in num (should be an int)\n    \"\"\"\n    if num == 0:\n        return False\n    num &= num - 1\n    return num == 0\n\ndef _make_class_unpicklable(obj):\n    \"\"\"\n    Make the given obj un-picklable.\n\n    obj should be either a dictionary, or an Enum\n    \"\"\"\n    def _break_on_call_reduce(self, proto):\n        raise TypeError('%r cannot be pickled' % self)\n    if isinstance(obj, dict):\n        obj['__reduce_ex__'] = _break_on_call_reduce\n        obj['__module__'] = '<unknown>'\n    else:\n        setattr(obj, '__reduce_ex__', _break_on_call_reduce)\n        setattr(obj, '__module__', '<unknown>')\n\ndef _iter_bits_lsb(num):\n    # num must be a positive integer\n    original = num\n    if isinstance(num, Enum):\n        num = num.value\n    if num < 0:\n        raise ValueError('%r is not a positive integer' % original)\n    while num:\n        b = num & (~num + 1)\n        yield b\n        num ^= b\n\ndef show_flag_values(value):\n    return list(_iter_bits_lsb(value))\n\ndef bin(num, max_bits=None):\n    \"\"\"\n    Like built-in bin(), except negative values are represented in\n    twos-compliment, and the leading bit always indicates sign\n    (0=positive, 1=negative).\n\n    >>> bin(10)\n    '0b0 1010'\n    >>> bin(~10)   # ~10 is -11\n    '0b1 0101'\n    \"\"\"\n\n    ceiling = 2 ** (num).bit_length()\n    if num >= 0:\n        s = bltns.bin(num + ceiling).replace('1', '0', 1)\n    else:\n        s = bltns.bin(~num ^ (ceiling - 1) + ceiling)\n    sign = s[:3]\n    digits = s[3:]\n    if max_bits is not None:\n        if len(digits) < max_bits:\n            digits = (sign[-1] * max_bits + digits)[-max_bits:]\n    return \"%s %s\" % (sign, digits)\n\ndef _dedent(text):\n    \"\"\"\n    Like textwrap.dedent.  Rewritten because we cannot import textwrap.\n    \"\"\"\n    lines = text.split('\\n')\n    for i, ch in enumerate(lines[0]):\n        if ch != ' ':\n            break\n    for j, l in enumerate(lines):\n        lines[j] = l[i:]\n    return '\\n'.join(lines)\n\nclass _not_given:\n    def __repr__(self):\n        return('<not given>')\n_not_given = _not_given()\n\nclass _auto_null:\n    def __repr__(self):\n        return '_auto_null'\n_auto_null = _auto_null()\n\nclass auto:\n    \"\"\"\n    Instances are replaced with an appropriate value in Enum class suites.\n    \"\"\"\n    def __init__(self, value=_auto_null):\n        self.value = value\n\n    def __repr__(self):\n        return \"auto(%r)\" % self.value\n\nclass property(DynamicClassAttribute):\n    \"\"\"\n    This is a descriptor, used to define attributes that act differently\n    when accessed through an enum member and through an enum class.\n    Instance access is the same as property(), but access to an attribute\n    through the enum class will instead look in the class' _member_map_ for\n    a corresponding enum member.\n    \"\"\"\n\n    member = None\n    _attr_type = None\n    _cls_type = None\n\n    def __get__(self, instance, ownerclass=None):\n        if instance is None:\n            if self.member is not None:\n                return self.member\n            else:\n                raise AttributeError(\n                        '%r has no attribute %r' % (ownerclass, self.name)\n                        )\n        if self.fget is not None:\n            # use previous enum.property\n            return self.fget(instance)\n        elif self._attr_type == 'attr':\n            # look up previous attibute\n            return getattr(self._cls_type, self.name)\n        elif self._attr_type == 'desc':\n            # use previous descriptor\n            return getattr(instance._value_, self.name)\n        # look for a member by this name.\n        try:\n            return ownerclass._member_map_[self.name]\n        except KeyError:\n            raise AttributeError(\n                    '%r has no attribute %r' % (ownerclass, self.name)\n                    ) from None\n\n    def __set__(self, instance, value):\n        if self.fset is not None:\n            return self.fset(instance, value)\n        raise AttributeError(\n                \"<enum %r> cannot set attribute %r\" % (self.clsname, self.name)\n                )\n\n    def __delete__(self, instance):\n        if self.fdel is not None:\n            return self.fdel(instance)\n        raise AttributeError(\n                \"<enum %r> cannot delete attribute %r\" % (self.clsname, self.name)\n                )\n\n    def __set_name__(self, ownerclass, name):\n        self.name = name\n        self.clsname = ownerclass.__name__\n\n\nclass _proto_member:\n    \"\"\"\n    intermediate step for enum members between class execution and final creation\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n    def __set_name__(self, enum_class, member_name):\n        \"\"\"\n        convert each quasi-member into an instance of the new enum class\n        \"\"\"\n        # first step: remove ourself from enum_class\n        delattr(enum_class, member_name)\n        # second step: create member based on enum_class\n        value = self.value\n        if not isinstance(value, tuple):\n            args = (value, )\n        else:\n            args = value\n        if enum_class._member_type_ is tuple:   # special case for tuple enums\n            args = (args, )     # wrap it one more time\n        if not enum_class._use_args_:\n            enum_member = enum_class._new_member_(enum_class)\n        else:\n            enum_member = enum_class._new_member_(enum_class, *args)\n        if not hasattr(enum_member, '_value_'):\n            if enum_class._member_type_ is object:\n                enum_member._value_ = value\n            else:\n                try:\n                    enum_member._value_ = enum_class._member_type_(*args)\n                except Exception as exc:\n                    new_exc = TypeError(\n                            '_value_ not set in __new__, unable to create it'\n                            )\n                    new_exc.__cause__ = exc\n                    raise new_exc\n        value = enum_member._value_\n        enum_member._name_ = member_name\n        enum_member.__objclass__ = enum_class\n        enum_member.__init__(*args)\n        enum_member._sort_order_ = len(enum_class._member_names_)\n\n        if Flag is not None and issubclass(enum_class, Flag):\n            if isinstance(value, int):\n                enum_class._flag_mask_ |= value\n                if _is_single_bit(value):\n                    enum_class._singles_mask_ |= value\n            enum_class._all_bits_ = 2 ** ((enum_class._flag_mask_).bit_length()) - 1\n\n        # If another member with the same value was already defined, the\n        # new member becomes an alias to the existing one.\n        try:\n            try:\n                # try to do a fast lookup to avoid the quadratic loop\n                enum_member = enum_class._value2member_map_[value]\n            except TypeError:\n                for name, canonical_member in enum_class._member_map_.items():\n                    if canonical_member._value_ == value:\n                        enum_member = canonical_member\n                        break\n                else:\n                    raise KeyError\n        except KeyError:\n            # this could still be an alias if the value is multi-bit and the\n            # class is a flag class\n            if (\n                    Flag is None\n                    or not issubclass(enum_class, Flag)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n            elif (\n                    Flag is not None\n                    and issubclass(enum_class, Flag)\n                    and isinstance(value, int)\n                    and _is_single_bit(value)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n\n        enum_class._add_member_(member_name, enum_member)\n        try:\n            # This may fail if value is not hashable. We can't add the value\n            # to the map, and by-value lookups for this value will be\n            # linear.\n            enum_class._value2member_map_.setdefault(value, enum_member)\n        except TypeError:\n            # keep track of the value in a list so containment checks are quick\n            enum_class._unhashable_values_.append(value)\n            enum_class._unhashable_values_map_.setdefault(member_name, []).append(value)\n\n\nclass EnumDict(dict):\n    \"\"\"\n    Track enum member order and ensure member names are not reused.\n\n    EnumType will use the names found in self._member_names as the\n    enumeration member names.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._member_names = {} # use a dict -- faster look-up than a list, and keeps insertion order since 3.7\n        self._last_values = []\n        self._ignore = []\n        self._auto_called = False\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Changes anything not dundered or not a descriptor.\n\n        If an enum member name is used twice, an error is raised; duplicate\n        values are not checked for.\n\n        Single underscore (sunder) names are reserved.\n        \"\"\"\n        if _is_private(self._cls_name, key):\n            # do nothing, name will be a normal attribute\n            pass\n        elif _is_sunder(key):\n            if key not in (\n                    '_order_',\n                    '_generate_next_value_', '_numeric_repr_', '_missing_', '_ignore_',\n                    '_iter_member_', '_iter_member_by_value_', '_iter_member_by_def_',\n                    '_add_alias_', '_add_value_alias_',\n                    # While not in use internally, those are common for pretty\n                    # printing and thus excluded from Enum's reservation of\n                    # _sunder_ names\n                    ) and not key.startswith('_repr_'):\n                raise ValueError(\n                        '_sunder_ names, such as %r, are reserved for future Enum use'\n                        % (key, )\n                        )\n            if key == '_generate_next_value_':\n                # check if members already defined as auto()\n                if self._auto_called:\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\n                _gnv = value.__func__ if isinstance(value, staticmethod) else value\n                setattr(self, '_generate_next_value', _gnv)\n            elif key == '_ignore_':\n                if isinstance(value, str):\n                    value = value.replace(',',' ').split()\n                else:\n                    value = list(value)\n                self._ignore = value\n                already = set(value) & set(self._member_names)\n                if already:\n                    raise ValueError(\n                            '_ignore_ cannot specify already set names: %r'\n                            % (already, )\n                            )\n        elif _is_dunder(key):\n            if key == '__order__':\n                key = '_order_'\n        elif key in self._member_names:\n            # descriptor overwriting an enum?\n            raise TypeError('%r already defined as %r' % (key, self[key]))\n        elif key in self._ignore:\n            pass\n        elif isinstance(value, nonmember):\n            # unwrap value here; it won't be processed by the below `else`\n            value = value.value\n        elif _is_descriptor(value):\n            pass\n        elif _is_internal_class(self._cls_name, value):\n            # do nothing, name will be a normal attribute\n            pass\n        else:\n            if key in self:\n                # enum overwriting a descriptor?\n                raise TypeError('%r already defined as %r' % (key, self[key]))\n            elif isinstance(value, member):\n                # unwrap value here -- it will become a member\n                value = value.value\n            non_auto_store = True\n            single = False\n            if isinstance(value, auto):\n                single = True\n                value = (value, )\n            if isinstance(value, tuple) and any(isinstance(v, auto) for v in value):\n                # insist on an actual tuple, no subclasses, in keeping with only supporting\n                # top-level auto() usage (not contained in any other data structure)\n                auto_valued = []\n                t = type(value)\n                for v in value:\n                    if isinstance(v, auto):\n                        non_auto_store = False\n                        if v.value == _auto_null:\n                            v.value = self._generate_next_value(\n                                    key, 1, len(self._member_names), self._last_values[:],\n                                    )\n                            self._auto_called = True\n                        v = v.value\n                        self._last_values.append(v)\n                    auto_valued.append(v)\n                if single:\n                    value = auto_valued[0]\n                else:\n                    try:\n                        # accepts iterable as multiple arguments?\n                        value = t(auto_valued)\n                    except TypeError:\n                        # then pass them in singlely\n                        value = t(*auto_valued)\n            self._member_names[key] = None\n            if non_auto_store:\n                self._last_values.append(value)\n        super().__setitem__(key, value)\n\n    @property\n    def member_names(self):\n        return list(self._member_names)\n\n    def update(self, members, **more_members):\n        try:\n            for name in members.keys():\n                self[name] = members[name]\n        except AttributeError:\n            for name, value in members:\n                self[name] = value\n        for name, value in more_members.items():\n            self[name] = value\n\n_EnumDict = EnumDict        # keep private name for backwards compatibility\n\n\nclass EnumType(type):\n    \"\"\"\n    Metaclass for Enum\n    \"\"\"\n\n    @classmethod\n    def __prepare__(metacls, cls, bases, **kwds):\n        # check that previous enum members do not exist\n        metacls._check_for_existing_members_(cls, bases)\n        # create the namespace dict\n        enum_dict = EnumDict()\n        enum_dict._cls_name = cls\n        # inherit previous flags and _generate_next_value_ function\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        if first_enum is not None:\n            enum_dict['_generate_next_value_'] = getattr(\n                    first_enum, '_generate_next_value_', None,\n                    )\n        return enum_dict\n\n    def __new__(metacls, cls, bases, classdict, *, boundary=None, _simple=False, **kwds):\n        # an Enum class is final once enumeration items have been defined; it\n        # cannot be mixed with other types (int, float, etc.) if it has an\n        # inherited __new__ unless a new __new__ is defined (or the resulting\n        # class will fail).\n        #\n        if _simple:\n            return super().__new__(metacls, cls, bases, classdict, **kwds)\n        #\n        # remove any keys listed in _ignore_\n        classdict.setdefault('_ignore_', []).append('_ignore_')\n        ignore = classdict['_ignore_']\n        for key in ignore:\n            classdict.pop(key, None)\n        #\n        # grab member names\n        member_names = classdict._member_names\n        #\n        # check for illegal enum names (any others?)\n        invalid_names = set(member_names) & {'mro', ''}\n        if invalid_names:\n            raise ValueError('invalid enum member name(s) %s'  % (\n                    ','.join(repr(n) for n in invalid_names)\n                    ))\n        #\n        # adjust the sunders\n        _order_ = classdict.pop('_order_', None)\n        _gnv = classdict.get('_generate_next_value_')\n        if _gnv is not None and type(_gnv) is not staticmethod:\n            _gnv = staticmethod(_gnv)\n        # convert to normal dict\n        classdict = dict(classdict.items())\n        if _gnv is not None:\n            classdict['_generate_next_value_'] = _gnv\n        #\n        # data type of member and the controlling Enum class\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        __new__, save_new, use_args = metacls._find_new_(\n                classdict, member_type, first_enum,\n                )\n        classdict['_new_member_'] = __new__\n        classdict['_use_args_'] = use_args\n        #\n        # convert future enum members into temporary _proto_members\n        for name in member_names:\n            value = classdict[name]\n            classdict[name] = _proto_member(value)\n        #\n        # house-keeping structures\n        classdict['_member_names_'] = []\n        classdict['_member_map_'] = {}\n        classdict['_value2member_map_'] = {}\n        classdict['_unhashable_values_'] = []\n        classdict['_unhashable_values_map_'] = {}\n        classdict['_member_type_'] = member_type\n        # now set the __repr__ for the value\n        classdict['_value_repr_'] = metacls._find_data_repr_(cls, bases)\n        #\n        # Flag structures (will be removed if final class is not a Flag\n        classdict['_boundary_'] = (\n                boundary\n                or getattr(first_enum, '_boundary_', None)\n                )\n        classdict['_flag_mask_'] = 0\n        classdict['_singles_mask_'] = 0\n        classdict['_all_bits_'] = 0\n        classdict['_inverted_'] = None\n        try:\n            exc = None\n            classdict['_%s__in_progress' % cls] = True\n            enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n            classdict['_%s__in_progress' % cls] = False\n            delattr(enum_class, '_%s__in_progress' % cls)\n        except Exception as e:\n            # since 3.12 the line \"Error calling __set_name__ on '_proto_member' instance ...\"\n            # is tacked on to the error instead of raising a RuntimeError\n            # recreate the exception to discard\n            exc = type(e)(str(e))\n            exc.__cause__ = e.__cause__\n            exc.__context__ = e.__context__\n            tb = e.__traceback__\n        if exc is not None:\n            raise exc.with_traceback(tb)\n        #\n        # update classdict with any changes made by __init_subclass__\n        classdict.update(enum_class.__dict__)\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        #\n        # Also, special handling for ReprEnum\n        if ReprEnum is not None and ReprEnum in bases:\n            if member_type is object:\n                raise TypeError(\n                        'ReprEnum subclasses must be mixed with a data type (i.e.'\n                        ' int, str, float, etc.)'\n                        )\n            if '__format__' not in classdict:\n                enum_class.__format__ = member_type.__format__\n                classdict['__format__'] = enum_class.__format__\n            if '__str__' not in classdict:\n                method = member_type.__str__\n                if method is object.__str__:\n                    # if member_type does not define __str__, object.__str__ will use\n                    # its __repr__ instead, so we'll also use its __repr__\n                    method = member_type.__repr__\n                enum_class.__str__ = method\n                classdict['__str__'] = enum_class.__str__\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in classdict:\n                # check for mixin overrides before replacing\n                enum_method = getattr(first_enum, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        #\n        # for Flag, add __or__, __and__, __xor__, and __invert__\n        if Flag is not None and issubclass(enum_class, Flag):\n            for name in (\n                    '__or__', '__and__', '__xor__',\n                    '__ror__', '__rand__', '__rxor__',\n                    '__invert__'\n                ):\n                if name not in classdict:\n                    enum_method = getattr(Flag, name)\n                    setattr(enum_class, name, enum_method)\n                    classdict[name] = enum_method\n        #\n        # replace any other __new__ with our own (as long as Enum is not None,\n        # anyway) -- again, this is to support pickle\n        if Enum is not None:\n            # if the user defined their own __new__, save it before it gets\n            # clobbered in case they subclass later\n            if save_new:\n                enum_class.__new_member__ = __new__\n            enum_class.__new__ = Enum.__new__\n        #\n        # py3 support for definition order (helps keep py2/py3 code in sync)\n        #\n        # _order_ checking is spread out into three/four steps\n        # - if enum_class is a Flag:\n        #   - remove any non-single-bit flags from _order_\n        # - remove any aliases from _order_\n        # - check that _order_ and _member_names_ match\n        #\n        # step 1: ensure we have a list\n        if _order_ is not None:\n            if isinstance(_order_, str):\n                _order_ = _order_.replace(',', ' ').split()\n        #\n        # remove Flag structures if final class is not a Flag\n        if (\n                Flag is None and cls != 'Flag'\n                or Flag is not None and not issubclass(enum_class, Flag)\n            ):\n            delattr(enum_class, '_boundary_')\n            delattr(enum_class, '_flag_mask_')\n            delattr(enum_class, '_singles_mask_')\n            delattr(enum_class, '_all_bits_')\n            delattr(enum_class, '_inverted_')\n        elif Flag is not None and issubclass(enum_class, Flag):\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n            if _order_:\n                # _order_ step 2: remove any items from _order_ that are not single-bit\n                _order_ = [\n                        o\n                        for o in _order_\n                        if o not in enum_class._member_map_ or _is_single_bit(enum_class[o]._value_)\n                        ]\n        #\n        if _order_:\n            # _order_ step 3: remove aliases from _order_\n            _order_ = [\n                    o\n                    for o in _order_\n                    if (\n                        o not in enum_class._member_map_\n                        or\n                        (o in enum_class._member_map_ and o in enum_class._member_names_)\n                        )]\n            # _order_ step 4: verify that _order_ and _member_names_ match\n            if _order_ != enum_class._member_names_:\n                raise TypeError(\n                        'member order does not match _order_:\\n  %r\\n  %r'\n                        % (enum_class._member_names_, _order_)\n                        )\n        #\n        return enum_class\n\n    def __bool__(cls):\n        \"\"\"\n        classes/types should always be True.\n        \"\"\"\n        return True\n\n    def __call__(cls, value, names=_not_given, *values, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Either returns an existing member, or creates a new enum class.\n\n        This method is used both when an enum class is given a value to match\n        to an enumeration member (i.e. Color(3)) and for the functional API\n        (i.e. Color = Enum('Color', names='RED GREEN BLUE')).\n\n        The value lookup branch is chosen if the enum is final.\n\n        When used for the functional API:\n\n        `value` will be the name of the new class.\n\n        `names` should be either a string of white-space/comma delimited names\n        (values will start at `start`), or an iterator/mapping of name, value pairs.\n\n        `module` should be set to the module this class is being created in;\n        if it is not set, an attempt to find that module will be made, but if\n        it fails the class will not be picklable.\n\n        `qualname` should be set to the actual location this class can be found\n        at in its module; by default it is set to the global scope.  If this is\n        not correct, unpickling will fail in some circumstances.\n\n        `type`, if set, will be mixed in as the first base class.\n        \"\"\"\n        if cls._member_map_:\n            # simple value lookup if members exist\n            if names is not _not_given:\n                value = (value, names) + values\n            return cls.__new__(cls, value)\n        # otherwise, functional API: we're creating a new Enum type\n        if names is _not_given and type is None:\n            # no body? no data-type? possibly wrong usage\n            raise TypeError(\n                    f\"{cls} has no members; specify `names=()` if you meant to create a new, empty, enum\"\n                    )\n        return cls._create_(\n                class_name=value,\n                names=None if names is _not_given else names,\n                module=module,\n                qualname=qualname,\n                type=type,\n                start=start,\n                boundary=boundary,\n                )\n\n    def __contains__(cls, value):\n        \"\"\"Return True if `value` is in `cls`.\n\n        `value` is in `cls` if:\n        1) `value` is a member of `cls`, or\n        2) `value` is the value of one of the `cls`'s members.\n        \"\"\"\n        if isinstance(value, cls):\n            return True\n        try:\n            return value in cls._value2member_map_\n        except TypeError:\n            return value in cls._unhashable_values_\n\n    def __delattr__(cls, attr):\n        # nicer error message when someone tries to delete an attribute\n        # (see issue19025).\n        if attr in cls._member_map_:\n            raise AttributeError(\"%r cannot delete member %r.\" % (cls.__name__, attr))\n        super().__delattr__(attr)\n\n    def __dir__(cls):\n        interesting = set([\n                '__class__', '__contains__', '__doc__', '__getitem__',\n                '__iter__', '__len__', '__members__', '__module__',\n                '__name__', '__qualname__',\n                ]\n                + cls._member_names_\n                )\n        if cls._new_member_ is not object.__new__:\n            interesting.add('__new__')\n        if cls.__init_subclass__ is not object.__init_subclass__:\n            interesting.add('__init_subclass__')\n        if cls._member_type_ is object:\n            return sorted(interesting)\n        else:\n            # return whatever mixed-in data type has\n            return sorted(set(dir(cls._member_type_)) | interesting)\n\n    def __getitem__(cls, name):\n        \"\"\"\n        Return the member matching `name`.\n        \"\"\"\n        return cls._member_map_[name]\n\n    def __iter__(cls):\n        \"\"\"\n        Return members in definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in cls._member_names_)\n\n    def __len__(cls):\n        \"\"\"\n        Return the number of members (no aliases)\n        \"\"\"\n        return len(cls._member_names_)\n\n    @bltns.property\n    def __members__(cls):\n        \"\"\"\n        Returns a mapping of member name->value.\n\n        This mapping lists all enum members, including aliases. Note that this\n        is a read-only view of the internal mapping.\n        \"\"\"\n        return MappingProxyType(cls._member_map_)\n\n    def __repr__(cls):\n        if Flag is not None and issubclass(cls, Flag):\n            return \"<flag %r>\" % cls.__name__\n        else:\n            return \"<enum %r>\" % cls.__name__\n\n    def __reversed__(cls):\n        \"\"\"\n        Return members in reverse definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in reversed(cls._member_names_))\n\n    def __setattr__(cls, name, value):\n        \"\"\"\n        Block attempts to reassign Enum members.\n\n        A simple assignment to the class namespace only changes one of the\n        several possible ways to get an Enum member from the Enum class,\n        resulting in an inconsistent Enumeration.\n        \"\"\"\n        member_map = cls.__dict__.get('_member_map_', {})\n        if name in member_map:\n            raise AttributeError('cannot reassign member %r' % (name, ))\n        super().__setattr__(name, value)\n\n    def _create_(cls, class_name, names, *, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Convenience method to create a new Enum class.\n\n        `names` can be:\n\n        * A string containing member names, separated either with spaces or\n          commas.  Values are incremented by 1 from `start`.\n        * An iterable of member names.  Values are incremented by 1 from `start`.\n        * An iterable of (member name, value) pairs.\n        * A mapping of member name -> value pairs.\n        \"\"\"\n        metacls = cls.__class__\n        bases = (cls, ) if type is None else (type, cls)\n        _, first_enum = cls._get_mixins_(class_name, bases)\n        classdict = metacls.__prepare__(class_name, bases)\n\n        # special processing needed for names?\n        if isinstance(names, str):\n            names = names.replace(',', ' ').split()\n        if isinstance(names, (tuple, list)) and names and isinstance(names[0], str):\n            original_names, names = names, []\n            last_values = []\n            for count, name in enumerate(original_names):\n                value = first_enum._generate_next_value_(name, start, count, last_values[:])\n                last_values.append(value)\n                names.append((name, value))\n        if names is None:\n            names = ()\n\n        # Here, names is either an iterable of (name, value) or a mapping.\n        for item in names:\n            if isinstance(item, str):\n                member_name, member_value = item, names[item]\n            else:\n                member_name, member_value = item\n            classdict[member_name] = member_value\n\n        if module is None:\n            try:\n                module = sys._getframemodulename(2)\n            except AttributeError:\n                # Fall back on _getframe if _getframemodulename is missing\n                try:\n                    module = sys._getframe(2).f_globals['__name__']\n                except (AttributeError, ValueError, KeyError):\n                    pass\n        if module is None:\n            _make_class_unpicklable(classdict)\n        else:\n            classdict['__module__'] = module\n        if qualname is not None:\n            classdict['__qualname__'] = qualname\n\n        return metacls.__new__(metacls, class_name, bases, classdict, boundary=boundary)\n\n    def _convert_(cls, name, module, filter, source=None, *, boundary=None, as_global=False):\n        \"\"\"\n        Create a new Enum subclass that replaces a collection of global constants\n        \"\"\"\n        # convert all constants from source (or module) that pass filter() to\n        # a new Enum called name, and export the enum and its members back to\n        # module;\n        # also, replace the __reduce_ex__ method so unpickling works in\n        # previous Python versions\n        module_globals = sys.modules[module].__dict__\n        if source:\n            source = source.__dict__\n        else:\n            source = module_globals\n        # _value2member_map_ is populated in the same order every time\n        # for a consistent reverse mapping of number to name when there\n        # are multiple names for the same number.\n        members = [\n                (name, value)\n                for name, value in source.items()\n                if filter(name)]\n        try:\n            # sort by value\n            members.sort(key=lambda t: (t[1], t[0]))\n        except TypeError:\n            # unless some values aren't comparable, in which case sort by name\n            members.sort(key=lambda t: t[0])\n        body = {t[0]: t[1] for t in members}\n        body['__module__'] = module\n        tmp_cls = type(name, (object, ), body)\n        cls = _simple_enum(etype=cls, boundary=boundary or KEEP)(tmp_cls)\n        if as_global:\n            global_enum(cls)\n        else:\n            sys.modules[cls.__module__].__dict__.update(cls.__members__)\n        module_globals[name] = cls\n        return cls\n\n    @classmethod\n    def _check_for_existing_members_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if isinstance(base, EnumType) and base._member_names_:\n                    raise TypeError(\n                            \"<enum %r> cannot extend %r\"\n                            % (class_name, base)\n                            )\n\n    @classmethod\n    def _get_mixins_(mcls, class_name, bases):\n        \"\"\"\n        Returns the type for creating enum members, and the first inherited\n        enum class.\n\n        bases: the tuple of bases that was given to __new__\n        \"\"\"\n        if not bases:\n            return object, Enum\n        # ensure final parent class is an Enum derivative, find any concrete\n        # data type, and check that Enum has no members\n        first_enum = bases[-1]\n        if not isinstance(first_enum, EnumType):\n            raise TypeError(\"new enumerations should be created as \"\n                    \"`EnumName([mixin_type, ...] [data_type,] enum_type)`\")\n        member_type = mcls._find_data_type_(class_name, bases) or object\n        return member_type, first_enum\n\n    @classmethod\n    def _find_data_repr_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    # if we hit an Enum, use it's _value_repr_\n                    return base._value_repr_\n                elif '__repr__' in base.__dict__:\n                    # this is our data repr\n                    # double-check if a dataclass with a default __repr__\n                    if (\n                            '__dataclass_fields__' in base.__dict__\n                            and '__dataclass_params__' in base.__dict__\n                            and base.__dict__['__dataclass_params__'].repr\n                        ):\n                        return _dataclass_repr\n                    else:\n                        return base.__dict__['__repr__']\n        return None\n\n    @classmethod\n    def _find_data_type_(mcls, class_name, bases):\n        # a datatype has a __new__ method, or a __dataclass_fields__ attribute\n        data_types = set()\n        base_chain = set()\n        for chain in bases:\n            candidate = None\n            for base in chain.__mro__:\n                base_chain.add(base)\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    if base._member_type_ is not object:\n                        data_types.add(base._member_type_)\n                        break\n                elif '__new__' in base.__dict__ or '__dataclass_fields__' in base.__dict__:\n                    data_types.add(candidate or base)\n                    break\n                else:\n                    candidate = candidate or base\n        if len(data_types) > 1:\n            raise TypeError('too many data types for %r: %r' % (class_name, data_types))\n        elif data_types:\n            return data_types.pop()\n        else:\n            return None\n\n    @classmethod\n    def _find_new_(mcls, classdict, member_type, first_enum):\n        \"\"\"\n        Returns the __new__ to be used for creating the enum members.\n\n        classdict: the class dictionary given to __new__\n        member_type: the data type whose __new__ will be used by default\n        first_enum: enumeration to check for an overriding __new__\n        \"\"\"\n        # now find the correct __new__, checking to see of one was defined\n        # by the user; also check earlier enum classes in case a __new__ was\n        # saved as __new_member__\n        __new__ = classdict.get('__new__', None)\n\n        # should __new__ be saved as __new_member__ later?\n        save_new = first_enum is not None and __new__ is not None\n\n        if __new__ is None:\n            # check all possibles for __new_member__ before falling back to\n            # __new__\n            for method in ('__new_member__', '__new__'):\n                for possible in (member_type, first_enum):\n                    target = getattr(possible, method, None)\n                    if target not in {\n                            None,\n                            None.__new__,\n                            object.__new__,\n                            Enum.__new__,\n                            }:\n                        __new__ = target\n                        break\n                if __new__ is not None:\n                    break\n            else:\n                __new__ = object.__new__\n\n        # if a non-object.__new__ is used then whatever value/tuple was\n        # assigned to the enum member name will be passed to __new__ and to the\n        # new enum member's __init__\n        if first_enum is None or __new__ in (Enum.__new__, object.__new__):\n            use_args = False\n        else:\n            use_args = True\n        return __new__, save_new, use_args\n\n    def _add_member_(cls, name, member):\n        # _value_ structures are not updated\n        if name in cls._member_map_:\n            if cls._member_map_[name] is not member:\n                raise NameError('%r is already bound: %r' % (name, cls._member_map_[name]))\n            return\n        #\n        # if necessary, get redirect in place and then add it to _member_map_\n        found_descriptor = None\n        descriptor_type = None\n        class_type = None\n        for base in cls.__mro__[1:]:\n            attr = base.__dict__.get(name)\n            if attr is not None:\n                if isinstance(attr, (property, DynamicClassAttribute)):\n                    found_descriptor = attr\n                    class_type = base\n                    descriptor_type = 'enum'\n                    break\n                elif _is_descriptor(attr):\n                    found_descriptor = attr\n                    descriptor_type = descriptor_type or 'desc'\n                    class_type = class_type or base\n                    continue\n                else:\n                    descriptor_type = 'attr'\n                    class_type = base\n        if found_descriptor:\n            redirect = property()\n            redirect.member = member\n            redirect.__set_name__(cls, name)\n            if descriptor_type in ('enum', 'desc'):\n                # earlier descriptor found; copy fget, fset, fdel to this one.\n                redirect.fget = getattr(found_descriptor, 'fget', None)\n                redirect._get = getattr(found_descriptor, '__get__', None)\n                redirect.fset = getattr(found_descriptor, 'fset', None)\n                redirect._set = getattr(found_descriptor, '__set__', None)\n                redirect.fdel = getattr(found_descriptor, 'fdel', None)\n                redirect._del = getattr(found_descriptor, '__delete__', None)\n            redirect._attr_type = descriptor_type\n            redirect._cls_type = class_type\n            setattr(cls, name, redirect)\n        else:\n            setattr(cls, name, member)\n        # now add to _member_map_ (even aliases)\n        cls._member_map_[name] = member\n\nEnumMeta = EnumType         # keep EnumMeta name for backwards compatibility\n\n\nclass Enum(metaclass=EnumType):\n    \"\"\"\n    Create a collection of name/value pairs.\n\n    Example enumeration:\n\n    >>> class Color(Enum):\n    ...     RED = 1\n    ...     BLUE = 2\n    ...     GREEN = 3\n\n    Access them by:\n\n    - attribute access:\n\n      >>> Color.RED\n      <Color.RED: 1>\n\n    - value lookup:\n\n      >>> Color(1)\n      <Color.RED: 1>\n\n    - name lookup:\n\n      >>> Color['RED']\n      <Color.RED: 1>\n\n    Enumerations can be iterated over, and know how many members they have:\n\n    >>> len(Color)\n    3\n\n    >>> list(Color)\n    [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\n    Methods can be added to enumerations, and members can have their own\n    attributes -- see the documentation for details.\n    \"\"\"\n\n    @classmethod\n    def __signature__(cls):\n        if cls._member_names_:\n            return '(*values)'\n        else:\n            return '(new_class_name, /, names, *, module=None, qualname=None, type=None, start=1, boundary=None)'\n\n    def __new__(cls, value):\n        # all enum instances are actually created during class construction\n        # without calling this method; this method is called by the metaclass'\n        # __call__ (i.e. Color(3) ), and by pickle\n        if type(value) is cls:\n            # For lookups like Color(Color.RED)\n            return value\n        # by-value search for a matching enum member\n        # see if it's in the reverse mapping (for hashable values)\n        try:\n            return cls._value2member_map_[value]\n        except KeyError:\n            # Not found, no need to do long O(n) search\n            pass\n        except TypeError:\n            # not there, now do long search -- O(n) behavior\n            for name, values in cls._unhashable_values_map_.items():\n                if value in values:\n                    return cls[name]\n        # still not found -- verify that members exist, in-case somebody got here mistakenly\n        # (such as via super when trying to override __new__)\n        if not cls._member_map_:\n            if getattr(cls, '_%s__in_progress' % cls.__name__, False):\n                raise TypeError('do not use `super().__new__; call the appropriate __new__ directly') from None\n            raise TypeError(\"%r has no members defined\" % cls)\n        #\n        # still not found -- try _missing_ hook\n        try:\n            exc = None\n            result = cls._missing_(value)\n        except Exception as e:\n            exc = e\n            result = None\n        try:\n            if isinstance(result, cls):\n                return result\n            elif (\n                    Flag is not None and issubclass(cls, Flag)\n                    and cls._boundary_ is EJECT and isinstance(result, int)\n                ):\n                return result\n            else:\n                ve_exc = ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n                if result is None and exc is None:\n                    raise ve_exc\n                elif exc is None:\n                    exc = TypeError(\n                            'error in %s._missing_: returned %r instead of None or a valid member'\n                            % (cls.__name__, result)\n                            )\n                if not isinstance(exc, ValueError):\n                    exc.__context__ = ve_exc\n                raise exc\n        finally:\n            # ensure all variables that could hold an exception are destroyed\n            exc = None\n            ve_exc = None\n\n    def __init__(self, *args, **kwds):\n        pass\n\n    def _add_alias_(self, name):\n        self.__class__._add_member_(name, self)\n\n    def _add_value_alias_(self, value):\n        cls = self.__class__\n        try:\n            if value in cls._value2member_map_:\n                if cls._value2member_map_[value] is not self:\n                    raise ValueError('%r is already bound: %r' % (value, cls._value2member_map_[value]))\n                return\n        except TypeError:\n            # unhashable value, do long search\n            for m in cls._member_map_.values():\n                if m._value_ == value:\n                    if m is not self:\n                        raise ValueError('%r is already bound: %r' % (value, cls._value2member_map_[value]))\n                    return\n        try:\n            # This may fail if value is not hashable. We can't add the value\n            # to the map, and by-value lookups for this value will be\n            # linear.\n            cls._value2member_map_.setdefault(value, self)\n        except TypeError:\n            # keep track of the value in a list so containment checks are quick\n            cls._unhashable_values_.append(value)\n            cls._unhashable_values_map_.setdefault(self.name, []).append(value)\n\n    @staticmethod\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the list of values assigned\n        \"\"\"\n        if not last_values:\n            return start\n        try:\n            last_value = sorted(last_values).pop()\n        except TypeError:\n            raise TypeError('unable to sort non-numeric values') from None\n        try:\n            return last_value + 1\n        except TypeError:\n            raise TypeError('unable to increment %r' % (last_value, )) from None\n\n    @classmethod\n    def _missing_(cls, value):\n        return None\n\n    def __repr__(self):\n        v_repr = self.__class__._value_repr_ or repr\n        return \"<%s.%s: %s>\" % (self.__class__.__name__, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        return \"%s.%s\" % (self.__class__.__name__, self._name_, )\n\n    def __dir__(self):\n        \"\"\"\n        Returns public methods and other interesting attributes.\n        \"\"\"\n        interesting = set()\n        if self.__class__._member_type_ is not object:\n            interesting = set(object.__dir__(self))\n        for name in getattr(self, '__dict__', []):\n            if name[0] != '_' and name not in self._member_map_:\n                interesting.add(name)\n        for cls in self.__class__.mro():\n            for name, obj in cls.__dict__.items():\n                if name[0] == '_':\n                    continue\n                if isinstance(obj, property):\n                    # that's an enum.property\n                    if obj.fget is not None or name not in self._member_map_:\n                        interesting.add(name)\n                    else:\n                        # in case it was added by `dir(self)`\n                        interesting.discard(name)\n                elif name not in self._member_map_:\n                    interesting.add(name)\n        names = sorted(\n                set(['__class__', '__doc__', '__eq__', '__hash__', '__module__'])\n                | interesting\n                )\n        return names\n\n    def __format__(self, format_spec):\n        return str.__format__(str(self), format_spec)\n\n    def __hash__(self):\n        return hash(self._name_)\n\n    def __reduce_ex__(self, proto):\n        return self.__class__, (self._value_, )\n\n    def __deepcopy__(self,memo):\n        return self\n\n    def __copy__(self):\n        return self\n\n    # enum.property is used to provide access to the `name` and\n    # `value` attributes of enum members while keeping some measure of\n    # protection from modification, while still allowing for an enumeration\n    # to have members named `name` and `value`.  This works because each\n    # instance of enum.property saves its companion member, which it returns\n    # on class lookup; on instance lookup it either executes a provided function\n    # or raises an AttributeError.\n\n    @property\n    def name(self):\n        \"\"\"The name of the Enum member.\"\"\"\n        return self._name_\n\n    @property\n    def value(self):\n        \"\"\"The value of the Enum member.\"\"\"\n        return self._value_\n\n\nclass ReprEnum(Enum):\n    \"\"\"\n    Only changes the repr(), leaving str() and format() to the mixed-in type.\n    \"\"\"\n\n\nclass IntEnum(int, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) ints\n    \"\"\"\n\n\nclass StrEnum(str, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) strings\n    \"\"\"\n\n    def __new__(cls, *values):\n        \"values must already be of type `str`\"\n        if len(values) > 3:\n            raise TypeError('too many arguments for str(): %r' % (values, ))\n        if len(values) == 1:\n            # it must be a string\n            if not isinstance(values[0], str):\n                raise TypeError('%r is not a string' % (values[0], ))\n        if len(values) >= 2:\n            # check that encoding argument is a string\n            if not isinstance(values[1], str):\n                raise TypeError('encoding must be a string, not %r' % (values[1], ))\n        if len(values) == 3:\n            # check that errors argument is a string\n            if not isinstance(values[2], str):\n                raise TypeError('errors must be a string, not %r' % (values[2]))\n        value = str(*values)\n        member = str.__new__(cls, value)\n        member._value_ = value\n        return member\n\n    @staticmethod\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Return the lower-cased version of the member name.\n        \"\"\"\n        return name.lower()\n\n\ndef pickle_by_global_name(self, proto):\n    # should not be used with Flag-type enums\n    return self.name\n_reduce_ex_by_global_name = pickle_by_global_name\n\ndef pickle_by_enum_name(self, proto):\n    # should not be used with Flag-type enums\n    return getattr, (self.__class__, self._name_)\n\nclass FlagBoundary(StrEnum):\n    \"\"\"\n    control how out of range values are handled\n    \"strict\" -> error is raised             [default for Flag]\n    \"conform\" -> extra bits are discarded\n    \"eject\" -> lose flag status\n    \"keep\" -> keep flag status and all bits [default for IntFlag]\n    \"\"\"\n    STRICT = auto()\n    CONFORM = auto()\n    EJECT = auto()\n    KEEP = auto()\nSTRICT, CONFORM, EJECT, KEEP = FlagBoundary\n\n\nclass Flag(Enum, boundary=STRICT):\n    \"\"\"\n    Support for flags\n    \"\"\"\n\n    _numeric_repr_ = repr\n\n    @staticmethod\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the last value assigned or None\n        \"\"\"\n        if not count:\n            return start if start is not None else 1\n        last_value = max(last_values)\n        try:\n            high_bit = _high_bit(last_value)\n        except Exception:\n            raise TypeError('invalid flag value %r' % last_value) from None\n        return 2 ** (high_bit+1)\n\n    @classmethod\n    def _iter_member_by_value_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition (i.e. increasing value) order.\n        \"\"\"\n        for val in _iter_bits_lsb(value & cls._flag_mask_):\n            yield cls._value2member_map_.get(val)\n\n    _iter_member_ = _iter_member_by_value_\n\n    @classmethod\n    def _iter_member_by_def_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition order.\n        \"\"\"\n        yield from sorted(\n                cls._iter_member_by_value_(value),\n                key=lambda m: m._sort_order_,\n                )\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Create a composite member containing all canonical members present in `value`.\n\n        If non-member values are present, result depends on `_boundary_` setting.\n        \"\"\"\n        if not isinstance(value, int):\n            raise ValueError(\n                    \"%r is not a valid %s\" % (value, cls.__qualname__)\n                    )\n        # check boundaries\n        # - value must be in range (e.g. -16 <-> +15, i.e. ~15 <-> 15)\n        # - value must not include any skipped flags (e.g. if bit 2 is not\n        #   defined, then 0d10 is invalid)\n        flag_mask = cls._flag_mask_\n        singles_mask = cls._singles_mask_\n        all_bits = cls._all_bits_\n        neg_value = None\n        if (\n                not ~all_bits <= value <= all_bits\n                or value & (all_bits ^ flag_mask)\n            ):\n            if cls._boundary_ is STRICT:\n                max_bits = max(value.bit_length(), flag_mask.bit_length())\n                raise ValueError(\n                        \"%r invalid value %r\\n    given %s\\n  allowed %s\" % (\n                            cls, value, bin(value, max_bits), bin(flag_mask, max_bits),\n                            ))\n            elif cls._boundary_ is CONFORM:\n                value = value & flag_mask\n            elif cls._boundary_ is EJECT:\n                return value\n            elif cls._boundary_ is KEEP:\n                if value < 0:\n                    value = (\n                            max(all_bits+1, 2**(value.bit_length()))\n                            + value\n                            )\n            else:\n                raise ValueError(\n                        '%r unknown flag boundary %r' % (cls, cls._boundary_, )\n                        )\n        if value < 0:\n            neg_value = value\n            value = all_bits + 1 + value\n        # get members and unknown\n        unknown = value & ~flag_mask\n        aliases = value & ~singles_mask\n        member_value = value & singles_mask\n        if unknown and cls._boundary_ is not KEEP:\n            raise ValueError(\n                    '%s(%r) -->  unknown values %r [%s]'\n                    % (cls.__name__, value, unknown, bin(unknown))\n                    )\n        # normal Flag?\n        if cls._member_type_ is object:\n            # construct a singleton enum pseudo-member\n            pseudo_member = object.__new__(cls)\n        else:\n            pseudo_member = cls._member_type_.__new__(cls, value)\n        if not hasattr(pseudo_member, '_value_'):\n            pseudo_member._value_ = value\n        if member_value or aliases:\n            members = []\n            combined_value = 0\n            for m in cls._iter_member_(member_value):\n                members.append(m)\n                combined_value |= m._value_\n            if aliases:\n                value = member_value | aliases\n                for n, pm in cls._member_map_.items():\n                    if pm not in members and pm._value_ and pm._value_ & value == pm._value_:\n                        members.append(pm)\n                        combined_value |= pm._value_\n            unknown = value ^ combined_value\n            pseudo_member._name_ = '|'.join([m._name_ for m in members])\n            if not combined_value:\n                pseudo_member._name_ = None\n            elif unknown and cls._boundary_ is STRICT:\n                raise ValueError('%r: no members with value %r' % (cls, unknown))\n            elif unknown:\n                pseudo_member._name_ += '|%s' % cls._numeric_repr_(unknown)\n        else:\n            pseudo_member._name_ = None\n        # use setdefault in case another thread already created a composite\n        # with this value\n        # note: zero is a special case -- always add it\n        pseudo_member = cls._value2member_map_.setdefault(value, pseudo_member)\n        if neg_value is not None:\n            cls._value2member_map_[neg_value] = pseudo_member\n        return pseudo_member\n\n    def __contains__(self, other):\n        \"\"\"\n        Returns True if self has at least the same flags set as other.\n        \"\"\"\n        if not isinstance(other, self.__class__):\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': %r and %r\" % (\n                    type(other).__qualname__, self.__class__.__qualname__))\n        return other._value_ & self._value_ == other._value_\n\n    def __iter__(self):\n        \"\"\"\n        Returns flags in definition order.\n        \"\"\"\n        yield from self._iter_member_(self._value_)\n\n    def __len__(self):\n        return self._value_.bit_count()\n\n    def __repr__(self):\n        cls_name = self.__class__.__name__\n        v_repr = self.__class__._value_repr_ or repr\n        if self._name_ is None:\n            return \"<%s: %s>\" % (cls_name, v_repr(self._value_))\n        else:\n            return \"<%s.%s: %s>\" % (cls_name, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        cls_name = self.__class__.__name__\n        if self._name_ is None:\n            return '%s(%r)' % (cls_name, self._value_)\n        else:\n            return \"%s.%s\" % (cls_name, self._name_)\n\n    def __bool__(self):\n        return bool(self._value_)\n\n    def _get_value(self, flag):\n        if isinstance(flag, self.__class__):\n            return flag._value_\n        elif self._member_type_ is not object and isinstance(flag, self._member_type_):\n            return flag\n        return NotImplemented\n\n    def __or__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with |\")\n        value = self._value_\n        return self.__class__(value | other_value)\n\n    def __and__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with &\")\n        value = self._value_\n        return self.__class__(value & other_value)\n\n    def __xor__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with ^\")\n        value = self._value_\n        return self.__class__(value ^ other_value)\n\n    def __invert__(self):\n        if self._get_value(self) is None:\n            raise TypeError(f\"'{self}' cannot be inverted\")\n\n        if self._inverted_ is None:\n            if self._boundary_ in (EJECT, KEEP):\n                self._inverted_ = self.__class__(~self._value_)\n            else:\n                self._inverted_ = self.__class__(self._singles_mask_ & ~self._value_)\n        return self._inverted_\n\n    __rand__ = __and__\n    __ror__ = __or__\n    __rxor__ = __xor__\n\n\nclass IntFlag(int, ReprEnum, Flag, boundary=KEEP):\n    \"\"\"\n    Support for integer-based Flags\n    \"\"\"\n\n\ndef _high_bit(value):\n    \"\"\"\n    returns index of highest bit, or -1 if value is zero or negative\n    \"\"\"\n    return value.bit_length() - 1\n\ndef unique(enumeration):\n    \"\"\"\n    Class decorator for enumerations ensuring unique member values.\n    \"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        alias_details = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n        raise ValueError('duplicate values found in %r: %s' %\n                (enumeration, alias_details))\n    return enumeration\n\ndef _dataclass_repr(self):\n    dcf = self.__dataclass_fields__\n    return ', '.join(\n            '%s=%r' % (k, getattr(self, k))\n            for k in dcf.keys()\n            if dcf[k].repr\n            )\n\ndef global_enum_repr(self):\n    \"\"\"\n    use module.enum_name instead of class.enum_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    return '%s.%s' % (module, self._name_)\n\ndef global_flag_repr(self):\n    \"\"\"\n    use module.flag_name instead of class.flag_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    cls_name = self.__class__.__name__\n    if self._name_ is None:\n        return \"%s.%s(%r)\" % (module, cls_name, self._value_)\n    if _is_single_bit(self._value_):\n        return '%s.%s' % (module, self._name_)\n    if self._boundary_ is not FlagBoundary.KEEP:\n        return '|'.join(['%s.%s' % (module, name) for name in self.name.split('|')])\n    else:\n        name = []\n        for n in self._name_.split('|'):\n            if n[0].isdigit():\n                name.append(n)\n            else:\n                name.append('%s.%s' % (module, n))\n        return '|'.join(name)\n\ndef global_str(self):\n    \"\"\"\n    use enum_name instead of class.enum_name\n    \"\"\"\n    if self._name_ is None:\n        cls_name = self.__class__.__name__\n        return \"%s(%r)\" % (cls_name, self._value_)\n    else:\n        return self._name_\n\ndef global_enum(cls, update_str=False):\n    \"\"\"\n    decorator that makes the repr() of an enum member reference its module\n    instead of its class; also exports all members to the enum's module's\n    global namespace\n    \"\"\"\n    if issubclass(cls, Flag):\n        cls.__repr__ = global_flag_repr\n    else:\n        cls.__repr__ = global_enum_repr\n    if not issubclass(cls, ReprEnum) or update_str:\n        cls.__str__ = global_str\n    sys.modules[cls.__module__].__dict__.update(cls.__members__)\n    return cls\n\ndef _simple_enum(etype=Enum, *, boundary=None, use_args=None):\n    \"\"\"\n    Class decorator that converts a normal class into an :class:`Enum`.  No\n    safety checks are done, and some advanced behavior (such as\n    :func:`__init_subclass__`) is not available.  Enum creation can be faster\n    using :func:`_simple_enum`.\n\n        >>> from enum import Enum, _simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> Color\n        <enum 'Color'>\n    \"\"\"\n    def convert_class(cls):\n        nonlocal use_args\n        cls_name = cls.__name__\n        if use_args is None:\n            use_args = etype._use_args_\n        __new__ = cls.__dict__.get('__new__')\n        if __new__ is not None:\n            new_member = __new__.__func__\n        else:\n            new_member = etype._member_type_.__new__\n        attrs = {}\n        body = {}\n        if __new__ is not None:\n            body['__new_member__'] = new_member\n        body['_new_member_'] = new_member\n        body['_use_args_'] = use_args\n        body['_generate_next_value_'] = gnv = etype._generate_next_value_\n        body['_member_names_'] = member_names = []\n        body['_member_map_'] = member_map = {}\n        body['_value2member_map_'] = value2member_map = {}\n        body['_unhashable_values_'] = unhashable_values = []\n        body['_unhashable_values_map_'] = {}\n        body['_member_type_'] = member_type = etype._member_type_\n        body['_value_repr_'] = etype._value_repr_\n        if issubclass(etype, Flag):\n            body['_boundary_'] = boundary or etype._boundary_\n            body['_flag_mask_'] = None\n            body['_all_bits_'] = None\n            body['_singles_mask_'] = None\n            body['_inverted_'] = None\n            body['__or__'] = Flag.__or__\n            body['__xor__'] = Flag.__xor__\n            body['__and__'] = Flag.__and__\n            body['__ror__'] = Flag.__ror__\n            body['__rxor__'] = Flag.__rxor__\n            body['__rand__'] = Flag.__rand__\n            body['__invert__'] = Flag.__invert__\n        for name, obj in cls.__dict__.items():\n            if name in ('__dict__', '__weakref__'):\n                continue\n            if _is_dunder(name) or _is_private(cls_name, name) or _is_sunder(name) or _is_descriptor(obj):\n                body[name] = obj\n            else:\n                attrs[name] = obj\n        if cls.__dict__.get('__doc__') is None:\n            body['__doc__'] = 'An enumeration.'\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        enum_class = type(cls_name, (etype, ), body, boundary=boundary, _simple=True)\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in body:\n                # check for mixin overrides before replacing\n                enum_method = getattr(etype, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        gnv_last_values = []\n        if issubclass(enum_class, Flag):\n            # Flag / IntFlag\n            single_bits = multi_bits = 0\n            for name, value in attrs.items():\n                if isinstance(value, auto) and auto.value is _auto_null:\n                    value = gnv(name, 1, len(member_names), gnv_last_values)\n                # create basic member (possibly isolate value for alias check)\n                if use_args:\n                    if not isinstance(value, tuple):\n                        value = (value, )\n                    member = new_member(enum_class, *value)\n                    value = value[0]\n                else:\n                    member = new_member(enum_class)\n                if __new__ is None:\n                    member._value_ = value\n                # now check if alias\n                try:\n                    contained = value2member_map.get(member._value_)\n                except TypeError:\n                    contained = None\n                    if member._value_ in unhashable_values:\n                        for m in enum_class:\n                            if m._value_ == member._value_:\n                                contained = m\n                                break\n                if contained is not None:\n                    # an alias to an existing member\n                    contained._add_alias_(name)\n                else:\n                    # finish creating member\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    member._sort_order_ = len(member_names)\n                    if name not in ('name', 'value'):\n                        setattr(enum_class, name, member)\n                        member_map[name] = member\n                    else:\n                        enum_class._add_member_(name, member)\n                    value2member_map[value] = member\n                    if _is_single_bit(value):\n                        # not a multi-bit alias, record in _member_names_ and _flag_mask_\n                        member_names.append(name)\n                        single_bits |= value\n                    else:\n                        multi_bits |= value\n                    gnv_last_values.append(value)\n            enum_class._flag_mask_ = single_bits | multi_bits\n            enum_class._singles_mask_ = single_bits\n            enum_class._all_bits_ = 2 ** ((single_bits|multi_bits).bit_length()) - 1\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n        else:\n            # Enum / IntEnum / StrEnum\n            for name, value in attrs.items():\n                if isinstance(value, auto):\n                    if value.value is _auto_null:\n                        value.value = gnv(name, 1, len(member_names), gnv_last_values)\n                    value = value.value\n                # create basic member (possibly isolate value for alias check)\n                if use_args:\n                    if not isinstance(value, tuple):\n                        value = (value, )\n                    member = new_member(enum_class, *value)\n                    value = value[0]\n                else:\n                    member = new_member(enum_class)\n                if __new__ is None:\n                    member._value_ = value\n                # now check if alias\n                try:\n                    contained = value2member_map.get(member._value_)\n                except TypeError:\n                    contained = None\n                    if member._value_ in unhashable_values:\n                        for m in enum_class:\n                            if m._value_ == member._value_:\n                                contained = m\n                                break\n                if contained is not None:\n                    # an alias to an existing member\n                    contained._add_alias_(name)\n                else:\n                    # finish creating member\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    member._sort_order_ = len(member_names)\n                    if name not in ('name', 'value'):\n                        setattr(enum_class, name, member)\n                        member_map[name] = member\n                    else:\n                        enum_class._add_member_(name, member)\n                    member_names.append(name)\n                    gnv_last_values.append(value)\n                    try:\n                        # This may fail if value is not hashable. We can't add the value\n                        # to the map, and by-value lookups for this value will be\n                        # linear.\n                        enum_class._value2member_map_.setdefault(value, member)\n                    except TypeError:\n                        # keep track of the value in a list so containment checks are quick\n                        enum_class._unhashable_values_.append(value)\n                        enum_class._unhashable_values_map_.setdefault(name, []).append(value)\n        if '__new__' in body:\n            enum_class.__new_member__ = enum_class.__new__\n        enum_class.__new__ = Enum.__new__\n        return enum_class\n    return convert_class\n\n@_simple_enum(StrEnum)\nclass EnumCheck:\n    \"\"\"\n    various conditions to check an enumeration for\n    \"\"\"\n    CONTINUOUS = \"no skipped integer values\"\n    NAMED_FLAGS = \"multi-flag aliases may not contain unnamed flags\"\n    UNIQUE = \"one name per value\"\nCONTINUOUS, NAMED_FLAGS, UNIQUE = EnumCheck\n\n\nclass verify:\n    \"\"\"\n    Check an enumeration for various constraints. (see EnumCheck)\n    \"\"\"\n    def __init__(self, *checks):\n        self.checks = checks\n    def __call__(self, enumeration):\n        checks = self.checks\n        cls_name = enumeration.__name__\n        if Flag is not None and issubclass(enumeration, Flag):\n            enum_type = 'flag'\n        elif issubclass(enumeration, Enum):\n            enum_type = 'enum'\n        else:\n            raise TypeError(\"the 'verify' decorator only works with Enum and Flag\")\n        for check in checks:\n            if check is UNIQUE:\n                # check for duplicate names\n                duplicates = []\n                for name, member in enumeration.__members__.items():\n                    if name != member.name:\n                        duplicates.append((name, member.name))\n                if duplicates:\n                    alias_details = ', '.join(\n                            [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n                    raise ValueError('aliases found in %r: %s' %\n                            (enumeration, alias_details))\n            elif check is CONTINUOUS:\n                values = set(e.value for e in enumeration)\n                if len(values) < 2:\n                    continue\n                low, high = min(values), max(values)\n                missing = []\n                if enum_type == 'flag':\n                    # check for powers of two\n                    for i in range(_high_bit(low)+1, _high_bit(high)):\n                        if 2**i not in values:\n                            missing.append(2**i)\n                elif enum_type == 'enum':\n                    # check for powers of one\n                    for i in range(low+1, high):\n                        if i not in values:\n                            missing.append(i)\n                else:\n                    raise Exception('verify: unknown type %r' % enum_type)\n                if missing:\n                    raise ValueError(('invalid %s %r: missing values %s' % (\n                            enum_type, cls_name, ', '.join((str(m) for m in missing)))\n                            )[:256])\n                            # limit max length to protect against DOS attacks\n            elif check is NAMED_FLAGS:\n                # examine each alias and check for unnamed flags\n                member_names = enumeration._member_names_\n                member_values = [m.value for m in enumeration]\n                missing_names = []\n                missing_value = 0\n                for name, alias in enumeration._member_map_.items():\n                    if name in member_names:\n                        # not an alias\n                        continue\n                    if alias.value < 0:\n                        # negative numbers are not checked\n                        continue\n                    values = list(_iter_bits_lsb(alias.value))\n                    missed = [v for v in values if v not in member_values]\n                    if missed:\n                        missing_names.append(name)\n                        for val in missed:\n                            missing_value |= val\n                if missing_names:\n                    if len(missing_names) == 1:\n                        alias = 'alias %s is missing' % missing_names[0]\n                    else:\n                        alias = 'aliases %s and %s are missing' % (\n                                ', '.join(missing_names[:-1]), missing_names[-1]\n                                )\n                    if _is_single_bit(missing_value):\n                        value = 'value 0x%x' % missing_value\n                    else:\n                        value = 'combined values of 0x%x' % missing_value\n                    raise ValueError(\n                            'invalid Flag %r: %s %s [use enum.show_flag_values(value) for details]'\n                            % (cls_name, alias, value)\n                            )\n        return enumeration\n\ndef _test_simple_enum(checked_enum, simple_enum):\n    \"\"\"\n    A function that can be used to test an enum created with :func:`_simple_enum`\n    against the version created by subclassing :class:`Enum`::\n\n        >>> from enum import Enum, _simple_enum, _test_simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> class CheckedColor(Enum):\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> _test_simple_enum(CheckedColor, Color)\n\n    If differences are found, a :exc:`TypeError` is raised.\n    \"\"\"\n    failed = []\n    if checked_enum.__dict__ != simple_enum.__dict__:\n        checked_dict = checked_enum.__dict__\n        checked_keys = list(checked_dict.keys())\n        simple_dict = simple_enum.__dict__\n        simple_keys = list(simple_dict.keys())\n        member_names = set(\n                list(checked_enum._member_map_.keys())\n                + list(simple_enum._member_map_.keys())\n                )\n        for key in set(checked_keys + simple_keys):\n            if key in ('__module__', '_member_map_', '_value2member_map_', '__doc__',\n                       '__static_attributes__', '__firstlineno__'):\n                # keys known to be different, or very long\n                continue\n            elif key in member_names:\n                # members are checked below\n                continue\n            elif key not in simple_keys:\n                failed.append(\"missing key: %r\" % (key, ))\n            elif key not in checked_keys:\n                failed.append(\"extra key:   %r\" % (key, ))\n            else:\n                checked_value = checked_dict[key]\n                simple_value = simple_dict[key]\n                if callable(checked_value) or isinstance(checked_value, bltns.property):\n                    continue\n                if key == '__doc__':\n                    # remove all spaces/tabs\n                    compressed_checked_value = checked_value.replace(' ','').replace('\\t','')\n                    compressed_simple_value = simple_value.replace(' ','').replace('\\t','')\n                    if compressed_checked_value != compressed_simple_value:\n                        failed.append(\"%r:\\n         %s\\n         %s\" % (\n                                key,\n                                \"checked -> %r\" % (checked_value, ),\n                                \"simple  -> %r\" % (simple_value, ),\n                                ))\n                elif checked_value != simple_value:\n                    failed.append(\"%r:\\n         %s\\n         %s\" % (\n                            key,\n                            \"checked -> %r\" % (checked_value, ),\n                            \"simple  -> %r\" % (simple_value, ),\n                            ))\n        failed.sort()\n        for name in member_names:\n            failed_member = []\n            if name not in simple_keys:\n                failed.append('missing member from simple enum: %r' % name)\n            elif name not in checked_keys:\n                failed.append('extra member in simple enum: %r' % name)\n            else:\n                checked_member_dict = checked_enum[name].__dict__\n                checked_member_keys = list(checked_member_dict.keys())\n                simple_member_dict = simple_enum[name].__dict__\n                simple_member_keys = list(simple_member_dict.keys())\n                for key in set(checked_member_keys + simple_member_keys):\n                    if key in ('__module__', '__objclass__', '_inverted_'):\n                        # keys known to be different or absent\n                        continue\n                    elif key not in simple_member_keys:\n                        failed_member.append(\"missing key %r not in the simple enum member %r\" % (key, name))\n                    elif key not in checked_member_keys:\n                        failed_member.append(\"extra key %r in simple enum member %r\" % (key, name))\n                    else:\n                        checked_value = checked_member_dict[key]\n                        simple_value = simple_member_dict[key]\n                        if checked_value != simple_value:\n                            failed_member.append(\"%r:\\n         %s\\n         %s\" % (\n                                    key,\n                                    \"checked member -> %r\" % (checked_value, ),\n                                    \"simple member  -> %r\" % (simple_value, ),\n                                    ))\n            if failed_member:\n                failed.append('%r member mismatch:\\n      %s' % (\n                        name, '\\n      '.join(failed_member),\n                        ))\n        for method in (\n                '__str__', '__repr__', '__reduce_ex__', '__format__',\n                '__getnewargs_ex__', '__getnewargs__', '__reduce_ex__', '__reduce__'\n            ):\n            if method in simple_keys and method in checked_keys:\n                # cannot compare functions, and it exists in both, so we're good\n                continue\n            elif method not in simple_keys and method not in checked_keys:\n                # method is inherited -- check it out\n                checked_method = getattr(checked_enum, method, None)\n                simple_method = getattr(simple_enum, method, None)\n                if hasattr(checked_method, '__func__'):\n                    checked_method = checked_method.__func__\n                    simple_method = simple_method.__func__\n                if checked_method != simple_method:\n                    failed.append(\"%r:  %-30s %s\" % (\n                            method,\n                            \"checked -> %r\" % (checked_method, ),\n                            \"simple -> %r\" % (simple_method, ),\n                            ))\n            else:\n                # if the method existed in only one of the enums, it will have been caught\n                # in the first checks above\n                pass\n    if failed:\n        raise TypeError('enum mismatch:\\n   %s' % '\\n   '.join(failed))\n\ndef _old_convert_(etype, name, module, filter, source=None, *, boundary=None):\n    \"\"\"\n    Create a new Enum subclass that replaces a collection of global constants\n    \"\"\"\n    # convert all constants from source (or module) that pass filter() to\n    # a new Enum called name, and export the enum and its members back to\n    # module;\n    # also, replace the __reduce_ex__ method so unpickling works in\n    # previous Python versions\n    module_globals = sys.modules[module].__dict__\n    if source:\n        source = source.__dict__\n    else:\n        source = module_globals\n    # _value2member_map_ is populated in the same order every time\n    # for a consistent reverse mapping of number to name when there\n    # are multiple names for the same number.\n    members = [\n            (name, value)\n            for name, value in source.items()\n            if filter(name)]\n    try:\n        # sort by value\n        members.sort(key=lambda t: (t[1], t[0]))\n    except TypeError:\n        # unless some values aren't comparable, in which case sort by name\n        members.sort(key=lambda t: t[0])\n    cls = etype(name, members, module=module, boundary=boundary or KEEP)\n    return cls\n\n_stdlib_enums = IntEnum, StrEnum, IntFlag\n", 2162], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/copy.py": ["\"\"\"Generic (shallow and deep) copying operations.\n\nInterface summary:\n\n        import copy\n\n        x = copy.copy(y)                # make a shallow copy of y\n        x = copy.deepcopy(y)            # make a deep copy of y\n        x = copy.replace(y, a=1, b=2)   # new object with fields replaced, as defined by `__replace__`\n\nFor module specific errors, copy.Error is raised.\n\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\n\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\n\nPython's deep copy operation avoids these problems by:\n\n a) keeping a table of objects already copied during the current\n    copying pass\n\n b) letting user-defined classes override the copying operation or the\n    set of components copied\n\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor any\nsimilar types.\n\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods.\n\"\"\"\n\nimport types\nimport weakref\nfrom copyreg import dispatch_table\n\nclass Error(Exception):\n    pass\nerror = Error   # backward compatibility\n\n__all__ = [\"Error\", \"copy\", \"deepcopy\", \"replace\"]\n\ndef copy(x):\n    \"\"\"Shallow copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    cls = type(x)\n\n    copier = _copy_dispatch.get(cls)\n    if copier:\n        return copier(x)\n\n    if issubclass(cls, type):\n        # treat it as a regular class:\n        return _copy_immutable(x)\n\n    copier = getattr(cls, \"__copy__\", None)\n    if copier is not None:\n        return copier(x)\n\n    reductor = dispatch_table.get(cls)\n    if reductor is not None:\n        rv = reductor(x)\n    else:\n        reductor = getattr(x, \"__reduce_ex__\", None)\n        if reductor is not None:\n            rv = reductor(4)\n        else:\n            reductor = getattr(x, \"__reduce__\", None)\n            if reductor:\n                rv = reductor()\n            else:\n                raise Error(\"un(shallow)copyable object of type %s\" % cls)\n\n    if isinstance(rv, str):\n        return x\n    return _reconstruct(x, None, *rv)\n\n\n_copy_dispatch = d = {}\n\ndef _copy_immutable(x):\n    return x\nfor t in (types.NoneType, int, float, bool, complex, str, tuple,\n          bytes, frozenset, type, range, slice, property,\n          types.BuiltinFunctionType, types.EllipsisType,\n          types.NotImplementedType, types.FunctionType, types.CodeType,\n          weakref.ref):\n    d[t] = _copy_immutable\n\nd[list] = list.copy\nd[dict] = dict.copy\nd[set] = set.copy\nd[bytearray] = bytearray.copy\n\ndel d, t\n\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    d = id(x)\n    if memo is None:\n        memo = {}\n    else:\n        y = memo.get(d, _nil)\n        if y is not _nil:\n            return y\n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier is not None:\n        y = copier(x, memo)\n    else:\n        if issubclass(cls, type):\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier is not None:\n                y = copier(memo)\n            else:\n                reductor = dispatch_table.get(cls)\n                if reductor:\n                    rv = reductor(x)\n                else:\n                    reductor = getattr(x, \"__reduce_ex__\", None)\n                    if reductor is not None:\n                        rv = reductor(4)\n                    else:\n                        reductor = getattr(x, \"__reduce__\", None)\n                        if reductor:\n                            rv = reductor()\n                        else:\n                            raise Error(\n                                \"un(deep)copyable object of type %s\" % cls)\n                if isinstance(rv, str):\n                    y = x\n                else:\n                    y = _reconstruct(x, memo, *rv)\n\n    # If is its own copy, don't memoize.\n    if y is not x:\n        memo[d] = y\n        _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\n\n_deepcopy_dispatch = d = {}\n\ndef _deepcopy_atomic(x, memo):\n    return x\nd[types.NoneType] = _deepcopy_atomic\nd[types.EllipsisType] = _deepcopy_atomic\nd[types.NotImplementedType] = _deepcopy_atomic\nd[int] = _deepcopy_atomic\nd[float] = _deepcopy_atomic\nd[bool] = _deepcopy_atomic\nd[complex] = _deepcopy_atomic\nd[bytes] = _deepcopy_atomic\nd[str] = _deepcopy_atomic\nd[types.CodeType] = _deepcopy_atomic\nd[type] = _deepcopy_atomic\nd[range] = _deepcopy_atomic\nd[types.BuiltinFunctionType] = _deepcopy_atomic\nd[types.FunctionType] = _deepcopy_atomic\nd[weakref.ref] = _deepcopy_atomic\nd[property] = _deepcopy_atomic\n\ndef _deepcopy_list(x, memo, deepcopy=deepcopy):\n    y = []\n    memo[id(x)] = y\n    append = y.append\n    for a in x:\n        append(deepcopy(a, memo))\n    return y\nd[list] = _deepcopy_list\n\ndef _deepcopy_tuple(x, memo, deepcopy=deepcopy):\n    y = [deepcopy(a, memo) for a in x]\n    # We're not going to put the tuple in the memo, but it's still important we\n    # check for it, in case the tuple contains recursive mutable structures.\n    try:\n        return memo[id(x)]\n    except KeyError:\n        pass\n    for k, j in zip(x, y):\n        if k is not j:\n            y = tuple(y)\n            break\n    else:\n        y = x\n    return y\nd[tuple] = _deepcopy_tuple\n\ndef _deepcopy_dict(x, memo, deepcopy=deepcopy):\n    y = {}\n    memo[id(x)] = y\n    for key, value in x.items():\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n    return y\nd[dict] = _deepcopy_dict\n\ndef _deepcopy_method(x, memo): # Copy instance methods\n    return type(x)(x.__func__, deepcopy(x.__self__, memo))\nd[types.MethodType] = _deepcopy_method\n\ndel d\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\ndef _reconstruct(x, memo, func, args,\n                 state=None, listiter=None, dictiter=None,\n                 *, deepcopy=deepcopy):\n    deep = memo is not None\n    if deep and args:\n        args = (deepcopy(arg, memo) for arg in args)\n    y = func(*args)\n    if deep:\n        memo[id(x)] = y\n\n    if state is not None:\n        if deep:\n            state = deepcopy(state, memo)\n        if hasattr(y, '__setstate__'):\n            y.__setstate__(state)\n        else:\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            else:\n                slotstate = None\n            if state is not None:\n                y.__dict__.update(state)\n            if slotstate is not None:\n                for key, value in slotstate.items():\n                    setattr(y, key, value)\n\n    if listiter is not None:\n        if deep:\n            for item in listiter:\n                item = deepcopy(item, memo)\n                y.append(item)\n        else:\n            for item in listiter:\n                y.append(item)\n    if dictiter is not None:\n        if deep:\n            for key, value in dictiter:\n                key = deepcopy(key, memo)\n                value = deepcopy(value, memo)\n                y[key] = value\n        else:\n            for key, value in dictiter:\n                y[key] = value\n    return y\n\ndel types, weakref\n\n\ndef replace(obj, /, **changes):\n    \"\"\"Return a new object replacing specified fields with new values.\n\n    This is especially useful for immutable objects, like named tuples or\n    frozen dataclasses.\n    \"\"\"\n    cls = obj.__class__\n    func = getattr(cls, '__replace__', None)\n    if func is None:\n        raise TypeError(f\"replace() does not support {cls.__name__} objects\")\n    return func(obj, **changes)\n", 306], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/typing.py": ["\"\"\"\nThe typing module: Support for gradual typing as defined by PEP 484 and subsequent PEPs.\n\nAmong other things, the module includes the following:\n* Generic, Protocol, and internal machinery to support generic aliases.\n  All subscripted types like X[int], Union[int, str] are generic aliases.\n* Various \"special forms\" that have unique meanings in type annotations:\n  NoReturn, Never, ClassVar, Self, Concatenate, Unpack, and others.\n* Classes whose instances can be type arguments to generic classes and functions:\n  TypeVar, ParamSpec, TypeVarTuple.\n* Public helper functions: get_type_hints, overload, cast, final, and others.\n* Several protocols to support duck-typing:\n  SupportsFloat, SupportsIndex, SupportsAbs, and others.\n* Special types: NewType, NamedTuple, TypedDict.\n* Deprecated aliases for builtin types and collections.abc ABCs.\n\nAny name not present in __all__ is an implementation detail\nthat may be changed without notice. Use at your own risk!\n\"\"\"\n\nfrom abc import abstractmethod, ABCMeta\nimport collections\nfrom collections import defaultdict\nimport collections.abc\nimport copyreg\nimport functools\nimport operator\nimport sys\nimport types\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType, GenericAlias\n\nfrom _typing import (\n    _idfunc,\n    TypeVar,\n    ParamSpec,\n    TypeVarTuple,\n    ParamSpecArgs,\n    ParamSpecKwargs,\n    TypeAliasType,\n    Generic,\n    NoDefault,\n)\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Annotated',\n    'Any',\n    'Callable',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'ForwardRef',\n    'Generic',\n    'Literal',\n    'Optional',\n    'ParamSpec',\n    'Protocol',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'ByteString',\n    'Container',\n    'ContextManager',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'Collection',\n    'AsyncGenerator',\n    'AsyncContextManager',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # Concrete collection types.\n    'ChainMap',\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'OrderedDict',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'TypedDict',  # Not really a type.\n    'Generator',\n\n    # Other concrete types.\n    'BinaryIO',\n    'IO',\n    'Match',\n    'Pattern',\n    'TextIO',\n\n    # One-off things.\n    'AnyStr',\n    'assert_type',\n    'assert_never',\n    'cast',\n    'clear_overloads',\n    'dataclass_transform',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_overloads',\n    'get_protocol_members',\n    'get_type_hints',\n    'is_protocol',\n    'is_typeddict',\n    'LiteralString',\n    'Never',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'NoDefault',\n    'NoReturn',\n    'NotRequired',\n    'overload',\n    'override',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'ReadOnly',\n    'Required',\n    'reveal_type',\n    'runtime_checkable',\n    'Self',\n    'Text',\n    'TYPE_CHECKING',\n    'TypeAlias',\n    'TypeGuard',\n    'TypeIs',\n    'TypeAliasType',\n    'Unpack',\n]\n\n\ndef _type_convert(arg, module=None, *, allow_special_forms=False):\n    \"\"\"For converting None to type(None), and strings to ForwardRef.\"\"\"\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\n    return arg\n\n\ndef _type_check(arg, msg, is_argument=True, module=None, *, allow_special_forms=False):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g.::\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    invalid_generic_forms = (Generic, Protocol)\n    if not allow_special_forms:\n        invalid_generic_forms += (ClassVar,)\n        if is_argument:\n            invalid_generic_forms += (Final,)\n\n    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    if arg in (Any, LiteralString, NoReturn, Never, Self, TypeAlias):\n        return arg\n    if allow_special_forms and arg in (ClassVar, Final):\n        return arg\n    if isinstance(arg, _SpecialForm) or arg in (Generic, Protocol):\n        raise TypeError(f\"Plain {arg} is not valid as type argument\")\n    if type(arg) is tuple:\n        raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n    return arg\n\n\ndef _is_param_expr(arg):\n    return arg is ... or isinstance(arg,\n            (tuple, list, ParamSpec, _ConcatenateGenericAlias))\n\n\ndef _should_unflatten_callable_args(typ, args):\n    \"\"\"Internal helper for munging collections.abc.Callable's __args__.\n\n    The canonical representation for a Callable's __args__ flattens the\n    argument types, see https://github.com/python/cpython/issues/86361.\n\n    For example::\n\n        >>> import collections.abc\n        >>> P = ParamSpec('P')\n        >>> collections.abc.Callable[[int, int], str].__args__ == (int, int, str)\n        True\n        >>> collections.abc.Callable[P, str].__args__ == (P, str)\n        True\n\n    As a result, if we need to reconstruct the Callable from its __args__,\n    we need to unflatten it.\n    \"\"\"\n    return (\n        typ.__origin__ is collections.abc.Callable\n        and not (len(args) == 2 and _is_param_expr(args[0]))\n    )\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    # When changing this function, don't forget about\n    # `_collections_abc._type_repr`, which does the same thing\n    # and must be consistent with this one.\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is ...:\n        return '...'\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    if isinstance(obj, tuple):\n        # Special case for `repr` of types with `ParamSpec`:\n        return '[' + ', '.join(_type_repr(t) for t in obj) + ']'\n    return repr(obj)\n\n\ndef _collect_type_parameters(args, *, enforce_default_ordering: bool = True):\n    \"\"\"Collect all type parameters in args\n    in order of first appearance (lexicographic order).\n\n    For example::\n\n        >>> P = ParamSpec('P')\n        >>> T = TypeVar('T')\n        >>> _collect_type_parameters((T, Callable[P, T]))\n        (~T, ~P)\n    \"\"\"\n    # required type parameter cannot appear after parameter with default\n    default_encountered = False\n    # or after TypeVarTuple\n    type_var_tuple_encountered = False\n    parameters = []\n    for t in args:\n        if isinstance(t, type):\n            # We don't want __parameters__ descriptor of a bare Python class.\n            pass\n        elif isinstance(t, tuple):\n            # `t` might be a tuple, when `ParamSpec` is substituted with\n            # `[T, int]`, or `[int, *Ts]`, etc.\n            for x in t:\n                for collected in _collect_type_parameters([x]):\n                    if collected not in parameters:\n                        parameters.append(collected)\n        elif hasattr(t, '__typing_subst__'):\n            if t not in parameters:\n                if enforce_default_ordering:\n                    if type_var_tuple_encountered and t.has_default():\n                        raise TypeError('Type parameter with a default'\n                                        ' follows TypeVarTuple')\n\n                    if t.has_default():\n                        default_encountered = True\n                    elif default_encountered:\n                        raise TypeError(f'Type parameter {t!r} without a default'\n                                        ' follows type parameter with a default')\n\n                parameters.append(t)\n        else:\n            if _is_unpacked_typevartuple(t):\n                type_var_tuple_encountered = True\n            for x in getattr(t, '__parameters__', ()):\n                if x not in parameters:\n                    parameters.append(x)\n    return tuple(parameters)\n\n\ndef _check_generic_specialization(cls, arguments):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    expected_len = len(cls.__parameters__)\n    if not expected_len:\n        raise TypeError(f\"{cls} is not a generic class\")\n    actual_len = len(arguments)\n    if actual_len != expected_len:\n        # deal with defaults\n        if actual_len < expected_len:\n            # If the parameter at index `actual_len` in the parameters list\n            # has a default, then all parameters after it must also have\n            # one, because we validated as much in _collect_type_parameters().\n            # That means that no error needs to be raised here, despite\n            # the number of arguments being passed not matching the number\n            # of parameters: all parameters that aren't explicitly\n            # specialized in this call are parameters with default values.\n            if cls.__parameters__[actual_len].has_default():\n                return\n\n            expected_len -= sum(p.has_default() for p in cls.__parameters__)\n            expect_val = f\"at least {expected_len}\"\n        else:\n            expect_val = expected_len\n\n        raise TypeError(f\"Too {'many' if actual_len > expected_len else 'few'} arguments\"\n                        f\" for {cls}; actual {actual_len}, expected {expect_val}\")\n\n\ndef _unpack_args(*args):\n    newargs = []\n    for arg in args:\n        subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n        if subargs is not None and not (subargs and subargs[-1] is ...):\n            newargs.extend(subargs)\n        else:\n            newargs.append(arg)\n    return newargs\n\ndef _deduplicate(params, *, unhashable_fallback=False):\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    try:\n        return dict.fromkeys(params)\n    except TypeError:\n        if not unhashable_fallback:\n            raise\n        # Happens for cases like `Annotated[dict, {'x': IntValidator()}]`\n        return _deduplicate_unhashable(params)\n\ndef _deduplicate_unhashable(unhashable_params):\n    new_unhashable = []\n    for t in unhashable_params:\n        if t not in new_unhashable:\n            new_unhashable.append(t)\n    return new_unhashable\n\ndef _compare_args_orderless(first_args, second_args):\n    first_unhashable = _deduplicate_unhashable(first_args)\n    second_unhashable = _deduplicate_unhashable(second_args)\n    t = list(second_unhashable)\n    try:\n        for elem in first_unhashable:\n            t.remove(elem)\n    except ValueError:\n        return False\n    return not t\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"Internal helper for Union creation and substitution.\n\n    Flatten Unions among parameters, then remove duplicates.\n    \"\"\"\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, (_UnionGenericAlias, types.UnionType)):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n\n    return tuple(_deduplicate(params, unhashable_fallback=True))\n\n\ndef _flatten_literal_params(parameters):\n    \"\"\"Internal helper for Literal creation: flatten Literals among parameters.\"\"\"\n    params = []\n    for p in parameters:\n        if isinstance(p, _LiteralGenericAlias):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n    return tuple(params)\n\n\n_cleanups = []\n_caches = {}\n\n\ndef _tp_cache(func=None, /, *, typed=False):\n    \"\"\"Internal wrapper caching __getitem__ of generic types.\n\n    For non-hashable arguments, the original function is used as a fallback.\n    \"\"\"\n    def decorator(func):\n        # The callback 'inner' references the newly created lru_cache\n        # indirectly by performing a lookup in the global '_caches' dictionary.\n        # This breaks a reference that can be problematic when combined with\n        # C API extensions that leak references to types. See GH-98253.\n\n        cache = functools.lru_cache(typed=typed)(func)\n        _caches[func] = cache\n        _cleanups.append(cache.cache_clear)\n        del cache\n\n        @functools.wraps(func)\n        def inner(*args, **kwds):\n            try:\n                return _caches[func](*args, **kwds)\n            except TypeError:\n                pass  # All real errors (not unhashable args) are raised below.\n            return func(*args, **kwds)\n        return inner\n\n    if func is not None:\n        return decorator(func)\n\n    return decorator\n\n\ndef _deprecation_warning_for_no_type_params_passed(funcname: str) -> None:\n    import warnings\n\n    depr_message = (\n        f\"Failing to pass a value to the 'type_params' parameter \"\n        f\"of {funcname!r} is deprecated, as it leads to incorrect behaviour \"\n        f\"when calling {funcname} on a stringified annotation \"\n        f\"that references a PEP 695 type parameter. \"\n        f\"It will be disallowed in Python 3.15.\"\n    )\n    warnings.warn(depr_message, category=DeprecationWarning, stacklevel=3)\n\n\nclass _Sentinel:\n    __slots__ = ()\n    def __repr__(self):\n        return '<sentinel>'\n\n\n_sentinel = _Sentinel()\n\n\ndef _eval_type(t, globalns, localns, type_params=_sentinel, *, recursive_guard=frozenset()):\n    \"\"\"Evaluate all forward references in the given type t.\n\n    For use of globalns and localns see the docstring for get_type_hints().\n    recursive_guard is used to prevent infinite recursion with a recursive\n    ForwardRef.\n    \"\"\"\n    if type_params is _sentinel:\n        _deprecation_warning_for_no_type_params_passed(\"typing._eval_type\")\n        type_params = ()\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns, type_params, recursive_guard=recursive_guard)\n    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\n        if isinstance(t, GenericAlias):\n            args = tuple(\n                ForwardRef(arg) if isinstance(arg, str) else arg\n                for arg in t.__args__\n            )\n            is_unpacked = t.__unpacked__\n            if _should_unflatten_callable_args(t, args):\n                t = t.__origin__[(args[:-1], args[-1])]\n            else:\n                t = t.__origin__[args]\n            if is_unpacked:\n                t = Unpack[t]\n\n        ev_args = tuple(\n            _eval_type(\n                a, globalns, localns, type_params, recursive_guard=recursive_guard\n            )\n            for a in t.__args__\n        )\n        if ev_args == t.__args__:\n            return t\n        if isinstance(t, GenericAlias):\n            return GenericAlias(t.__origin__, ev_args)\n        if isinstance(t, types.UnionType):\n            return functools.reduce(operator.or_, ev_args)\n        else:\n            return t.copy_with(ev_args)\n    return t\n\n\nclass _Final:\n    \"\"\"Mixin to prohibit subclassing.\"\"\"\n\n    __slots__ = ('__weakref__',)\n\n    def __init_subclass__(cls, /, *args, **kwds):\n        if '_root' not in kwds:\n            raise TypeError(\"Cannot subclass special typing classes\")\n\n\nclass _NotIterable:\n    \"\"\"Mixin to prevent iteration, without being compatible with Iterable.\n\n    That is, we could do::\n\n        def __iter__(self): raise TypeError()\n\n    But this would make users of this mixin duck type-compatible with\n    collections.abc.Iterable - isinstance(foo, Iterable) would be True.\n\n    Luckily, we can instead prevent iteration by setting __iter__ to None, which\n    is treated specially.\n    \"\"\"\n\n    __slots__ = ()\n    __iter__ = None\n\n\n# Internal indicator of special typing constructs.\n# See __doc__ instance attribute for specific docs.\nclass _SpecialForm(_Final, _NotIterable, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nclass _TypedCacheSpecialForm(_SpecialForm, _root=True):\n    def __getitem__(self, parameters):\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        return self._getitem(self, *parameters)\n\n\nclass _AnyMeta(type):\n    def __instancecheck__(self, obj):\n        if self is Any:\n            raise TypeError(\"typing.Any cannot be used with isinstance()\")\n        return super().__instancecheck__(obj)\n\n    def __repr__(self):\n        if self is Any:\n            return \"typing.Any\"\n        return super().__repr__()  # respect to subclasses\n\n\nclass Any(metaclass=_AnyMeta):\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    checks.\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        if cls is Any:\n            raise TypeError(\"Any cannot be instantiated\")\n        return super().__new__(cls)\n\n\n@_SpecialForm\ndef NoReturn(self, parameters):\n    \"\"\"Special type indicating functions that never return.\n\n    Example::\n\n        from typing import NoReturn\n\n        def stop() -> NoReturn:\n            raise Exception('no way')\n\n    NoReturn can also be used as a bottom type, a type that\n    has no values. Starting in Python 3.11, the Never type should\n    be used for this concept instead. Type checkers should treat the two\n    equivalently.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n# This is semantically identical to NoReturn, but it is implemented\n# separately so that type checkers can distinguish between the two\n# if they want.\n@_SpecialForm\ndef Never(self, parameters):\n    \"\"\"The bottom type, a type that has no members.\n\n    This can be used to define a function that should never be\n    called, or a function that never returns::\n\n        from typing import Never\n\n        def never_call_me(arg: Never) -> None:\n            pass\n\n        def int_or_str(arg: int | str) -> None:\n            never_call_me(arg)  # type checker error\n            match arg:\n                case int():\n                    print(\"It's an int\")\n                case str():\n                    print(\"It's a str\")\n                case _:\n                    never_call_me(arg)  # OK, arg is of type Never\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef Self(self, parameters):\n    \"\"\"Used to spell the type of \"self\" in classes.\n\n    Example::\n\n        from typing import Self\n\n        class Foo:\n            def return_self(self) -> Self:\n                ...\n                return self\n\n    This is especially useful for:\n        - classmethods that are used as alternative constructors\n        - annotating an `__enter__` method which returns self\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef LiteralString(self, parameters):\n    \"\"\"Represents an arbitrary literal string.\n\n    Example::\n\n        from typing import LiteralString\n\n        def run_query(sql: LiteralString) -> None:\n            ...\n\n        def caller(arbitrary_string: str, literal_string: LiteralString) -> None:\n            run_query(\"SELECT * FROM students\")  # OK\n            run_query(literal_string)  # OK\n            run_query(\"SELECT * FROM \" + literal_string)  # OK\n            run_query(arbitrary_string)  # type checker error\n            run_query(  # type checker error\n                f\"SELECT * FROM students WHERE name = {arbitrary_string}\"\n            )\n\n    Only string literals and other LiteralStrings are compatible\n    with LiteralString. This provides a tool to help prevent\n    security issues such as SQL injection.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef ClassVar(self, parameters):\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class.\n\n    Usage::\n\n        class Starship:\n            stats: ClassVar[dict[str, int]] = {} # class variable\n            damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.', allow_special_forms=True)\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Final(self, parameters):\n    \"\"\"Special typing construct to indicate final names to type checkers.\n\n    A final name cannot be re-assigned or overridden in a subclass.\n\n    For example::\n\n        MAX_SIZE: Final = 9000\n        MAX_SIZE += 1  # Error reported by type checker\n\n        class Connection:\n            TIMEOUT: Final[int] = 10\n\n        class FastConnector(Connection):\n            TIMEOUT = 1  # Error reported by type checker\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.', allow_special_forms=True)\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Union(self, parameters):\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    On Python 3.10 and higher, the | operator\n    can also be used to denote unions;\n    X | Y means the same thing to the type checker as Union[X, Y].\n\n    To define a union, use e.g. Union[int, str]. Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        assert Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        assert Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        assert Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        assert Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Union of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    msg = \"Union[arg, ...]: each arg must be a type.\"\n    parameters = tuple(_type_check(p, msg) for p in parameters)\n    parameters = _remove_dups_flatten(parameters)\n    if len(parameters) == 1:\n        return parameters[0]\n    if len(parameters) == 2 and type(None) in parameters:\n        return _UnionGenericAlias(self, parameters, name=\"Optional\")\n    return _UnionGenericAlias(self, parameters)\n\ndef _make_union(left, right):\n    \"\"\"Used from the C implementation of TypeVar.\n\n    TypeVar.__or__ calls this instead of returning types.UnionType\n    because we want to allow unions between TypeVars and strings\n    (forward references).\n    \"\"\"\n    return Union[left, right]\n\n@_SpecialForm\ndef Optional(self, parameters):\n    \"\"\"Optional[X] is equivalent to Union[X, None].\"\"\"\n    arg = _type_check(parameters, f\"{self} requires a single type.\")\n    return Union[arg, type(None)]\n\n@_TypedCacheSpecialForm\n@_tp_cache(typed=True)\ndef Literal(self, *parameters):\n    \"\"\"Special typing form to define literal types (a.k.a. value types).\n\n    This form can be used to indicate to type checkers that the corresponding\n    variable or function parameter has a value equivalent to the provided\n    literal (or one of several literals)::\n\n        def validate_simple(data: Any) -> Literal[True]:  # always returns True\n            ...\n\n        MODE = Literal['r', 'rb', 'w', 'wb']\n        def open_helper(file: str, mode: MODE) -> str:\n            ...\n\n        open_helper('/some/path', 'r')  # Passes type check\n        open_helper('/other/path', 'typo')  # Error in type checker\n\n    Literal[...] cannot be subclassed. At runtime, an arbitrary value\n    is allowed as type argument to Literal[...], but type checkers may\n    impose restrictions.\n    \"\"\"\n    # There is no '_type_check' call because arguments to Literal[...] are\n    # values, not types.\n    parameters = _flatten_literal_params(parameters)\n\n    try:\n        parameters = tuple(p for p, _ in _deduplicate(list(_value_and_type_iter(parameters))))\n    except TypeError:  # unhashable parameters\n        pass\n\n    return _LiteralGenericAlias(self, parameters)\n\n\n@_SpecialForm\ndef TypeAlias(self, parameters):\n    \"\"\"Special form for marking type aliases.\n\n    Use TypeAlias to indicate that an assignment should\n    be recognized as a proper type alias definition by type\n    checkers.\n\n    For example::\n\n        Predicate: TypeAlias = Callable[..., bool]\n\n    It's invalid when used anywhere except as in the example above.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef Concatenate(self, parameters):\n    \"\"\"Special form for annotating higher-order functions.\n\n    ``Concatenate`` can be used in conjunction with ``ParamSpec`` and\n    ``Callable`` to represent a higher-order function which adds, removes or\n    transforms the parameters of a callable.\n\n    For example::\n\n        Callable[Concatenate[int, P], int]\n\n    See PEP 612 for detailed information.\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not (parameters[-1] is ... or isinstance(parameters[-1], ParamSpec)):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable or ellipsis.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = (*(_type_check(p, msg) for p in parameters[:-1]), parameters[-1])\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n@_SpecialForm\ndef TypeGuard(self, parameters):\n    \"\"\"Special typing construct for marking user-defined type predicate functions.\n\n    ``TypeGuard`` can be used to annotate the return type of a user-defined\n    type predicate function.  ``TypeGuard`` only accepts a single type argument.\n    At runtime, functions marked this way should return a boolean.\n\n    ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n    type checkers to determine a more precise type of an expression within a\n    program's code flow.  Usually type narrowing is done by analyzing\n    conditional code flow and applying the narrowing to a block of code.  The\n    conditional expression here is sometimes referred to as a \"type predicate\".\n\n    Sometimes it would be convenient to use a user-defined boolean function\n    as a type predicate.  Such a function should use ``TypeGuard[...]`` or\n    ``TypeIs[...]`` as its return type to alert static type checkers to\n    this intention. ``TypeGuard`` should be used over ``TypeIs`` when narrowing\n    from an incompatible type (e.g., ``list[object]`` to ``list[int]``) or when\n    the function does not return ``True`` for all instances of the narrowed type.\n\n    Using  ``-> TypeGuard[NarrowedType]`` tells the static type checker that\n    for a given function:\n\n    1. The return value is a boolean.\n    2. If the return value is ``True``, the type of its argument\n       is ``NarrowedType``.\n\n    For example::\n\n         def is_str_list(val: list[object]) -> TypeGuard[list[str]]:\n             '''Determines whether all objects in the list are strings'''\n             return all(isinstance(x, str) for x in val)\n\n         def func1(val: list[object]):\n             if is_str_list(val):\n                 # Type of ``val`` is narrowed to ``list[str]``.\n                 print(\" \".join(val))\n             else:\n                 # Type of ``val`` remains as ``list[object]``.\n                 print(\"Not a list of strings!\")\n\n    Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n    form of ``TypeA`` (it can even be a wider form) and this may lead to\n    type-unsafe results.  The main reason is to allow for things like\n    narrowing ``list[object]`` to ``list[str]`` even though the latter is not\n    a subtype of the former, since ``list`` is invariant.  The responsibility of\n    writing type-safe type predicates is left to the user.\n\n    ``TypeGuard`` also works with type variables.  For more information, see\n    PEP 647 (User-Defined Type Guards).\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n\n@_SpecialForm\ndef TypeIs(self, parameters):\n    \"\"\"Special typing construct for marking user-defined type predicate functions.\n\n    ``TypeIs`` can be used to annotate the return type of a user-defined\n    type predicate function.  ``TypeIs`` only accepts a single type argument.\n    At runtime, functions marked this way should return a boolean and accept\n    at least one argument.\n\n    ``TypeIs`` aims to benefit *type narrowing* -- a technique used by static\n    type checkers to determine a more precise type of an expression within a\n    program's code flow.  Usually type narrowing is done by analyzing\n    conditional code flow and applying the narrowing to a block of code.  The\n    conditional expression here is sometimes referred to as a \"type predicate\".\n\n    Sometimes it would be convenient to use a user-defined boolean function\n    as a type predicate.  Such a function should use ``TypeIs[...]`` or\n    ``TypeGuard[...]`` as its return type to alert static type checkers to\n    this intention.  ``TypeIs`` usually has more intuitive behavior than\n    ``TypeGuard``, but it cannot be used when the input and output types\n    are incompatible (e.g., ``list[object]`` to ``list[int]``) or when the\n    function does not return ``True`` for all instances of the narrowed type.\n\n    Using  ``-> TypeIs[NarrowedType]`` tells the static type checker that for\n    a given function:\n\n    1. The return value is a boolean.\n    2. If the return value is ``True``, the type of its argument\n       is the intersection of the argument's original type and\n       ``NarrowedType``.\n    3. If the return value is ``False``, the type of its argument\n       is narrowed to exclude ``NarrowedType``.\n\n    For example::\n\n        from typing import assert_type, final, TypeIs\n\n        class Parent: pass\n        class Child(Parent): pass\n        @final\n        class Unrelated: pass\n\n        def is_parent(val: object) -> TypeIs[Parent]:\n            return isinstance(val, Parent)\n\n        def run(arg: Child | Unrelated):\n            if is_parent(arg):\n                # Type of ``arg`` is narrowed to the intersection\n                # of ``Parent`` and ``Child``, which is equivalent to\n                # ``Child``.\n                assert_type(arg, Child)\n            else:\n                # Type of ``arg`` is narrowed to exclude ``Parent``,\n                # so only ``Unrelated`` is left.\n                assert_type(arg, Unrelated)\n\n    The type inside ``TypeIs`` must be consistent with the type of the\n    function's argument; if it is not, static type checkers will raise\n    an error.  An incorrectly written ``TypeIs`` function can lead to\n    unsound behavior in the type system; it is the user's responsibility\n    to write such functions in a type-safe manner.\n\n    ``TypeIs`` also works with type variables.  For more information, see\n    PEP 742 (Narrowing types with ``TypeIs``).\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n\nclass ForwardRef(_Final, _root=True):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__',\n                 '__forward_is_argument__', '__forward_is_class__',\n                 '__forward_module__')\n\n    def __init__(self, arg, is_argument=True, module=None, *, is_class=False):\n        if not isinstance(arg, str):\n            raise TypeError(f\"Forward reference must be a string -- got {arg!r}\")\n\n        # If we do `def f(*args: *Ts)`, then we'll have `arg = '*Ts'`.\n        # Unfortunately, this isn't a valid expression on its own, so we\n        # do the unpacking manually.\n        if arg.startswith('*'):\n            arg_to_compile = f'({arg},)[0]'  # E.g. (*Ts,)[0] or (*tuple[int, int],)[0]\n        else:\n            arg_to_compile = arg\n        try:\n            code = compile(arg_to_compile, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError(f\"Forward reference must be an expression -- got {arg!r}\")\n\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n        self.__forward_is_class__ = is_class\n        self.__forward_module__ = module\n\n    def _evaluate(self, globalns, localns, type_params=_sentinel, *, recursive_guard):\n        if type_params is _sentinel:\n            _deprecation_warning_for_no_type_params_passed(\"typing.ForwardRef._evaluate\")\n            type_params = ()\n        if self.__forward_arg__ in recursive_guard:\n            return self\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            if self.__forward_module__ is not None:\n                globalns = getattr(\n                    sys.modules.get(self.__forward_module__, None), '__dict__', globalns\n                )\n\n            # type parameters require some special handling,\n            # as they exist in their own scope\n            # but `eval()` does not have a dedicated parameter for that scope.\n            # For classes, names in type parameter scopes should override\n            # names in the global scope (which here are called `localns`!),\n            # but should in turn be overridden by names in the class scope\n            # (which here are called `globalns`!)\n            if type_params:\n                globalns, localns = dict(globalns), dict(localns)\n                for param in type_params:\n                    param_name = param.__name__\n                    if not self.__forward_is_class__ or param_name not in globalns:\n                        globalns[param_name] = param\n                        localns.pop(param_name, None)\n\n            type_ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\",\n                is_argument=self.__forward_is_argument__,\n                allow_special_forms=self.__forward_is_class__,\n            )\n            self.__forward_value__ = _eval_type(\n                type_,\n                globalns,\n                localns,\n                type_params,\n                recursive_guard=(recursive_guard | {self.__forward_arg__}),\n            )\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return (self.__forward_arg__ == other.__forward_arg__ and\n                self.__forward_module__ == other.__forward_module__)\n\n    def __hash__(self):\n        return hash((self.__forward_arg__, self.__forward_module__))\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n    def __repr__(self):\n        if self.__forward_module__ is None:\n            module_repr = ''\n        else:\n            module_repr = f', module={self.__forward_module__!r}'\n        return f'ForwardRef({self.__forward_arg__!r}{module_repr})'\n\n\ndef _is_unpacked_typevartuple(x: Any) -> bool:\n    return ((not isinstance(x, type)) and\n            getattr(x, '__typing_is_unpacked_typevartuple__', False))\n\n\ndef _is_typevar_like(x: Any) -> bool:\n    return isinstance(x, (TypeVar, ParamSpec)) or _is_unpacked_typevartuple(x)\n\n\ndef _typevar_subst(self, arg):\n    msg = \"Parameters to generic types must be types.\"\n    arg = _type_check(arg, msg, is_argument=True)\n    if ((isinstance(arg, _GenericAlias) and arg.__origin__ is Unpack) or\n        (isinstance(arg, GenericAlias) and getattr(arg, '__unpacked__', False))):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    return arg\n\n\ndef _typevartuple_prepare_subst(self, alias, args):\n    params = alias.__parameters__\n    typevartuple_index = params.index(self)\n    for param in params[typevartuple_index + 1:]:\n        if isinstance(param, TypeVarTuple):\n            raise TypeError(f\"More than one TypeVarTuple parameter in {alias}\")\n\n    alen = len(args)\n    plen = len(params)\n    left = typevartuple_index\n    right = plen - typevartuple_index - 1\n    var_tuple_index = None\n    fillarg = None\n    for k, arg in enumerate(args):\n        if not isinstance(arg, type):\n            subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n            if subargs and len(subargs) == 2 and subargs[-1] is ...:\n                if var_tuple_index is not None:\n                    raise TypeError(\"More than one unpacked arbitrary-length tuple argument\")\n                var_tuple_index = k\n                fillarg = subargs[0]\n    if var_tuple_index is not None:\n        left = min(left, var_tuple_index)\n        right = min(right, alen - var_tuple_index - 1)\n    elif left + right > alen:\n        raise TypeError(f\"Too few arguments for {alias};\"\n                        f\" actual {alen}, expected at least {plen-1}\")\n    if left == alen - right and self.has_default():\n        replacement = _unpack_args(self.__default__)\n    else:\n        replacement = args[left: alen - right]\n\n    return (\n        *args[:left],\n        *([fillarg]*(typevartuple_index - left)),\n        replacement,\n        *([fillarg]*(plen - right - left - typevartuple_index - 1)),\n        *args[alen - right:],\n    )\n\n\ndef _paramspec_subst(self, arg):\n    if isinstance(arg, (list, tuple)):\n        arg = tuple(_type_check(a, \"Expected a type.\") for a in arg)\n    elif not _is_param_expr(arg):\n        raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                        f\"ParamSpec, or Concatenate. Got {arg}\")\n    return arg\n\n\ndef _paramspec_prepare_subst(self, alias, args):\n    params = alias.__parameters__\n    i = params.index(self)\n    if i == len(args) and self.has_default():\n        args = [*args, self.__default__]\n    if i >= len(args):\n        raise TypeError(f\"Too few arguments for {alias}\")\n    # Special case where Z[[int, str, bool]] == Z[int, str, bool] in PEP 612.\n    if len(params) == 1 and not _is_param_expr(args[0]):\n        assert i == 0\n        args = (args,)\n    # Convert lists to tuples to help other libraries cache the results.\n    elif isinstance(args[i], list):\n        args = (*args[:i], tuple(args[i]), *args[i+1:])\n    return args\n\n\n@_tp_cache\ndef _generic_class_getitem(cls, args):\n    \"\"\"Parameterizes a generic class.\n\n    At least, parameterizing a generic class is the *main* thing this method\n    does. For example, for some generic class `Foo`, this is called when we\n    do `Foo[int]` - there, with `cls=Foo` and `args=int`.\n\n    However, note that this method is also called when defining generic\n    classes in the first place with `class Foo(Generic[T]): ...`.\n    \"\"\"\n    if not isinstance(args, tuple):\n        args = (args,)\n\n    args = tuple(_type_convert(p) for p in args)\n    is_generic_or_protocol = cls in (Generic, Protocol)\n\n    if is_generic_or_protocol:\n        # Generic and Protocol can only be subscripted with unique type variables.\n        if not args:\n            raise TypeError(\n                f\"Parameter list to {cls.__qualname__}[...] cannot be empty\"\n            )\n        if not all(_is_typevar_like(p) for p in args):\n            raise TypeError(\n                f\"Parameters to {cls.__name__}[...] must all be type variables \"\n                f\"or parameter specification variables.\")\n        if len(set(args)) != len(args):\n            raise TypeError(\n                f\"Parameters to {cls.__name__}[...] must all be unique\")\n    else:\n        # Subscripting a regular Generic subclass.\n        for param in cls.__parameters__:\n            prepare = getattr(param, '__typing_prepare_subst__', None)\n            if prepare is not None:\n                args = prepare(cls, args)\n        _check_generic_specialization(cls, args)\n\n        new_args = []\n        for param, new_arg in zip(cls.__parameters__, args):\n            if isinstance(param, TypeVarTuple):\n                new_args.extend(new_arg)\n            else:\n                new_args.append(new_arg)\n        args = tuple(new_args)\n\n    return _GenericAlias(cls, args)\n\n\ndef _generic_init_subclass(cls, *args, **kwargs):\n    super(Generic, cls).__init_subclass__(*args, **kwargs)\n    tvars = []\n    if '__orig_bases__' in cls.__dict__:\n        error = Generic in cls.__orig_bases__\n    else:\n        error = (Generic in cls.__bases__ and\n                    cls.__name__ != 'Protocol' and\n                    type(cls) != _TypedDictMeta)\n    if error:\n        raise TypeError(\"Cannot inherit from plain Generic\")\n    if '__orig_bases__' in cls.__dict__:\n        tvars = _collect_type_parameters(cls.__orig_bases__)\n        # Look for Generic[T1, ..., Tn].\n        # If found, tvars must be a subset of it.\n        # If not found, tvars is it.\n        # Also check for and reject plain Generic,\n        # and reject multiple Generic[...].\n        gvars = None\n        for base in cls.__orig_bases__:\n            if (isinstance(base, _GenericAlias) and\n                    base.__origin__ is Generic):\n                if gvars is not None:\n                    raise TypeError(\n                        \"Cannot inherit from Generic[...] multiple times.\")\n                gvars = base.__parameters__\n        if gvars is not None:\n            tvarset = set(tvars)\n            gvarset = set(gvars)\n            if not tvarset <= gvarset:\n                s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                s_args = ', '.join(str(g) for g in gvars)\n                raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                f\" not listed in Generic[{s_args}]\")\n            tvars = gvars\n    cls.__parameters__ = tuple(tvars)\n\n\ndef _is_dunder(attr):\n    return attr.startswith('__') and attr.endswith('__')\n\nclass _BaseGenericAlias(_Final, _root=True):\n    \"\"\"The central part of the internal API.\n\n    This represents a generic version of type 'origin' with type arguments 'params'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have 'name' always set. If 'inst' is False, then the alias can't be instantiated;\n    this is used by e.g. typing.List and typing.Dict.\n    \"\"\"\n\n    def __init__(self, origin, *, inst=True, name=None):\n        self._inst = inst\n        self._name = name\n        self.__origin__ = origin\n        self.__slots__ = None  # This is not documented.\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n                            f\"use {self.__origin__.__name__}() instead\")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        # Some objects raise TypeError (or something even more exotic)\n        # if you try to set attributes on them; we guard against that here\n        except Exception:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        res = []\n        if self.__origin__ not in bases:\n            res.append(self.__origin__)\n\n        # Check if any base that occurs after us in `bases` is either itself a\n        # subclass of Generic, or something which will add a subclass of Generic\n        # to `__bases__` via its `__mro_entries__`. If not, add Generic\n        # ourselves. The goal is to ensure that Generic (or a subclass) will\n        # appear exactly once in the final bases tuple. If we let it appear\n        # multiple times, we risk \"can't form a consistent MRO\" errors.\n        i = bases.index(self)\n        for b in bases[i+1:]:\n            if isinstance(b, _BaseGenericAlias):\n                break\n            if not isinstance(b, type):\n                meth = getattr(b, \"__mro_entries__\", None)\n                new_bases = meth(bases) if meth else None\n                if (\n                    isinstance(new_bases, tuple) and\n                    any(\n                        isinstance(b2, type) and issubclass(b2, Generic)\n                        for b2 in new_bases\n                    )\n                ):\n                    break\n            elif issubclass(b, Generic):\n                break\n        else:\n            res.append(Generic)\n        return tuple(res)\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return self._name or self.__origin__.__name__\n\n        # We are careful for copy and pickle.\n        # Also for simplicity we don't relay any dunder names\n        if '__origin__' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in {'_name', '_inst', '_nparams', '_defaults'}:\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Subscripted generics cannot be used with\"\n                        \" class and instance checks\")\n\n    def __dir__(self):\n        return list(set(super().__dir__()\n                + [attr for attr in dir(self.__origin__) if not _is_dunder(attr)]))\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\nclass _GenericAlias(_BaseGenericAlias, _root=True):\n    # The type of parameterized generics.\n    #\n    # That is, for example, `type(List[int])` is `_GenericAlias`.\n    #\n    # Objects which are instances of this class include:\n    # * Parameterized container types, e.g. `Tuple[int]`, `List[int]`.\n    #  * Note that native container types, e.g. `tuple`, `list`, use\n    #    `types.GenericAlias` instead.\n    # * Parameterized classes:\n    #     class C[T]: pass\n    #     # C[int] is a _GenericAlias\n    # * `Callable` aliases, generic `Callable` aliases, and\n    #   parameterized `Callable` aliases:\n    #     T = TypeVar('T')\n    #     # _CallableGenericAlias inherits from _GenericAlias.\n    #     A = Callable[[], None]  # _CallableGenericAlias\n    #     B = Callable[[T], None]  # _CallableGenericAlias\n    #     C = B[int]  # _CallableGenericAlias\n    # * Parameterized `Final`, `ClassVar`, `TypeGuard`, and `TypeIs`:\n    #     # All _GenericAlias\n    #     Final[int]\n    #     ClassVar[float]\n    #     TypeGuard[bool]\n    #     TypeIs[range]\n\n    def __init__(self, origin, args, *, inst=True, name=None):\n        super().__init__(origin, inst=inst, name=name)\n        if not isinstance(args, tuple):\n            args = (args,)\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              a for a in args)\n        enforce_default_ordering = origin in (Generic, Protocol)\n        self.__parameters__ = _collect_type_parameters(\n            args,\n            enforce_default_ordering=enforce_default_ordering,\n        )\n        if not name:\n            self.__module__ = origin.__module__\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__args__ == other.__args__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__args__))\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n    @_tp_cache\n    def __getitem__(self, args):\n        # Parameterizes an already-parameterized object.\n        #\n        # For example, we arrive here doing something like:\n        #   T1 = TypeVar('T1')\n        #   T2 = TypeVar('T2')\n        #   T3 = TypeVar('T3')\n        #   class A(Generic[T1]): pass\n        #   B = A[T2]  # B is a _GenericAlias\n        #   C = B[T3]  # Invokes _GenericAlias.__getitem__\n        #\n        # We also arrive here when parameterizing a generic `Callable` alias:\n        #   T = TypeVar('T')\n        #   C = Callable[[T], None]\n        #   C[int]  # Invokes _GenericAlias.__getitem__\n\n        if self.__origin__ in (Generic, Protocol):\n            # Can't subscript Generic[...] or Protocol[...].\n            raise TypeError(f\"Cannot subscript already-subscripted {self}\")\n        if not self.__parameters__:\n            raise TypeError(f\"{self} is not a generic class\")\n\n        # Preprocess `args`.\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = _unpack_args(*(_type_convert(p) for p in args))\n        new_args = self._determine_new_args(args)\n        r = self.copy_with(new_args)\n        return r\n\n    def _determine_new_args(self, args):\n        # Determines new __args__ for __getitem__.\n        #\n        # For example, suppose we had:\n        #   T1 = TypeVar('T1')\n        #   T2 = TypeVar('T2')\n        #   class A(Generic[T1, T2]): pass\n        #   T3 = TypeVar('T3')\n        #   B = A[int, T3]\n        #   C = B[str]\n        # `B.__args__` is `(int, T3)`, so `C.__args__` should be `(int, str)`.\n        # Unfortunately, this is harder than it looks, because if `T3` is\n        # anything more exotic than a plain `TypeVar`, we need to consider\n        # edge cases.\n\n        params = self.__parameters__\n        # In the example above, this would be {T3: str}\n        for param in params:\n            prepare = getattr(param, '__typing_prepare_subst__', None)\n            if prepare is not None:\n                args = prepare(self, args)\n        alen = len(args)\n        plen = len(params)\n        if alen != plen:\n            raise TypeError(f\"Too {'many' if alen > plen else 'few'} arguments for {self};\"\n                            f\" actual {alen}, expected {plen}\")\n        new_arg_by_param = dict(zip(params, args))\n        return tuple(self._make_substitution(self.__args__, new_arg_by_param))\n\n    def _make_substitution(self, args, new_arg_by_param):\n        \"\"\"Create a list of new type arguments.\"\"\"\n        new_args = []\n        for old_arg in args:\n            if isinstance(old_arg, type):\n                new_args.append(old_arg)\n                continue\n\n            substfunc = getattr(old_arg, '__typing_subst__', None)\n            if substfunc:\n                new_arg = substfunc(new_arg_by_param[old_arg])\n            else:\n                subparams = getattr(old_arg, '__parameters__', ())\n                if not subparams:\n                    new_arg = old_arg\n                else:\n                    subargs = []\n                    for x in subparams:\n                        if isinstance(x, TypeVarTuple):\n                            subargs.extend(new_arg_by_param[x])\n                        else:\n                            subargs.append(new_arg_by_param[x])\n                    new_arg = old_arg[tuple(subargs)]\n\n            if self.__origin__ == collections.abc.Callable and isinstance(new_arg, tuple):\n                # Consider the following `Callable`.\n                #   C = Callable[[int], str]\n                # Here, `C.__args__` should be (int, str) - NOT ([int], str).\n                # That means that if we had something like...\n                #   P = ParamSpec('P')\n                #   T = TypeVar('T')\n                #   C = Callable[P, T]\n                #   D = C[[int, str], float]\n                # ...we need to be careful; `new_args` should end up as\n                # `(int, str, float)` rather than `([int, str], float)`.\n                new_args.extend(new_arg)\n            elif _is_unpacked_typevartuple(old_arg):\n                # Consider the following `_GenericAlias`, `B`:\n                #   class A(Generic[*Ts]): ...\n                #   B = A[T, *Ts]\n                # If we then do:\n                #   B[float, int, str]\n                # The `new_arg` corresponding to `T` will be `float`, and the\n                # `new_arg` corresponding to `*Ts` will be `(int, str)`. We\n                # should join all these types together in a flat list\n                # `(float, int, str)` - so again, we should `extend`.\n                new_args.extend(new_arg)\n            elif isinstance(old_arg, tuple):\n                # Corner case:\n                #    P = ParamSpec('P')\n                #    T = TypeVar('T')\n                #    class Base(Generic[P]): ...\n                # Can be substituted like this:\n                #    X = Base[[int, T]]\n                # In this case, `old_arg` will be a tuple:\n                new_args.append(\n                    tuple(self._make_substitution(old_arg, new_arg_by_param)),\n                )\n            else:\n                new_args.append(new_arg)\n        return new_args\n\n    def copy_with(self, args):\n        return self.__class__(self.__origin__, args, name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        if self._name:\n            name = 'typing.' + self._name\n        else:\n            name = _type_repr(self.__origin__)\n        if self.__args__:\n            args = \", \".join([_type_repr(a) for a in self.__args__])\n        else:\n            # To ensure the repr is eval-able.\n            args = \"()\"\n        return f'{name}[{args}]'\n\n    def __reduce__(self):\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        args = tuple(self.__args__)\n        if len(args) == 1 and not isinstance(args[0], tuple):\n            args, = args\n        return operator.getitem, (origin, args)\n\n    def __mro_entries__(self, bases):\n        if isinstance(self.__origin__, _SpecialForm):\n            raise TypeError(f\"Cannot subclass {self!r}\")\n\n        if self._name:  # generic version of an ABC or built-in class\n            return super().__mro_entries__(bases)\n        if self.__origin__ is Generic:\n            if Protocol in bases:\n                return ()\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _BaseGenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n    def __iter__(self):\n        yield Unpack[self]\n\n\n# _nparams is the number of accepted parameters, e.g. 0 for Hashable,\n# 1 for List and 2 for Dict.  It may be -1 if variable number of\n# parameters are accepted (needs custom __getitem__).\n\nclass _SpecialGenericAlias(_NotIterable, _BaseGenericAlias, _root=True):\n    def __init__(self, origin, nparams, *, inst=True, name=None, defaults=()):\n        if name is None:\n            name = origin.__name__\n        super().__init__(origin, inst=inst, name=name)\n        self._nparams = nparams\n        self._defaults = defaults\n        if origin.__module__ == 'builtins':\n            self.__doc__ = f'A generic version of {origin.__qualname__}.'\n        else:\n            self.__doc__ = f'A generic version of {origin.__module__}.{origin.__qualname__}.'\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        if (self._defaults\n            and len(params) < self._nparams\n            and len(params) + len(self._defaults) >= self._nparams\n        ):\n            params = (*params, *self._defaults[len(params) - self._nparams:])\n        actual_len = len(params)\n\n        if actual_len != self._nparams:\n            if self._defaults:\n                expected = f\"at least {self._nparams - len(self._defaults)}\"\n            else:\n                expected = str(self._nparams)\n            if not self._nparams:\n                raise TypeError(f\"{self} is not a generic class\")\n            raise TypeError(f\"Too {'many' if actual_len > self._nparams else 'few'} arguments for {self};\"\n                            f\" actual {actual_len}, expected {expected}\")\n        return self.copy_with(params)\n\n    def copy_with(self, params):\n        return _GenericAlias(self.__origin__, params,\n                             name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __subclasscheck__(self, cls):\n        if isinstance(cls, _SpecialGenericAlias):\n            return issubclass(cls.__origin__, self.__origin__)\n        if not isinstance(cls, _GenericAlias):\n            return issubclass(cls, self.__origin__)\n        return super().__subclasscheck__(cls)\n\n    def __reduce__(self):\n        return self._name\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n\nclass _DeprecatedGenericAlias(_SpecialGenericAlias, _root=True):\n    def __init__(\n        self, origin, nparams, *, removal_version, inst=True, name=None\n    ):\n        super().__init__(origin, nparams, inst=inst, name=name)\n        self._removal_version = removal_version\n\n    def __instancecheck__(self, inst):\n        import warnings\n        warnings._deprecated(\n            f\"{self.__module__}.{self._name}\", remove=self._removal_version\n        )\n        return super().__instancecheck__(inst)\n\n\nclass _CallableGenericAlias(_NotIterable, _GenericAlias, _root=True):\n    def __repr__(self):\n        assert self._name == 'Callable'\n        args = self.__args__\n        if len(args) == 2 and _is_param_expr(args[0]):\n            return super().__repr__()\n        return (f'typing.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in args[:-1]])}], '\n                f'{_type_repr(args[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and _is_param_expr(args[0])):\n            args = list(args[:-1]), args[-1]\n        return operator.getitem, (Callable, args)\n\n\nclass _CallableType(_SpecialGenericAlias, _root=True):\n    def copy_with(self, params):\n        return _CallableGenericAlias(self.__origin__, params,\n                                     name=self._name, inst=self._inst)\n\n    def __getitem__(self, params):\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = params\n        # This relaxes what args can be on purpose to allow things like\n        # PEP 612 ParamSpec.  Responsibility for whether a user is using\n        # Callable[...] properly is deferred to static type checkers.\n        if isinstance(args, list):\n            params = (tuple(args), result)\n        else:\n            params = (args, result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        args, result = params\n        msg = \"Callable[args, result]: result must be a type.\"\n        result = _type_check(result, msg)\n        if args is Ellipsis:\n            return self.copy_with((_TypingEllipsis, result))\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = tuple(_type_convert(arg) for arg in args)\n        params = args + (result,)\n        return self.copy_with(params)\n\n\nclass _TupleType(_SpecialGenericAlias, _root=True):\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if len(params) >= 2 and params[-1] is ...:\n            msg = \"Tuple[t, ...]: t must be a type.\"\n            params = tuple(_type_check(p, msg) for p in params[:-1])\n            return self.copy_with((*params, _TypingEllipsis))\n        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        return self.copy_with(params)\n\n\nclass _UnionGenericAlias(_NotIterable, _GenericAlias, _root=True):\n    def copy_with(self, params):\n        return Union[params]\n\n    def __eq__(self, other):\n        if not isinstance(other, (_UnionGenericAlias, types.UnionType)):\n            return NotImplemented\n        try:  # fast path\n            return set(self.__args__) == set(other.__args__)\n        except TypeError:  # not hashable, slow path\n            return _compare_args_orderless(self.__args__, other.__args__)\n\n    def __hash__(self):\n        return hash(frozenset(self.__args__))\n\n    def __repr__(self):\n        args = self.__args__\n        if len(args) == 2:\n            if args[0] is type(None):\n                return f'typing.Optional[{_type_repr(args[1])}]'\n            elif args[1] is type(None):\n                return f'typing.Optional[{_type_repr(args[0])}]'\n        return super().__repr__()\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        for arg in self.__args__:\n            if issubclass(cls, arg):\n                return True\n\n    def __reduce__(self):\n        func, (origin, args) = super().__reduce__()\n        return func, (Union, args)\n\n\ndef _value_and_type_iter(parameters):\n    return ((p, type(p)) for p in parameters)\n\n\nclass _LiteralGenericAlias(_GenericAlias, _root=True):\n    def __eq__(self, other):\n        if not isinstance(other, _LiteralGenericAlias):\n            return NotImplemented\n\n        return set(_value_and_type_iter(self.__args__)) == set(_value_and_type_iter(other.__args__))\n\n    def __hash__(self):\n        return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n\nclass _ConcatenateGenericAlias(_GenericAlias, _root=True):\n    def copy_with(self, params):\n        if isinstance(params[-1], (list, tuple)):\n            return (*params[:-1], *params[-1])\n        if isinstance(params[-1], _ConcatenateGenericAlias):\n            params = (*params[:-1], *params[-1].__args__)\n        return super().copy_with(params)\n\n\n@_SpecialForm\ndef Unpack(self, parameters):\n    \"\"\"Type unpack operator.\n\n    The type unpack operator takes the child types from some container type,\n    such as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'.\n\n    For example::\n\n        # For some generic class `Foo`:\n        Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n        Ts = TypeVarTuple('Ts')\n        # Specifies that `Bar` is generic in an arbitrary number of types.\n        # (Think of `Ts` as a tuple of an arbitrary number of individual\n        #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n        #  `Generic[]`.)\n        class Bar(Generic[Unpack[Ts]]): ...\n        Bar[int]  # Valid\n        Bar[int, str]  # Also valid\n\n    From Python 3.11, this can also be done using the `*` operator::\n\n        Foo[*tuple[int, str]]\n        class Bar(Generic[*Ts]): ...\n\n    And from Python 3.12, it can be done using built-in syntax for generics::\n\n        Foo[*tuple[int, str]]\n        class Bar[*Ts]: ...\n\n    The operator can also be used along with a `TypedDict` to annotate\n    `**kwargs` in a function signature::\n\n        class Movie(TypedDict):\n            name: str\n            year: int\n\n        # This function expects two keyword arguments - *name* of type `str` and\n        # *year* of type `int`.\n        def foo(**kwargs: Unpack[Movie]): ...\n\n    Note that there is only some runtime checking of this operator. Not\n    everything the runtime allows may be accepted by static type checkers.\n\n    For more information, see PEPs 646 and 692.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _UnpackGenericAlias(origin=self, args=(item,))\n\n\nclass _UnpackGenericAlias(_GenericAlias, _root=True):\n    def __repr__(self):\n        # `Unpack` only takes one argument, so __args__ should contain only\n        # a single item.\n        return f'typing.Unpack[{_type_repr(self.__args__[0])}]'\n\n    def __getitem__(self, args):\n        if self.__typing_is_unpacked_typevartuple__:\n            return args\n        return super().__getitem__(args)\n\n    @property\n    def __typing_unpacked_tuple_args__(self):\n        assert self.__origin__ is Unpack\n        assert len(self.__args__) == 1\n        arg, = self.__args__\n        if isinstance(arg, (_GenericAlias, types.GenericAlias)):\n            if arg.__origin__ is not tuple:\n                raise TypeError(\"Unpack[...] must be used with a tuple type\")\n            return arg.__args__\n        return None\n\n    @property\n    def __typing_is_unpacked_typevartuple__(self):\n        assert self.__origin__ is Unpack\n        assert len(self.__args__) == 1\n        return isinstance(self.__args__[0], TypeVarTuple)\n\n\nclass _TypingEllipsis:\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\n_TYPING_INTERNALS = frozenset({\n    '__parameters__', '__orig_bases__',  '__orig_class__',\n    '_is_protocol', '_is_runtime_protocol', '__protocol_attrs__',\n    '__non_callable_proto_members__', '__type_params__',\n})\n\n_SPECIAL_NAMES = frozenset({\n    '__abstractmethods__', '__annotations__', '__dict__', '__doc__',\n    '__init__', '__module__', '__new__', '__slots__',\n    '__subclasshook__', '__weakref__', '__class_getitem__',\n    '__match_args__', '__static_attributes__', '__firstlineno__',\n})\n\n# These special attributes will be not collected as protocol members.\nEXCLUDED_ATTRIBUTES = _TYPING_INTERNALS | _SPECIAL_NAMES | {'_MutableMapping__marker'}\n\n\ndef _get_protocol_attrs(cls):\n    \"\"\"Collect protocol members from a protocol class objects.\n\n    This includes names actually defined in the class dictionary, as well\n    as names that appear in annotations. Special names (above) are skipped.\n    \"\"\"\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if not attr.startswith('_abc_') and attr not in EXCLUDED_ATTRIBUTES:\n                attrs.add(attr)\n    return attrs\n\n\ndef _no_init_or_replace_init(self, *args, **kwargs):\n    cls = type(self)\n\n    if cls._is_protocol:\n        raise TypeError('Protocols cannot be instantiated')\n\n    # Already using a custom `__init__`. No need to calculate correct\n    # `__init__` to call. This can lead to RecursionError. See bpo-45121.\n    if cls.__init__ is not _no_init_or_replace_init:\n        return\n\n    # Initially, `__init__` of a protocol subclass is set to `_no_init_or_replace_init`.\n    # The first instantiation of the subclass will call `_no_init_or_replace_init` which\n    # searches for a proper new `__init__` in the MRO. The new `__init__`\n    # replaces the subclass' old `__init__` (ie `_no_init_or_replace_init`). Subsequent\n    # instantiation of the protocol subclass will thus use the new\n    # `__init__` and no longer call `_no_init_or_replace_init`.\n    for base in cls.__mro__:\n        init = base.__dict__.get('__init__', _no_init_or_replace_init)\n        if init is not _no_init_or_replace_init:\n            cls.__init__ = init\n            break\n    else:\n        # should not happen\n        cls.__init__ = object.__init__\n\n    cls.__init__(self, *args, **kwargs)\n\n\ndef _caller(depth=1, default='__main__'):\n    try:\n        return sys._getframemodulename(depth + 1) or default\n    except AttributeError:  # For platforms without _getframemodulename()\n        pass\n    try:\n        return sys._getframe(depth + 1).f_globals.get('__name__', default)\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        pass\n    return None\n\ndef _allow_reckless_class_checks(depth=2):\n    \"\"\"Allow instance and class checks for special stdlib modules.\n\n    The abc and functools modules indiscriminately call isinstance() and\n    issubclass() on the whole MRO of a user class, which may contain protocols.\n    \"\"\"\n    return _caller(depth) in {'abc', 'functools', None}\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n}\n\n\n@functools.cache\ndef _lazy_load_getattr_static():\n    # Import getattr_static lazily so as not to slow down the import of typing.py\n    # Cache the result so we don't slow down _ProtocolMeta.__instancecheck__ unnecessarily\n    from inspect import getattr_static\n    return getattr_static\n\n\n_cleanups.append(_lazy_load_getattr_static.cache_clear)\n\ndef _pickle_psargs(psargs):\n    return ParamSpecArgs, (psargs.__origin__,)\n\ncopyreg.pickle(ParamSpecArgs, _pickle_psargs)\n\ndef _pickle_pskwargs(pskwargs):\n    return ParamSpecKwargs, (pskwargs.__origin__,)\n\ncopyreg.pickle(ParamSpecKwargs, _pickle_pskwargs)\n\ndel _pickle_psargs, _pickle_pskwargs\n\n\n# Preload these once, as globals, as a micro-optimisation.\n# This makes a significant difference to the time it takes\n# to do `isinstance()`/`issubclass()` checks\n# against runtime-checkable protocols with only one callable member.\n_abc_instancecheck = ABCMeta.__instancecheck__\n_abc_subclasscheck = ABCMeta.__subclasscheck__\n\n\ndef _type_check_issubclass_arg_1(arg):\n    \"\"\"Raise TypeError if `arg` is not an instance of `type`\n    in `issubclass(arg, <protocol>)`.\n\n    In most cases, this is verified by type.__subclasscheck__.\n    Checking it again unnecessarily would slow down issubclass() checks,\n    so, we don't perform this check unless we absolutely have to.\n\n    For various error paths, however,\n    we want to ensure that *this* error message is shown to the user\n    where relevant, rather than a typing.py-specific error message.\n    \"\"\"\n    if not isinstance(arg, type):\n        # Same error message as for issubclass(1, int).\n        raise TypeError('issubclass() arg 1 must be a class')\n\n\nclass _ProtocolMeta(ABCMeta):\n    # This metaclass is somewhat unfortunate,\n    # but is necessary for several reasons...\n    def __new__(mcls, name, bases, namespace, /, **kwargs):\n        if name == \"Protocol\" and bases == (Generic,):\n            pass\n        elif Protocol in bases:\n            for base in bases:\n                if not (\n                    base in {object, Generic}\n                    or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])\n                    or (\n                        issubclass(base, Generic)\n                        and getattr(base, \"_is_protocol\", False)\n                    )\n                ):\n                    raise TypeError(\n                        f\"Protocols can only inherit from other protocols, \"\n                        f\"got {base!r}\"\n                    )\n        return super().__new__(mcls, name, bases, namespace, **kwargs)\n\n    def __init__(cls, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if getattr(cls, \"_is_protocol\", False):\n            cls.__protocol_attrs__ = _get_protocol_attrs(cls)\n\n    def __subclasscheck__(cls, other):\n        if cls is Protocol:\n            return type.__subclasscheck__(cls, other)\n        if (\n            getattr(cls, '_is_protocol', False)\n            and not _allow_reckless_class_checks()\n        ):\n            if not getattr(cls, '_is_runtime_protocol', False):\n                _type_check_issubclass_arg_1(other)\n                raise TypeError(\n                    \"Instance and class checks can only be used with \"\n                    \"@runtime_checkable protocols\"\n                )\n            if (\n                # this attribute is set by @runtime_checkable:\n                cls.__non_callable_proto_members__\n                and cls.__dict__.get(\"__subclasshook__\") is _proto_hook\n            ):\n                _type_check_issubclass_arg_1(other)\n                non_method_attrs = sorted(cls.__non_callable_proto_members__)\n                raise TypeError(\n                    \"Protocols with non-method members don't support issubclass().\"\n                    f\" Non-method members: {str(non_method_attrs)[1:-1]}.\"\n                )\n        return _abc_subclasscheck(cls, other)\n\n    def __instancecheck__(cls, instance):\n        # We need this method for situations where attributes are\n        # assigned in __init__.\n        if cls is Protocol:\n            return type.__instancecheck__(cls, instance)\n        if not getattr(cls, \"_is_protocol\", False):\n            # i.e., it's a concrete subclass of a protocol\n            return _abc_instancecheck(cls, instance)\n\n        if (\n            not getattr(cls, '_is_runtime_protocol', False) and\n            not _allow_reckless_class_checks()\n        ):\n            raise TypeError(\"Instance and class checks can only be used with\"\n                            \" @runtime_checkable protocols\")\n\n        if _abc_instancecheck(cls, instance):\n            return True\n\n        getattr_static = _lazy_load_getattr_static()\n        for attr in cls.__protocol_attrs__:\n            try:\n                val = getattr_static(instance, attr)\n            except AttributeError:\n                break\n            # this attribute is set by @runtime_checkable:\n            if val is None and attr not in cls.__non_callable_proto_members__:\n                break\n        else:\n            return True\n\n        return False\n\n\n@classmethod\ndef _proto_hook(cls, other):\n    if not cls.__dict__.get('_is_protocol', False):\n        return NotImplemented\n\n    for attr in cls.__protocol_attrs__:\n        for base in other.__mro__:\n            # Check if the members appears in the class dictionary...\n            if attr in base.__dict__:\n                if base.__dict__[attr] is None:\n                    return NotImplemented\n                break\n\n            # ...or in annotations, if it is a sub-protocol.\n            annotations = getattr(base, '__annotations__', {})\n            if (isinstance(annotations, collections.abc.Mapping) and\n                    attr in annotations and\n                    issubclass(other, Generic) and getattr(other, '_is_protocol', False)):\n                break\n        else:\n            return NotImplemented\n    return True\n\n\nclass Protocol(Generic, metaclass=_ProtocolMeta):\n    \"\"\"Base class for protocol classes.\n\n    Protocol classes are defined as::\n\n        class Proto(Protocol):\n            def meth(self) -> int:\n                ...\n\n    Such classes are primarily used with static type checkers that recognize\n    structural subtyping (static duck-typing).\n\n    For example::\n\n        class C:\n            def meth(self) -> int:\n                return 0\n\n        def func(x: Proto) -> int:\n            return x.meth()\n\n        func(C())  # Passes static type check\n\n    See PEP 544 for details. Protocol classes decorated with\n    @typing.runtime_checkable act as simple-minded runtime protocols that check\n    only the presence of given attributes, ignoring their type signatures.\n    Protocol classes can be generic, they are defined as::\n\n        class GenProto[T](Protocol):\n            def meth(self) -> T:\n                ...\n    \"\"\"\n\n    __slots__ = ()\n    _is_protocol = True\n    _is_runtime_protocol = False\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n\n        # Determine if this is a protocol or a concrete subclass.\n        if not cls.__dict__.get('_is_protocol', False):\n            cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n        # Set (or override) the protocol subclass hook.\n        if '__subclasshook__' not in cls.__dict__:\n            cls.__subclasshook__ = _proto_hook\n\n        # Prohibit instantiation for protocol classes\n        if cls._is_protocol and cls.__init__ is Protocol.__init__:\n            cls.__init__ = _no_init_or_replace_init\n\n\nclass _AnnotatedAlias(_NotIterable, _GenericAlias, _root=True):\n    \"\"\"Runtime representation of an annotated type.\n\n    At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n    with extra annotations. The alias behaves like a normal typing alias.\n    Instantiating is the same as instantiating the underlying type; binding\n    it to types is also the same.\n\n    The metadata itself is stored in a '__metadata__' attribute as a tuple.\n    \"\"\"\n\n    def __init__(self, origin, metadata):\n        if isinstance(origin, _AnnotatedAlias):\n            metadata = origin.__metadata__ + metadata\n            origin = origin.__origin__\n        super().__init__(origin, origin, name='Annotated')\n        self.__metadata__ = metadata\n\n    def copy_with(self, params):\n        assert len(params) == 1\n        new_type = params[0]\n        return _AnnotatedAlias(new_type, self.__metadata__)\n\n    def __repr__(self):\n        return \"typing.Annotated[{}, {}]\".format(\n            _type_repr(self.__origin__),\n            \", \".join(repr(a) for a in self.__metadata__)\n        )\n\n    def __reduce__(self):\n        return operator.getitem, (\n            Annotated, (self.__origin__,) + self.__metadata__\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, _AnnotatedAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__metadata__ == other.__metadata__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__metadata__))\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return 'Annotated'\n        return super().__getattr__(attr)\n\n    def __mro_entries__(self, bases):\n        return (self.__origin__,)\n\n\n@_TypedCacheSpecialForm\n@_tp_cache(typed=True)\ndef Annotated(self, *params):\n    \"\"\"Add context-specific metadata to a type.\n\n    Example: Annotated[int, runtime_check.Unsigned] indicates to the\n    hypothetical runtime_check module that this type is an unsigned int.\n    Every other consumer of this type can ignore this metadata and treat\n    this type as int.\n\n    The first argument to Annotated must be a valid type.\n\n    Details:\n\n    - It's an error to call `Annotated` with less than two arguments.\n    - Access the metadata via the ``__metadata__`` attribute::\n\n        assert Annotated[int, '$'].__metadata__ == ('$',)\n\n    - Nested Annotated types are flattened::\n\n        assert Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n    - Instantiating an annotated type is equivalent to instantiating the\n    underlying type::\n\n        assert Annotated[C, Ann1](5) == C(5)\n\n    - Annotated can be used as a generic type alias::\n\n        type Optimized[T] = Annotated[T, runtime.Optimize()]\n        # type checker will treat Optimized[int]\n        # as equivalent to Annotated[int, runtime.Optimize()]\n\n        type OptimizedList[T] = Annotated[list[T], runtime.Optimize()]\n        # type checker will treat OptimizedList[int]\n        # as equivalent to Annotated[list[int], runtime.Optimize()]\n\n    - Annotated cannot be used with an unpacked TypeVarTuple::\n\n        type Variadic[*Ts] = Annotated[*Ts, Ann1]  # NOT valid\n\n      This would be equivalent to::\n\n        Annotated[T1, T2, T3, ..., Ann1]\n\n      where T1, T2 etc. are TypeVars, which would be invalid, because\n      only one type should be passed to Annotated.\n    \"\"\"\n    if len(params) < 2:\n        raise TypeError(\"Annotated[...] should be used \"\n                        \"with at least two arguments (a type and an \"\n                        \"annotation).\")\n    if _is_unpacked_typevartuple(params[0]):\n        raise TypeError(\"Annotated[...] should not be used with an \"\n                        \"unpacked TypeVarTuple\")\n    msg = \"Annotated[t, ...]: t must be a type.\"\n    origin = _type_check(params[0], msg, allow_special_forms=True)\n    metadata = tuple(params[1:])\n    return _AnnotatedAlias(origin, metadata)\n\n\ndef runtime_checkable(cls):\n    \"\"\"Mark a protocol class as a runtime protocol.\n\n    Such protocol can be used with isinstance() and issubclass().\n    Raise TypeError if applied to a non-protocol class.\n    This allows a simple-minded structural check very similar to\n    one trick ponies in collections.abc such as Iterable.\n\n    For example::\n\n        @runtime_checkable\n        class Closable(Protocol):\n            def close(self): ...\n\n        assert isinstance(open('/some/file'), Closable)\n\n    Warning: this will check only the presence of the required methods,\n    not their type signatures!\n    \"\"\"\n    if not issubclass(cls, Generic) or not getattr(cls, '_is_protocol', False):\n        raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                        ' got %r' % cls)\n    cls._is_runtime_protocol = True\n    # PEP 544 prohibits using issubclass()\n    # with protocols that have non-method members.\n    # See gh-113320 for why we compute this attribute here,\n    # rather than in `_ProtocolMeta.__init__`\n    cls.__non_callable_proto_members__ = set()\n    for attr in cls.__protocol_attrs__:\n        try:\n            is_callable = callable(getattr(cls, attr, None))\n        except Exception as e:\n            raise TypeError(\n                f\"Failed to determine whether protocol member {attr!r} \"\n                \"is a method member\"\n            ) from e\n        else:\n            if not is_callable:\n                cls.__non_callable_proto_members__.add(attr)\n    return cls\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef assert_type(val, typ, /):\n    \"\"\"Ask a static type checker to confirm that the value is of the given type.\n\n    At runtime this does nothing: it returns the first argument unchanged with no\n    checks or side effects, no matter the actual type of the argument.\n\n    When a static type checker encounters a call to assert_type(), it\n    emits an error if the value is not of the specified type::\n\n        def greet(name: str) -> None:\n            assert_type(name, str)  # OK\n            assert_type(name, int)  # type checker error\n    \"\"\"\n    return val\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n    \"\"\"Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals and recursively replaces all\n    'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module's globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.  For classes, the search\n      order is globals first then locals.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    \"\"\"\n    if getattr(obj, '__no_type_check__', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = getattr(sys.modules.get(base.__module__, None), '__dict__', {})\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get('__annotations__', {})\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = {}\n            base_locals = dict(vars(base)) if localns is None else localns\n            if localns is None and globalns is None:\n                # This is surprising, but required.  Before Python 3.10,\n                # get_type_hints only evaluated the globalns of\n                # a class.  To maintain backwards compatibility, we reverse\n                # the globalns and localns order so that eval() looks into\n                # *base_globals* first rather than *base_locals*.\n                # This only affects ForwardRefs.\n                base_globals, base_locals = base_locals, base_globals\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False, is_class=True)\n                value = _eval_type(value, base_globals, base_locals, base.__type_params__)\n                hints[name] = value\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, '__wrapped__'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, '__globals__', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, '__annotations__', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError('{!r} is not a module, class, method, '\n                            'or function.'.format(obj))\n    hints = dict(hints)\n    type_params = getattr(obj, \"__type_params__\", ())\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            # class-level forward refs were handled above, this must be either\n            # a module-level annotation or a function argument annotation\n            value = ForwardRef(\n                value,\n                is_argument=not isinstance(obj, types.ModuleType),\n                is_class=False,\n            )\n        hints[name] = _eval_type(value, globalns, localns, type_params)\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n\ndef _strip_annotations(t):\n    \"\"\"Strip the annotations from a given type.\"\"\"\n    if isinstance(t, _AnnotatedAlias):\n        return _strip_annotations(t.__origin__)\n    if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired, ReadOnly):\n        return _strip_annotations(t.__args__[0])\n    if isinstance(t, _GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return t.copy_with(stripped_args)\n    if isinstance(t, GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return GenericAlias(t.__origin__, stripped_args)\n    if isinstance(t, types.UnionType):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return functools.reduce(operator.or_, stripped_args)\n\n    return t\n\n\ndef get_origin(tp):\n    \"\"\"Get the unsubscripted version of a type.\n\n    This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar,\n    Annotated, and others. Return None for unsupported types.\n\n    Examples::\n\n        >>> P = ParamSpec('P')\n        >>> assert get_origin(Literal[42]) is Literal\n        >>> assert get_origin(int) is None\n        >>> assert get_origin(ClassVar[int]) is ClassVar\n        >>> assert get_origin(Generic) is Generic\n        >>> assert get_origin(Generic[T]) is Generic\n        >>> assert get_origin(Union[T, int]) is Union\n        >>> assert get_origin(List[Tuple[T, T]][int]) is list\n        >>> assert get_origin(P.args) is P\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return Annotated\n    if isinstance(tp, (_BaseGenericAlias, GenericAlias,\n                       ParamSpecArgs, ParamSpecKwargs)):\n        return tp.__origin__\n    if tp is Generic:\n        return Generic\n    if isinstance(tp, types.UnionType):\n        return types.UnionType\n    return None\n\n\ndef get_args(tp):\n    \"\"\"Get type arguments with all substitutions performed.\n\n    For unions, basic simplifications used by Union constructor are performed.\n\n    Examples::\n\n        >>> T = TypeVar('T')\n        >>> assert get_args(Dict[str, int]) == (str, int)\n        >>> assert get_args(int) == ()\n        >>> assert get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        >>> assert get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n        >>> assert get_args(Callable[[], T][int]) == ([], int)\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return (tp.__origin__,) + tp.__metadata__\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\n        res = tp.__args__\n        if _should_unflatten_callable_args(tp, res):\n            res = (list(res[:-1]), res[-1])\n        return res\n    if isinstance(tp, types.UnionType):\n        return tp.__args__\n    return ()\n\n\ndef is_typeddict(tp):\n    \"\"\"Check if an annotation is a TypedDict class.\n\n    For example::\n\n        >>> from typing import TypedDict\n        >>> class Film(TypedDict):\n        ...     title: str\n        ...     year: int\n        ...\n        >>> is_typeddict(Film)\n        True\n        >>> is_typeddict(dict)\n        False\n    \"\"\"\n    return isinstance(tp, _TypedDictMeta)\n\n\n_ASSERT_NEVER_REPR_MAX_LENGTH = 100\n\n\ndef assert_never(arg: Never, /) -> Never:\n    \"\"\"Statically assert that a line of code is unreachable.\n\n    Example::\n\n        def int_or_str(arg: int | str) -> None:\n            match arg:\n                case int():\n                    print(\"It's an int\")\n                case str():\n                    print(\"It's a str\")\n                case _:\n                    assert_never(arg)\n\n    If a type checker finds that a call to assert_never() is\n    reachable, it will emit an error.\n\n    At runtime, this throws an exception when called.\n    \"\"\"\n    value = repr(arg)\n    if len(value) > _ASSERT_NEVER_REPR_MAX_LENGTH:\n        value = value[:_ASSERT_NEVER_REPR_MAX_LENGTH] + '...'\n    raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        for key in dir(arg):\n            obj = getattr(arg, key)\n            if (\n                not hasattr(obj, '__qualname__')\n                or obj.__qualname__ != f'{arg.__qualname__}.{obj.__name__}'\n                or getattr(obj, '__module__', None) != arg.__module__\n            ):\n                # We only modify objects that are defined in this type directly.\n                # If classes / methods are nested in multiple layers,\n                # we will modify them when processing their direct holders.\n                continue\n            # Instance, class, and static methods:\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, types.MethodType):\n                obj.__func__.__no_type_check__ = True\n            # Nested types:\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n    import warnings\n    warnings._deprecated(\"typing.no_type_check_decorator\", remove=(3, 15))\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\n# {module: {qualname: {firstlineno: func}}}\n_overload_registry = defaultdict(functools.partial(defaultdict, dict))\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    \"\"\"\n    # classmethod and staticmethod\n    f = getattr(func, \"__func__\", func)\n    try:\n        _overload_registry[f.__module__][f.__qualname__][f.__code__.co_firstlineno] = func\n    except AttributeError:\n        # Not a normal function; ignore.\n        pass\n    return _overload_dummy\n\n\ndef get_overloads(func):\n    \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n    # classmethod and staticmethod\n    f = getattr(func, \"__func__\", func)\n    if f.__module__ not in _overload_registry:\n        return []\n    mod_dict = _overload_registry[f.__module__]\n    if f.__qualname__ not in mod_dict:\n        return []\n    return list(mod_dict[f.__qualname__].values())\n\n\ndef clear_overloads():\n    \"\"\"Clear all overloads in the registry.\"\"\"\n    _overload_registry.clear()\n\n\ndef final(f):\n    \"\"\"Decorator to indicate final methods and final classes.\n\n    Use this decorator to indicate to type checkers that the decorated\n    method cannot be overridden, and decorated class cannot be subclassed.\n\n    For example::\n\n        class Base:\n            @final\n            def done(self) -> None:\n                ...\n        class Sub(Base):\n            def done(self) -> None:  # Error reported by type checker\n                ...\n\n        @final\n        class Leaf:\n            ...\n        class Other(Leaf):  # Error reported by type checker\n            ...\n\n    There is no runtime checking of these properties. The decorator\n    attempts to set the ``__final__`` attribute to ``True`` on the decorated\n    object to allow runtime introspection.\n    \"\"\"\n    try:\n        f.__final__ = True\n    except (AttributeError, TypeError):\n        # Skip the attribute silently if it is not writable.\n        # AttributeError happens if the object has __slots__ or a\n        # read-only property, TypeError if it's a builtin class.\n        pass\n    return f\n\n\n# Some unconstrained type variables.  These were initially used by the container types.\n# They were never meant for export and are now unused, but we keep them around to\n# avoid breaking compatibility with users who import them.\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\n_alias = _SpecialGenericAlias\n\nHashable = _alias(collections.abc.Hashable, 0)  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, 1)\nCoroutine = _alias(collections.abc.Coroutine, 3)\nAsyncIterable = _alias(collections.abc.AsyncIterable, 1)\nAsyncIterator = _alias(collections.abc.AsyncIterator, 1)\nIterable = _alias(collections.abc.Iterable, 1)\nIterator = _alias(collections.abc.Iterator, 1)\nReversible = _alias(collections.abc.Reversible, 1)\nSized = _alias(collections.abc.Sized, 0)  # Not generic.\nContainer = _alias(collections.abc.Container, 1)\nCollection = _alias(collections.abc.Collection, 1)\nCallable = _CallableType(collections.abc.Callable, 2)\nCallable.__doc__ = \\\n    \"\"\"Deprecated alias to collections.abc.Callable.\n\n    Callable[[int], str] signifies a function that takes a single\n    parameter of type int and returns a str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.\n    The argument list must be a list of types, a ParamSpec,\n    Concatenate or ellipsis. The return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments;\n    such function types are rarely used as callback types.\n    \"\"\"\nAbstractSet = _alias(collections.abc.Set, 1, name='AbstractSet')\nMutableSet = _alias(collections.abc.MutableSet, 1)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, 2)\nMutableMapping = _alias(collections.abc.MutableMapping, 2)\nSequence = _alias(collections.abc.Sequence, 1)\nMutableSequence = _alias(collections.abc.MutableSequence, 1)\nByteString = _DeprecatedGenericAlias(\n    collections.abc.ByteString, 0, removal_version=(3, 14)  # Not generic.\n)\n# Tuple accepts variable number of parameters.\nTuple = _TupleType(tuple, -1, inst=False, name='Tuple')\nTuple.__doc__ = \\\n    \"\"\"Deprecated alias to builtins.tuple.\n\n    Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\nList = _alias(list, 1, inst=False, name='List')\nDeque = _alias(collections.deque, 1, name='Deque')\nSet = _alias(set, 1, inst=False, name='Set')\nFrozenSet = _alias(frozenset, 1, inst=False, name='FrozenSet')\nMappingView = _alias(collections.abc.MappingView, 1)\nKeysView = _alias(collections.abc.KeysView, 1)\nItemsView = _alias(collections.abc.ItemsView, 2)\nValuesView = _alias(collections.abc.ValuesView, 1)\nDict = _alias(dict, 2, inst=False, name='Dict')\nDefaultDict = _alias(collections.defaultdict, 2, name='DefaultDict')\nOrderedDict = _alias(collections.OrderedDict, 2)\nCounter = _alias(collections.Counter, 1)\nChainMap = _alias(collections.ChainMap, 2)\nGenerator = _alias(collections.abc.Generator, 3, defaults=(types.NoneType, types.NoneType))\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, 2, defaults=(types.NoneType,))\nType = _alias(type, 1, inst=False, name='Type')\nType.__doc__ = \\\n    \"\"\"Deprecated alias to builtins.type.\n\n    builtins.type or typing.Type can be used to annotate class objects.\n    For example, suppose we have the following classes::\n\n        class User: ...  # Abstract base for User classes\n        class BasicUser(User): ...\n        class ProUser(User): ...\n        class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n        def new_user[U](user_class: Type[U]) -> U:\n            user = user_class()\n            # (Here we could write the user object to a database)\n            return user\n\n        joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n\n\n@runtime_checkable\nclass SupportsInt(Protocol):\n    \"\"\"An ABC with one abstract method __int__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsFloat(Protocol):\n    \"\"\"An ABC with one abstract method __float__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\n@runtime_checkable\nclass SupportsBytes(Protocol):\n    \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\n@runtime_checkable\nclass SupportsIndex(Protocol):\n    \"\"\"An ABC with one abstract method __index__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __index__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsAbs[T](Protocol):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its return type.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T:\n        pass\n\n\n@runtime_checkable\nclass SupportsRound[T](Protocol):\n    \"\"\"An ABC with one abstract method __round__ that is covariant in its return type.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T:\n        pass\n\n\ndef _make_nmtuple(name, types, module, defaults = ()):\n    fields = [n for n, t in types]\n    types = {n: _type_check(t, f\"field {n} annotation must be a type\")\n             for n, t in types}\n    nm_tpl = collections.namedtuple(name, fields,\n                                    defaults=defaults, module=module)\n    nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = types\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = frozenset({'__new__', '__init__', '__slots__', '__getnewargs__',\n                         '_fields', '_field_defaults',\n                         '_make', '_replace', '_asdict', '_source'})\n\n_special = frozenset({'__module__', '__name__', '__annotations__'})\n\n\nclass NamedTupleMeta(type):\n    def __new__(cls, typename, bases, ns):\n        assert _NamedTuple in bases\n        for base in bases:\n            if base is not _NamedTuple and base is not Generic:\n                raise TypeError(\n                    'can only inherit from a NamedTuple type and Generic')\n        bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n        types = ns.get('__annotations__', {})\n        default_names = []\n        for field_name in types:\n            if field_name in ns:\n                default_names.append(field_name)\n            elif default_names:\n                raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                f\"cannot follow default field\"\n                                f\"{'s' if len(default_names) > 1 else ''} \"\n                                f\"{', '.join(default_names)}\")\n        nm_tpl = _make_nmtuple(typename, types.items(),\n                               defaults=[ns[n] for n in default_names],\n                               module=ns['__module__'])\n        nm_tpl.__bases__ = bases\n        if Generic in bases:\n            class_getitem = _generic_class_getitem\n            nm_tpl.__class_getitem__ = classmethod(class_getitem)\n        # update from user namespace without overriding special namedtuple attributes\n        for key, val in ns.items():\n            if key in _prohibited:\n                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n            elif key not in _special:\n                if key not in nm_tpl._fields:\n                    setattr(nm_tpl, key, val)\n                try:\n                    set_name = type(val).__set_name__\n                except AttributeError:\n                    pass\n                else:\n                    try:\n                        set_name(val, nm_tpl, key)\n                    except BaseException as e:\n                        e.add_note(\n                            f\"Error calling __set_name__ on {type(val).__name__!r} \"\n                            f\"instance {key!r} in {typename!r}\"\n                        )\n                        raise\n\n        if Generic in bases:\n            nm_tpl.__init_subclass__()\n        return nm_tpl\n\n\ndef NamedTuple(typename, fields=_sentinel, /, **kwargs):\n    \"\"\"Typed version of namedtuple.\n\n    Usage::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    An alternative equivalent functional syntax is also accepted::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    \"\"\"\n    if fields is _sentinel:\n        if kwargs:\n            deprecated_thing = \"Creating NamedTuple classes using keyword arguments\"\n            deprecation_msg = (\n                \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                \"Use the class-based or functional syntax instead.\"\n            )\n        else:\n            deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n            example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n            deprecation_msg = (\n                \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                \"To create a NamedTuple class with 0 fields \"\n                \"using the functional syntax, \"\n                \"pass an empty list, e.g. \"\n            ) + example + \".\"\n    elif fields is None:\n        if kwargs:\n            raise TypeError(\n                \"Cannot pass `None` as the 'fields' parameter \"\n                \"and also specify fields using keyword arguments\"\n            )\n        else:\n            deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n            example = f\"`{typename} = NamedTuple({typename!r}, [])`\"\n            deprecation_msg = (\n                \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                \"To create a NamedTuple class with 0 fields \"\n                \"using the functional syntax, \"\n                \"pass an empty list, e.g. \"\n            ) + example + \".\"\n    elif kwargs:\n        raise TypeError(\"Either list of fields or keywords\"\n                        \" can be provided to NamedTuple, not both\")\n    if fields is _sentinel or fields is None:\n        import warnings\n        warnings._deprecated(deprecated_thing, message=deprecation_msg, remove=(3, 15))\n        fields = kwargs.items()\n    nt = _make_nmtuple(typename, fields, module=_caller())\n    nt.__orig_bases__ = (NamedTuple,)\n    return nt\n\n_NamedTuple = type.__new__(NamedTupleMeta, 'NamedTuple', (), {})\n\ndef _namedtuple_mro_entries(bases):\n    assert NamedTuple in bases\n    return (_NamedTuple,)\n\nNamedTuple.__mro_entries__ = _namedtuple_mro_entries\n\n\ndef _get_typeddict_qualifiers(annotation_type):\n    while True:\n        annotation_origin = get_origin(annotation_type)\n        if annotation_origin is Annotated:\n            annotation_args = get_args(annotation_type)\n            if annotation_args:\n                annotation_type = annotation_args[0]\n            else:\n                break\n        elif annotation_origin is Required:\n            yield Required\n            (annotation_type,) = get_args(annotation_type)\n        elif annotation_origin is NotRequired:\n            yield NotRequired\n            (annotation_type,) = get_args(annotation_type)\n        elif annotation_origin is ReadOnly:\n            yield ReadOnly\n            (annotation_type,) = get_args(annotation_type)\n        else:\n            break\n\n\nclass _TypedDictMeta(type):\n    def __new__(cls, name, bases, ns, total=True):\n        \"\"\"Create a new typed dict class object.\n\n        This method is called when TypedDict is subclassed,\n        or when TypedDict is instantiated. This way\n        TypedDict supports all three syntax forms described in its docstring.\n        Subclasses and instances of TypedDict return actual dictionaries.\n        \"\"\"\n        for base in bases:\n            if type(base) is not _TypedDictMeta and base is not Generic:\n                raise TypeError('cannot inherit from both a TypedDict type '\n                                'and a non-TypedDict base class')\n\n        if any(issubclass(b, Generic) for b in bases):\n            generic_base = (Generic,)\n        else:\n            generic_base = ()\n\n        tp_dict = type.__new__(_TypedDictMeta, name, (*generic_base, dict), ns)\n\n        if not hasattr(tp_dict, '__orig_bases__'):\n            tp_dict.__orig_bases__ = bases\n\n        annotations = {}\n        own_annotations = ns.get('__annotations__', {})\n        msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n        own_annotations = {\n            n: _type_check(tp, msg, module=tp_dict.__module__)\n            for n, tp in own_annotations.items()\n        }\n        required_keys = set()\n        optional_keys = set()\n        readonly_keys = set()\n        mutable_keys = set()\n\n        for base in bases:\n            annotations.update(base.__dict__.get('__annotations__', {}))\n\n            base_required = base.__dict__.get('__required_keys__', set())\n            required_keys |= base_required\n            optional_keys -= base_required\n\n            base_optional = base.__dict__.get('__optional_keys__', set())\n            required_keys -= base_optional\n            optional_keys |= base_optional\n\n            readonly_keys.update(base.__dict__.get('__readonly_keys__', ()))\n            mutable_keys.update(base.__dict__.get('__mutable_keys__', ()))\n\n        annotations.update(own_annotations)\n        for annotation_key, annotation_type in own_annotations.items():\n            qualifiers = set(_get_typeddict_qualifiers(annotation_type))\n            if Required in qualifiers:\n                is_required = True\n            elif NotRequired in qualifiers:\n                is_required = False\n            else:\n                is_required = total\n\n            if is_required:\n                required_keys.add(annotation_key)\n                optional_keys.discard(annotation_key)\n            else:\n                optional_keys.add(annotation_key)\n                required_keys.discard(annotation_key)\n\n            if ReadOnly in qualifiers:\n                if annotation_key in mutable_keys:\n                    raise TypeError(\n                        f\"Cannot override mutable key {annotation_key!r}\"\n                        \" with read-only key\"\n                    )\n                readonly_keys.add(annotation_key)\n            else:\n                mutable_keys.add(annotation_key)\n                readonly_keys.discard(annotation_key)\n\n        assert required_keys.isdisjoint(optional_keys), (\n            f\"Required keys overlap with optional keys in {name}:\"\n            f\" {required_keys=}, {optional_keys=}\"\n        )\n        tp_dict.__annotations__ = annotations\n        tp_dict.__required_keys__ = frozenset(required_keys)\n        tp_dict.__optional_keys__ = frozenset(optional_keys)\n        tp_dict.__readonly_keys__ = frozenset(readonly_keys)\n        tp_dict.__mutable_keys__ = frozenset(mutable_keys)\n        tp_dict.__total__ = total\n        return tp_dict\n\n    __call__ = dict  # static method\n\n    def __subclasscheck__(cls, other):\n        # Typed dicts are only for static structural subtyping.\n        raise TypeError('TypedDict does not support instance and class checks')\n\n    __instancecheck__ = __subclasscheck__\n\n\ndef TypedDict(typename, fields=_sentinel, /, *, total=True):\n    \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n    TypedDict creates a dictionary type such that a type checker will expect all\n    instances to have a certain set of keys, where each key is\n    associated with a value of a consistent type. This expectation\n    is not checked at runtime.\n\n    Usage::\n\n        >>> class Point2D(TypedDict):\n        ...     x: int\n        ...     y: int\n        ...     label: str\n        ...\n        >>> a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n        >>> b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n        >>> Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n        True\n\n    The type info can be accessed via the Point2D.__annotations__ dict, and\n    the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n    TypedDict supports an additional equivalent form::\n\n        Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n    By default, all keys must be present in a TypedDict. It is possible\n    to override this by specifying totality::\n\n        class Point2D(TypedDict, total=False):\n            x: int\n            y: int\n\n    This means that a Point2D TypedDict can have any of the keys omitted. A type\n    checker is only expected to support a literal False or True as the value of\n    the total argument. True is the default, and makes all items defined in the\n    class body be required.\n\n    The Required and NotRequired special forms can also be used to mark\n    individual keys as being required or not required::\n\n        class Point2D(TypedDict):\n            x: int               # the \"x\" key must always be present (Required is the default)\n            y: NotRequired[int]  # the \"y\" key can be omitted\n\n    See PEP 655 for more details on Required and NotRequired.\n\n    The ReadOnly special form can be used\n    to mark individual keys as immutable for type checkers::\n\n        class DatabaseUser(TypedDict):\n            id: ReadOnly[int]  # the \"id\" key must not be modified\n            username: str      # the \"username\" key can be changed\n\n    \"\"\"\n    if fields is _sentinel or fields is None:\n        import warnings\n\n        if fields is _sentinel:\n            deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n        else:\n            deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n\n        example = f\"`{typename} = TypedDict({typename!r}, {{{{}}}})`\"\n        deprecation_msg = (\n            \"{name} is deprecated and will be disallowed in Python {remove}. \"\n            \"To create a TypedDict class with 0 fields \"\n            \"using the functional syntax, \"\n            \"pass an empty dictionary, e.g. \"\n        ) + example + \".\"\n        warnings._deprecated(deprecated_thing, message=deprecation_msg, remove=(3, 15))\n        fields = {}\n\n    ns = {'__annotations__': dict(fields)}\n    module = _caller()\n    if module is not None:\n        # Setting correct module is necessary to make typed dict classes pickleable.\n        ns['__module__'] = module\n\n    td = _TypedDictMeta(typename, (), ns, total=total)\n    td.__orig_bases__ = (TypedDict,)\n    return td\n\n_TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\nTypedDict.__mro_entries__ = lambda bases: (_TypedDict,)\n\n\n@_SpecialForm\ndef Required(self, parameters):\n    \"\"\"Special typing construct to mark a TypedDict key as required.\n\n    This is mainly useful for total=False TypedDicts.\n\n    For example::\n\n        class Movie(TypedDict, total=False):\n            title: Required[str]\n            year: int\n\n        m = Movie(\n            title='The Matrix',  # typechecker error if key is omitted\n            year=1999,\n        )\n\n    There is no runtime checking that a required key is actually provided\n    when instantiating a related TypedDict.\n    \"\"\"\n    item = _type_check(parameters, f'{self._name} accepts only a single type.')\n    return _GenericAlias(self, (item,))\n\n\n@_SpecialForm\ndef NotRequired(self, parameters):\n    \"\"\"Special typing construct to mark a TypedDict key as potentially missing.\n\n    For example::\n\n        class Movie(TypedDict):\n            title: str\n            year: NotRequired[int]\n\n        m = Movie(\n            title='The Matrix',  # typechecker error if key is omitted\n            year=1999,\n        )\n    \"\"\"\n    item = _type_check(parameters, f'{self._name} accepts only a single type.')\n    return _GenericAlias(self, (item,))\n\n\n@_SpecialForm\ndef ReadOnly(self, parameters):\n    \"\"\"A special typing construct to mark an item of a TypedDict as read-only.\n\n    For example::\n\n        class Movie(TypedDict):\n            title: ReadOnly[str]\n            year: int\n\n        def mutate_movie(m: Movie) -> None:\n            m[\"year\"] = 1992  # allowed\n            m[\"title\"] = \"The Matrix\"  # typechecker error\n\n    There is no runtime checking for this property.\n    \"\"\"\n    item = _type_check(parameters, f'{self._name} accepts only a single type.')\n    return _GenericAlias(self, (item,))\n\n\nclass NewType:\n    \"\"\"NewType creates simple unique types with almost zero runtime overhead.\n\n    NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy callable that simply returns its argument.\n\n    Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    __call__ = _idfunc\n\n    def __init__(self, name, tp):\n        self.__qualname__ = name\n        if '.' in name:\n            name = name.rpartition('.')[-1]\n        self.__name__ = name\n        self.__supertype__ = tp\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __mro_entries__(self, bases):\n        # We defined __mro_entries__ to get a better error message\n        # if a user attempts to subclass a NewType instance. bpo-46170\n        superclass_name = self.__name__\n\n        class Dummy:\n            def __init_subclass__(cls):\n                subclass_name = cls.__name__\n                raise TypeError(\n                    f\"Cannot subclass an instance of NewType. Perhaps you were looking for: \"\n                    f\"`{subclass_name} = NewType({subclass_name!r}, {superclass_name})`\"\n                )\n\n        return (Dummy,)\n\n    def __repr__(self):\n        return f'{self.__module__}.{self.__qualname__}'\n\n    def __reduce__(self):\n        return self.__qualname__\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def mode(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'IO[AnyStr]':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'BinaryIO':\n        pass\n\n\nclass TextIO(IO[str]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @property\n    @abstractmethod\n    def encoding(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def errors(self) -> Optional[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def line_buffering(self) -> bool:\n        pass\n\n    @property\n    @abstractmethod\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'TextIO':\n        pass\n\n\ndef reveal_type[T](obj: T, /) -> T:\n    \"\"\"Ask a static type checker to reveal the inferred type of an expression.\n\n    When a static type checker encounters a call to ``reveal_type()``,\n    it will emit the inferred type of the argument::\n\n        x: int = 1\n        reveal_type(x)\n\n    Running a static type checker (e.g., mypy) on this example\n    will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n    At runtime, the function prints the runtime type of the\n    argument and returns the argument unchanged.\n    \"\"\"\n    print(f\"Runtime type is {type(obj).__name__!r}\", file=sys.stderr)\n    return obj\n\n\nclass _IdentityCallable(Protocol):\n    def __call__[T](self, arg: T, /) -> T:\n        ...\n\n\ndef dataclass_transform(\n    *,\n    eq_default: bool = True,\n    order_default: bool = False,\n    kw_only_default: bool = False,\n    frozen_default: bool = False,\n    field_specifiers: tuple[type[Any] | Callable[..., Any], ...] = (),\n    **kwargs: Any,\n) -> _IdentityCallable:\n    \"\"\"Decorator to mark an object as providing dataclass-like behaviour.\n\n    The decorator can be applied to a function, class, or metaclass.\n\n    Example usage with a decorator function::\n\n        @dataclass_transform()\n        def create_model[T](cls: type[T]) -> type[T]:\n            ...\n            return cls\n\n        @create_model\n        class CustomerModel:\n            id: int\n            name: str\n\n    On a base class::\n\n        @dataclass_transform()\n        class ModelBase: ...\n\n        class CustomerModel(ModelBase):\n            id: int\n            name: str\n\n    On a metaclass::\n\n        @dataclass_transform()\n        class ModelMeta(type): ...\n\n        class ModelBase(metaclass=ModelMeta): ...\n\n        class CustomerModel(ModelBase):\n            id: int\n            name: str\n\n    The ``CustomerModel`` classes defined above will\n    be treated by type checkers similarly to classes created with\n    ``@dataclasses.dataclass``.\n    For example, type checkers will assume these classes have\n    ``__init__`` methods that accept ``id`` and ``name``.\n\n    The arguments to this decorator can be used to customize this behavior:\n    - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n        ``True`` or ``False`` if it is omitted by the caller.\n    - ``order_default`` indicates whether the ``order`` parameter is\n        assumed to be True or False if it is omitted by the caller.\n    - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n        assumed to be True or False if it is omitted by the caller.\n    - ``frozen_default`` indicates whether the ``frozen`` parameter is\n        assumed to be True or False if it is omitted by the caller.\n    - ``field_specifiers`` specifies a static list of supported classes\n        or functions that describe fields, similar to ``dataclasses.field()``.\n    - Arbitrary other keyword arguments are accepted in order to allow for\n        possible future extensions.\n\n    At runtime, this decorator records its arguments in the\n    ``__dataclass_transform__`` attribute on the decorated object.\n    It has no other runtime effect.\n\n    See PEP 681 for more details.\n    \"\"\"\n    def decorator(cls_or_fn):\n        cls_or_fn.__dataclass_transform__ = {\n            \"eq_default\": eq_default,\n            \"order_default\": order_default,\n            \"kw_only_default\": kw_only_default,\n            \"frozen_default\": frozen_default,\n            \"field_specifiers\": field_specifiers,\n            \"kwargs\": kwargs,\n        }\n        return cls_or_fn\n    return decorator\n\n\ntype _Func = Callable[..., Any]\n\n\ndef override[F: _Func](method: F, /) -> F:\n    \"\"\"Indicate that a method is intended to override a method in a base class.\n\n    Usage::\n\n        class Base:\n            def method(self) -> None:\n                pass\n\n        class Child(Base):\n            @override\n            def method(self) -> None:\n                super().method()\n\n    When this decorator is applied to a method, the type checker will\n    validate that it overrides a method or attribute with the same name on a\n    base class.  This helps prevent bugs that may occur when a base class is\n    changed without an equivalent change to a child class.\n\n    There is no runtime checking of this property. The decorator attempts to\n    set the ``__override__`` attribute to ``True`` on the decorated object to\n    allow runtime introspection.\n\n    See PEP 698 for details.\n    \"\"\"\n    try:\n        method.__override__ = True\n    except (AttributeError, TypeError):\n        # Skip the attribute silently if it is not writable.\n        # AttributeError happens if the object has __slots__ or a\n        # read-only property, TypeError if it's a builtin class.\n        pass\n    return method\n\n\ndef is_protocol(tp: type, /) -> bool:\n    \"\"\"Return True if the given type is a Protocol.\n\n    Example::\n\n        >>> from typing import Protocol, is_protocol\n        >>> class P(Protocol):\n        ...     def a(self) -> str: ...\n        ...     b: int\n        >>> is_protocol(P)\n        True\n        >>> is_protocol(int)\n        False\n    \"\"\"\n    return (\n        isinstance(tp, type)\n        and getattr(tp, '_is_protocol', False)\n        and tp != Protocol\n    )\n\n\ndef get_protocol_members(tp: type, /) -> frozenset[str]:\n    \"\"\"Return the set of members defined in a Protocol.\n\n    Example::\n\n        >>> from typing import Protocol, get_protocol_members\n        >>> class P(Protocol):\n        ...     def a(self) -> str: ...\n        ...     b: int\n        >>> get_protocol_members(P) == frozenset({'a', 'b'})\n        True\n\n    Raise a TypeError for arguments that are not Protocols.\n    \"\"\"\n    if not is_protocol(tp):\n        raise TypeError(f'{tp!r} is not a Protocol')\n    return frozenset(tp.__protocol_attrs__)\n\n\ndef __getattr__(attr):\n    \"\"\"Improve the import time of the typing module.\n\n    Soft-deprecated objects which are costly to create\n    are only created on-demand here.\n    \"\"\"\n    if attr in {\"Pattern\", \"Match\"}:\n        import re\n        obj = _alias(getattr(re, attr), 1)\n    elif attr in {\"ContextManager\", \"AsyncContextManager\"}:\n        import contextlib\n        obj = _alias(getattr(contextlib, f\"Abstract{attr}\"), 2, name=attr, defaults=(bool | None,))\n    elif attr == \"_collect_parameters\":\n        import warnings\n\n        depr_message = (\n            \"The private _collect_parameters function is deprecated and will be\"\n            \" removed in a future version of Python. Any use of private functions\"\n            \" is discouraged and may break in the future.\"\n        )\n        warnings.warn(depr_message, category=DeprecationWarning, stacklevel=2)\n        obj = _collect_type_parameters\n    else:\n        raise AttributeError(f\"module {__name__!r} has no attribute {attr!r}\")\n    globals()[attr] = obj\n    return obj\n", 3814], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py": ["#\n# A higher level module for using sockets (or Windows named pipes)\n#\n# multiprocessing/connection.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = [ 'Client', 'Listener', 'Pipe', 'wait' ]\n\nimport errno\nimport io\nimport os\nimport sys\nimport socket\nimport struct\nimport time\nimport tempfile\nimport itertools\n\n\nfrom . import util\n\nfrom . import AuthenticationError, BufferTooShort\nfrom .context import reduction\n_ForkingPickler = reduction.ForkingPickler\n\ntry:\n    import _multiprocessing\n    import _winapi\n    from _winapi import WAIT_OBJECT_0, WAIT_ABANDONED_0, WAIT_TIMEOUT, INFINITE\nexcept ImportError:\n    if sys.platform == 'win32':\n        raise\n    _winapi = None\n\n#\n#\n#\n\nBUFSIZE = 8192\n# A very generous timeout when it comes to local connections...\nCONNECTION_TIMEOUT = 20.\n\n_mmap_counter = itertools.count()\n\ndefault_family = 'AF_INET'\nfamilies = ['AF_INET']\n\nif hasattr(socket, 'AF_UNIX'):\n    default_family = 'AF_UNIX'\n    families += ['AF_UNIX']\n\nif sys.platform == 'win32':\n    default_family = 'AF_PIPE'\n    families += ['AF_PIPE']\n\n\ndef _init_timeout(timeout=CONNECTION_TIMEOUT):\n    return time.monotonic() + timeout\n\ndef _check_timeout(t):\n    return time.monotonic() > t\n\n#\n#\n#\n\ndef arbitrary_address(family):\n    '''\n    Return an arbitrary free address for the given family\n    '''\n    if family == 'AF_INET':\n        return ('localhost', 0)\n    elif family == 'AF_UNIX':\n        return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n    elif family == 'AF_PIPE':\n        return tempfile.mktemp(prefix=r'\\\\.\\pipe\\pyc-%d-%d-' %\n                               (os.getpid(), next(_mmap_counter)), dir=\"\")\n    else:\n        raise ValueError('unrecognized family')\n\ndef _validate_family(family):\n    '''\n    Checks if the family is valid for the current environment.\n    '''\n    if sys.platform != 'win32' and family == 'AF_PIPE':\n        raise ValueError('Family %s is not recognized.' % family)\n\n    if sys.platform == 'win32' and family == 'AF_UNIX':\n        # double check\n        if not hasattr(socket, family):\n            raise ValueError('Family %s is not recognized.' % family)\n\ndef address_type(address):\n    '''\n    Return the types of the address\n\n    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'\n    '''\n    if type(address) == tuple:\n        return 'AF_INET'\n    elif type(address) is str and address.startswith('\\\\\\\\'):\n        return 'AF_PIPE'\n    elif type(address) is str or util.is_abstract_socket_namespace(address):\n        return 'AF_UNIX'\n    else:\n        raise ValueError('address type of %r unrecognized' % address)\n\n#\n# Connection classes\n#\n\nclass _ConnectionBase:\n    _handle = None\n\n    def __init__(self, handle, readable=True, writable=True):\n        handle = handle.__index__()\n        if handle < 0:\n            raise ValueError(\"invalid handle\")\n        if not readable and not writable:\n            raise ValueError(\n                \"at least one of `readable` and `writable` must be True\")\n        self._handle = handle\n        self._readable = readable\n        self._writable = writable\n\n    # XXX should we use util.Finalize instead of a __del__?\n\n    def __del__(self):\n        if self._handle is not None:\n            self._close()\n\n    def _check_closed(self):\n        if self._handle is None:\n            raise OSError(\"handle is closed\")\n\n    def _check_readable(self):\n        if not self._readable:\n            raise OSError(\"connection is write-only\")\n\n    def _check_writable(self):\n        if not self._writable:\n            raise OSError(\"connection is read-only\")\n\n    def _bad_message_length(self):\n        if self._writable:\n            self._readable = False\n        else:\n            self.close()\n        raise OSError(\"bad message length\")\n\n    @property\n    def closed(self):\n        \"\"\"True if the connection is closed\"\"\"\n        return self._handle is None\n\n    @property\n    def readable(self):\n        \"\"\"True if the connection is readable\"\"\"\n        return self._readable\n\n    @property\n    def writable(self):\n        \"\"\"True if the connection is writable\"\"\"\n        return self._writable\n\n    def fileno(self):\n        \"\"\"File descriptor or handle of the connection\"\"\"\n        self._check_closed()\n        return self._handle\n\n    def close(self):\n        \"\"\"Close the connection\"\"\"\n        if self._handle is not None:\n            try:\n                self._close()\n            finally:\n                self._handle = None\n\n    def send_bytes(self, buf, offset=0, size=None):\n        \"\"\"Send the bytes data from a bytes-like object\"\"\"\n        self._check_closed()\n        self._check_writable()\n        m = memoryview(buf)\n        if m.itemsize > 1:\n            m = m.cast('B')\n        n = m.nbytes\n        if offset < 0:\n            raise ValueError(\"offset is negative\")\n        if n < offset:\n            raise ValueError(\"buffer length < offset\")\n        if size is None:\n            size = n - offset\n        elif size < 0:\n            raise ValueError(\"size is negative\")\n        elif offset + size > n:\n            raise ValueError(\"buffer length < offset + size\")\n        self._send_bytes(m[offset:offset + size])\n\n    def send(self, obj):\n        \"\"\"Send a (picklable) object\"\"\"\n        self._check_closed()\n        self._check_writable()\n        self._send_bytes(_ForkingPickler.dumps(obj))\n\n    def recv_bytes(self, maxlength=None):\n        \"\"\"\n        Receive bytes data as a bytes object.\n        \"\"\"\n        self._check_closed()\n        self._check_readable()\n        if maxlength is not None and maxlength < 0:\n            raise ValueError(\"negative maxlength\")\n        buf = self._recv_bytes(maxlength)\n        if buf is None:\n            self._bad_message_length()\n        return buf.getvalue()\n\n    def recv_bytes_into(self, buf, offset=0):\n        \"\"\"\n        Receive bytes data into a writeable bytes-like object.\n        Return the number of bytes read.\n        \"\"\"\n        self._check_closed()\n        self._check_readable()\n        with memoryview(buf) as m:\n            # Get bytesize of arbitrary buffer\n            itemsize = m.itemsize\n            bytesize = itemsize * len(m)\n            if offset < 0:\n                raise ValueError(\"negative offset\")\n            elif offset > bytesize:\n                raise ValueError(\"offset too large\")\n            result = self._recv_bytes()\n            size = result.tell()\n            if bytesize < offset + size:\n                raise BufferTooShort(result.getvalue())\n            # Message can fit in dest\n            result.seek(0)\n            result.readinto(m[offset // itemsize :\n                              (offset + size) // itemsize])\n            return size\n\n    def recv(self):\n        \"\"\"Receive a (picklable) object\"\"\"\n        self._check_closed()\n        self._check_readable()\n        buf = self._recv_bytes()\n        return _ForkingPickler.loads(buf.getbuffer())\n\n    def poll(self, timeout=0.0):\n        \"\"\"Whether there is any input available to be read\"\"\"\n        self._check_closed()\n        self._check_readable()\n        return self._poll(timeout)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        self.close()\n\n\nif _winapi:\n\n    class PipeConnection(_ConnectionBase):\n        \"\"\"\n        Connection class based on a Windows named pipe.\n        Overlapped I/O is used, so the handles must have been created\n        with FILE_FLAG_OVERLAPPED.\n        \"\"\"\n        _got_empty_message = False\n        _send_ov = None\n\n        def _close(self, _CloseHandle=_winapi.CloseHandle):\n            ov = self._send_ov\n            if ov is not None:\n                # Interrupt WaitForMultipleObjects() in _send_bytes()\n                ov.cancel()\n            _CloseHandle(self._handle)\n\n        def _send_bytes(self, buf):\n            if self._send_ov is not None:\n                # A connection should only be used by a single thread\n                raise ValueError(\"concurrent send_bytes() calls \"\n                                 \"are not supported\")\n            ov, err = _winapi.WriteFile(self._handle, buf, overlapped=True)\n            self._send_ov = ov\n            try:\n                if err == _winapi.ERROR_IO_PENDING:\n                    waitres = _winapi.WaitForMultipleObjects(\n                        [ov.event], False, INFINITE)\n                    assert waitres == WAIT_OBJECT_0\n            except:\n                ov.cancel()\n                raise\n            finally:\n                self._send_ov = None\n                nwritten, err = ov.GetOverlappedResult(True)\n            if err == _winapi.ERROR_OPERATION_ABORTED:\n                # close() was called by another thread while\n                # WaitForMultipleObjects() was waiting for the overlapped\n                # operation.\n                raise OSError(errno.EPIPE, \"handle is closed\")\n            assert err == 0\n            assert nwritten == len(buf)\n\n        def _recv_bytes(self, maxsize=None):\n            if self._got_empty_message:\n                self._got_empty_message = False\n                return io.BytesIO()\n            else:\n                bsize = 128 if maxsize is None else min(maxsize, 128)\n                try:\n                    ov, err = _winapi.ReadFile(self._handle, bsize,\n                                                overlapped=True)\n                    try:\n                        if err == _winapi.ERROR_IO_PENDING:\n                            waitres = _winapi.WaitForMultipleObjects(\n                                [ov.event], False, INFINITE)\n                            assert waitres == WAIT_OBJECT_0\n                    except:\n                        ov.cancel()\n                        raise\n                    finally:\n                        nread, err = ov.GetOverlappedResult(True)\n                        if err == 0:\n                            f = io.BytesIO()\n                            f.write(ov.getbuffer())\n                            return f\n                        elif err == _winapi.ERROR_MORE_DATA:\n                            return self._get_more_data(ov, maxsize)\n                except OSError as e:\n                    if e.winerror == _winapi.ERROR_BROKEN_PIPE:\n                        raise EOFError\n                    else:\n                        raise\n            raise RuntimeError(\"shouldn't get here; expected KeyboardInterrupt\")\n\n        def _poll(self, timeout):\n            if (self._got_empty_message or\n                        _winapi.PeekNamedPipe(self._handle)[0] != 0):\n                return True\n            return bool(wait([self], timeout))\n\n        def _get_more_data(self, ov, maxsize):\n            buf = ov.getbuffer()\n            f = io.BytesIO()\n            f.write(buf)\n            left = _winapi.PeekNamedPipe(self._handle)[1]\n            assert left > 0\n            if maxsize is not None and len(buf) + left > maxsize:\n                self._bad_message_length()\n            ov, err = _winapi.ReadFile(self._handle, left, overlapped=True)\n            rbytes, err = ov.GetOverlappedResult(True)\n            assert err == 0\n            assert rbytes == left\n            f.write(ov.getbuffer())\n            return f\n\n\nclass Connection(_ConnectionBase):\n    \"\"\"\n    Connection class based on an arbitrary file descriptor (Unix only), or\n    a socket handle (Windows).\n    \"\"\"\n\n    if _winapi:\n        def _close(self, _close=_multiprocessing.closesocket):\n            _close(self._handle)\n        _write = _multiprocessing.send\n        _read = _multiprocessing.recv\n    else:\n        def _close(self, _close=os.close):\n            _close(self._handle)\n        _write = os.write\n        _read = os.read\n\n    def _send(self, buf, write=_write):\n        remaining = len(buf)\n        while True:\n            n = write(self._handle, buf)\n            remaining -= n\n            if remaining == 0:\n                break\n            buf = buf[n:]\n\n    def _recv(self, size, read=_read):\n        buf = io.BytesIO()\n        handle = self._handle\n        remaining = size\n        while remaining > 0:\n            chunk = read(handle, remaining)\n            n = len(chunk)\n            if n == 0:\n                if remaining == size:\n                    raise EOFError\n                else:\n                    raise OSError(\"got end of file during message\")\n            buf.write(chunk)\n            remaining -= n\n        return buf\n\n    def _send_bytes(self, buf):\n        n = len(buf)\n        if n > 0x7fffffff:\n            pre_header = struct.pack(\"!i\", -1)\n            header = struct.pack(\"!Q\", n)\n            self._send(pre_header)\n            self._send(header)\n            self._send(buf)\n        else:\n            # For wire compatibility with 3.7 and lower\n            header = struct.pack(\"!i\", n)\n            if n > 16384:\n                # The payload is large so Nagle's algorithm won't be triggered\n                # and we'd better avoid the cost of concatenation.\n                self._send(header)\n                self._send(buf)\n            else:\n                # Issue #20540: concatenate before sending, to avoid delays due\n                # to Nagle's algorithm on a TCP socket.\n                # Also note we want to avoid sending a 0-length buffer separately,\n                # to avoid \"broken pipe\" errors if the other end closed the pipe.\n                self._send(header + buf)\n\n    def _recv_bytes(self, maxsize=None):\n        buf = self._recv(4)\n        size, = struct.unpack(\"!i\", buf.getvalue())\n        if size == -1:\n            buf = self._recv(8)\n            size, = struct.unpack(\"!Q\", buf.getvalue())\n        if maxsize is not None and size > maxsize:\n            return None\n        return self._recv(size)\n\n    def _poll(self, timeout):\n        r = wait([self], timeout)\n        return bool(r)\n\n\n#\n# Public functions\n#\n\nclass Listener(object):\n    '''\n    Returns a listener object.\n\n    This is a wrapper for a bound socket which is 'listening' for\n    connections, or for a Windows named pipe.\n    '''\n    def __init__(self, address=None, family=None, backlog=1, authkey=None):\n        family = family or (address and address_type(address)) \\\n                 or default_family\n        address = address or arbitrary_address(family)\n\n        _validate_family(family)\n        if family == 'AF_PIPE':\n            self._listener = PipeListener(address, backlog)\n        else:\n            self._listener = SocketListener(address, family, backlog)\n\n        if authkey is not None and not isinstance(authkey, bytes):\n            raise TypeError('authkey should be a byte string')\n\n        self._authkey = authkey\n\n    def accept(self):\n        '''\n        Accept a connection on the bound socket or named pipe of `self`.\n\n        Returns a `Connection` object.\n        '''\n        if self._listener is None:\n            raise OSError('listener is closed')\n\n        c = self._listener.accept()\n        if self._authkey is not None:\n            deliver_challenge(c, self._authkey)\n            answer_challenge(c, self._authkey)\n        return c\n\n    def close(self):\n        '''\n        Close the bound socket or named pipe of `self`.\n        '''\n        listener = self._listener\n        if listener is not None:\n            self._listener = None\n            listener.close()\n\n    @property\n    def address(self):\n        return self._listener._address\n\n    @property\n    def last_accepted(self):\n        return self._listener._last_accepted\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        self.close()\n\n\ndef Client(address, family=None, authkey=None):\n    '''\n    Returns a connection to the address of a `Listener`\n    '''\n    family = family or address_type(address)\n    _validate_family(family)\n    if family == 'AF_PIPE':\n        c = PipeClient(address)\n    else:\n        c = SocketClient(address)\n\n    if authkey is not None and not isinstance(authkey, bytes):\n        raise TypeError('authkey should be a byte string')\n\n    if authkey is not None:\n        answer_challenge(c, authkey)\n        deliver_challenge(c, authkey)\n\n    return c\n\n\nif sys.platform != 'win32':\n\n    def Pipe(duplex=True):\n        '''\n        Returns pair of connection objects at either end of a pipe\n        '''\n        if duplex:\n            s1, s2 = socket.socketpair()\n            s1.setblocking(True)\n            s2.setblocking(True)\n            c1 = Connection(s1.detach())\n            c2 = Connection(s2.detach())\n        else:\n            fd1, fd2 = os.pipe()\n            c1 = Connection(fd1, writable=False)\n            c2 = Connection(fd2, readable=False)\n\n        return c1, c2\n\nelse:\n\n    def Pipe(duplex=True):\n        '''\n        Returns pair of connection objects at either end of a pipe\n        '''\n        address = arbitrary_address('AF_PIPE')\n        if duplex:\n            openmode = _winapi.PIPE_ACCESS_DUPLEX\n            access = _winapi.GENERIC_READ | _winapi.GENERIC_WRITE\n            obsize, ibsize = BUFSIZE, BUFSIZE\n        else:\n            openmode = _winapi.PIPE_ACCESS_INBOUND\n            access = _winapi.GENERIC_WRITE\n            obsize, ibsize = 0, BUFSIZE\n\n        h1 = _winapi.CreateNamedPipe(\n            address, openmode | _winapi.FILE_FLAG_OVERLAPPED |\n            _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE,\n            _winapi.PIPE_TYPE_MESSAGE | _winapi.PIPE_READMODE_MESSAGE |\n            _winapi.PIPE_WAIT,\n            1, obsize, ibsize, _winapi.NMPWAIT_WAIT_FOREVER,\n            # default security descriptor: the handle cannot be inherited\n            _winapi.NULL\n            )\n        h2 = _winapi.CreateFile(\n            address, access, 0, _winapi.NULL, _winapi.OPEN_EXISTING,\n            _winapi.FILE_FLAG_OVERLAPPED, _winapi.NULL\n            )\n        _winapi.SetNamedPipeHandleState(\n            h2, _winapi.PIPE_READMODE_MESSAGE, None, None\n            )\n\n        overlapped = _winapi.ConnectNamedPipe(h1, overlapped=True)\n        _, err = overlapped.GetOverlappedResult(True)\n        assert err == 0\n\n        c1 = PipeConnection(h1, writable=duplex)\n        c2 = PipeConnection(h2, readable=duplex)\n\n        return c1, c2\n\n#\n# Definitions for connections based on sockets\n#\n\nclass SocketListener(object):\n    '''\n    Representation of a socket which is bound to an address and listening\n    '''\n    def __init__(self, address, family, backlog=1):\n        self._socket = socket.socket(getattr(socket, family))\n        try:\n            # SO_REUSEADDR has different semantics on Windows (issue #2550).\n            if os.name == 'posix':\n                self._socket.setsockopt(socket.SOL_SOCKET,\n                                        socket.SO_REUSEADDR, 1)\n            self._socket.setblocking(True)\n            self._socket.bind(address)\n            self._socket.listen(backlog)\n            self._address = self._socket.getsockname()\n        except OSError:\n            self._socket.close()\n            raise\n        self._family = family\n        self._last_accepted = None\n\n        if family == 'AF_UNIX' and not util.is_abstract_socket_namespace(address):\n            # Linux abstract socket namespaces do not need to be explicitly unlinked\n            self._unlink = util.Finalize(\n                self, os.unlink, args=(address,), exitpriority=0\n                )\n        else:\n            self._unlink = None\n\n    def accept(self):\n        s, self._last_accepted = self._socket.accept()\n        s.setblocking(True)\n        return Connection(s.detach())\n\n    def close(self):\n        try:\n            self._socket.close()\n        finally:\n            unlink = self._unlink\n            if unlink is not None:\n                self._unlink = None\n                unlink()\n\n\ndef SocketClient(address):\n    '''\n    Return a connection object connected to the socket given by `address`\n    '''\n    family = address_type(address)\n    with socket.socket( getattr(socket, family) ) as s:\n        s.setblocking(True)\n        s.connect(address)\n        return Connection(s.detach())\n\n#\n# Definitions for connections based on named pipes\n#\n\nif sys.platform == 'win32':\n\n    class PipeListener(object):\n        '''\n        Representation of a named pipe\n        '''\n        def __init__(self, address, backlog=None):\n            self._address = address\n            self._handle_queue = [self._new_handle(first=True)]\n\n            self._last_accepted = None\n            util.sub_debug('listener created with address=%r', self._address)\n            self.close = util.Finalize(\n                self, PipeListener._finalize_pipe_listener,\n                args=(self._handle_queue, self._address), exitpriority=0\n                )\n\n        def _new_handle(self, first=False):\n            flags = _winapi.PIPE_ACCESS_DUPLEX | _winapi.FILE_FLAG_OVERLAPPED\n            if first:\n                flags |= _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE\n            return _winapi.CreateNamedPipe(\n                self._address, flags,\n                _winapi.PIPE_TYPE_MESSAGE | _winapi.PIPE_READMODE_MESSAGE |\n                _winapi.PIPE_WAIT,\n                _winapi.PIPE_UNLIMITED_INSTANCES, BUFSIZE, BUFSIZE,\n                _winapi.NMPWAIT_WAIT_FOREVER, _winapi.NULL\n                )\n\n        def accept(self):\n            self._handle_queue.append(self._new_handle())\n            handle = self._handle_queue.pop(0)\n            try:\n                ov = _winapi.ConnectNamedPipe(handle, overlapped=True)\n            except OSError as e:\n                if e.winerror != _winapi.ERROR_NO_DATA:\n                    raise\n                # ERROR_NO_DATA can occur if a client has already connected,\n                # written data and then disconnected -- see Issue 14725.\n            else:\n                try:\n                    res = _winapi.WaitForMultipleObjects(\n                        [ov.event], False, INFINITE)\n                except:\n                    ov.cancel()\n                    _winapi.CloseHandle(handle)\n                    raise\n                finally:\n                    _, err = ov.GetOverlappedResult(True)\n                    assert err == 0\n            return PipeConnection(handle)\n\n        @staticmethod\n        def _finalize_pipe_listener(queue, address):\n            util.sub_debug('closing listener with address=%r', address)\n            for handle in queue:\n                _winapi.CloseHandle(handle)\n\n    def PipeClient(address):\n        '''\n        Return a connection object connected to the pipe given by `address`\n        '''\n        t = _init_timeout()\n        while 1:\n            try:\n                _winapi.WaitNamedPipe(address, 1000)\n                h = _winapi.CreateFile(\n                    address, _winapi.GENERIC_READ | _winapi.GENERIC_WRITE,\n                    0, _winapi.NULL, _winapi.OPEN_EXISTING,\n                    _winapi.FILE_FLAG_OVERLAPPED, _winapi.NULL\n                    )\n            except OSError as e:\n                if e.winerror not in (_winapi.ERROR_SEM_TIMEOUT,\n                                      _winapi.ERROR_PIPE_BUSY) or _check_timeout(t):\n                    raise\n            else:\n                break\n        else:\n            raise\n\n        _winapi.SetNamedPipeHandleState(\n            h, _winapi.PIPE_READMODE_MESSAGE, None, None\n            )\n        return PipeConnection(h)\n\n#\n# Authentication stuff\n#\n\nMESSAGE_LENGTH = 40  # MUST be > 20\n\n_CHALLENGE = b'#CHALLENGE#'\n_WELCOME = b'#WELCOME#'\n_FAILURE = b'#FAILURE#'\n\n# multiprocessing.connection Authentication Handshake Protocol Description\n# (as documented for reference after reading the existing code)\n# =============================================================================\n#\n# On Windows: native pipes with \"overlapped IO\" are used to send the bytes,\n# instead of the length prefix SIZE scheme described below. (ie: the OS deals\n# with message sizes for us)\n#\n# Protocol error behaviors:\n#\n# On POSIX, any failure to receive the length prefix into SIZE, for SIZE greater\n# than the requested maxsize to receive, or receiving fewer than SIZE bytes\n# results in the connection being closed and auth to fail.\n#\n# On Windows, receiving too few bytes is never a low level _recv_bytes read\n# error, receiving too many will trigger an error only if receive maxsize\n# value was larger than 128 OR the if the data arrived in smaller pieces.\n#\n#      Serving side                           Client side\n#     ------------------------------  ---------------------------------------\n# 0.                                  Open a connection on the pipe.\n# 1.  Accept connection.\n# 2.  Random 20+ bytes -> MESSAGE\n#     Modern servers always send\n#     more than 20 bytes and include\n#     a {digest} prefix on it with\n#     their preferred HMAC digest.\n#     Legacy ones send ==20 bytes.\n# 3.  send 4 byte length (net order)\n#     prefix followed by:\n#       b'#CHALLENGE#' + MESSAGE\n# 4.                                  Receive 4 bytes, parse as network byte\n#                                     order integer. If it is -1, receive an\n#                                     additional 8 bytes, parse that as network\n#                                     byte order. The result is the length of\n#                                     the data that follows -> SIZE.\n# 5.                                  Receive min(SIZE, 256) bytes -> M1\n# 6.                                  Assert that M1 starts with:\n#                                       b'#CHALLENGE#'\n# 7.                                  Strip that prefix from M1 into -> M2\n# 7.1.                                Parse M2: if it is exactly 20 bytes in\n#                                     length this indicates a legacy server\n#                                     supporting only HMAC-MD5. Otherwise the\n# 7.2.                                preferred digest is looked up from an\n#                                     expected \"{digest}\" prefix on M2. No prefix\n#                                     or unsupported digest? <- AuthenticationError\n# 7.3.                                Put divined algorithm name in -> D_NAME\n# 8.                                  Compute HMAC-D_NAME of AUTHKEY, M2 -> C_DIGEST\n# 9.                                  Send 4 byte length prefix (net order)\n#                                     followed by C_DIGEST bytes.\n# 10. Receive 4 or 4+8 byte length\n#     prefix (#4 dance) -> SIZE.\n# 11. Receive min(SIZE, 256) -> C_D.\n# 11.1. Parse C_D: legacy servers\n#     accept it as is, \"md5\" -> D_NAME\n# 11.2. modern servers check the length\n#     of C_D, IF it is 16 bytes?\n# 11.2.1. \"md5\" -> D_NAME\n#         and skip to step 12.\n# 11.3. longer? expect and parse a \"{digest}\"\n#     prefix into -> D_NAME.\n#     Strip the prefix and store remaining\n#     bytes in -> C_D.\n# 11.4. Don't like D_NAME? <- AuthenticationError\n# 12. Compute HMAC-D_NAME of AUTHKEY,\n#     MESSAGE into -> M_DIGEST.\n# 13. Compare M_DIGEST == C_D:\n# 14a: Match? Send length prefix &\n#       b'#WELCOME#'\n#    <- RETURN\n# 14b: Mismatch? Send len prefix &\n#       b'#FAILURE#'\n#    <- CLOSE & AuthenticationError\n# 15.                                 Receive 4 or 4+8 byte length prefix (net\n#                                     order) again as in #4 into -> SIZE.\n# 16.                                 Receive min(SIZE, 256) bytes -> M3.\n# 17.                                 Compare M3 == b'#WELCOME#':\n# 17a.                                Match? <- RETURN\n# 17b.                                Mismatch? <- CLOSE & AuthenticationError\n#\n# If this RETURNed, the connection remains open: it has been authenticated.\n#\n# Length prefixes are used consistently. Even on the legacy protocol, this\n# was good fortune and allowed us to evolve the protocol by using the length\n# of the opening challenge or length of the returned digest as a signal as\n# to which protocol the other end supports.\n\n_ALLOWED_DIGESTS = frozenset(\n        {b'md5', b'sha256', b'sha384', b'sha3_256', b'sha3_384'})\n_MAX_DIGEST_LEN = max(len(_) for _ in _ALLOWED_DIGESTS)\n\n# Old hmac-md5 only server versions from Python <=3.11 sent a message of this\n# length. It happens to not match the length of any supported digest so we can\n# use a message of this length to indicate that we should work in backwards\n# compatible md5-only mode without a {digest_name} prefix on our response.\n_MD5ONLY_MESSAGE_LENGTH = 20\n_MD5_DIGEST_LEN = 16\n_LEGACY_LENGTHS = (_MD5ONLY_MESSAGE_LENGTH, _MD5_DIGEST_LEN)\n\n\ndef _get_digest_name_and_payload(message: bytes) -> (str, bytes):\n    \"\"\"Returns a digest name and the payload for a response hash.\n\n    If a legacy protocol is detected based on the message length\n    or contents the digest name returned will be empty to indicate\n    legacy mode where MD5 and no digest prefix should be sent.\n    \"\"\"\n    # modern message format: b\"{digest}payload\" longer than 20 bytes\n    # legacy message format: 16 or 20 byte b\"payload\"\n    if len(message) in _LEGACY_LENGTHS:\n        # Either this was a legacy server challenge, or we're processing\n        # a reply from a legacy client that sent an unprefixed 16-byte\n        # HMAC-MD5 response. All messages using the modern protocol will\n        # be longer than either of these lengths.\n        return '', message\n    if (message.startswith(b'{') and\n        (curly := message.find(b'}', 1, _MAX_DIGEST_LEN+2)) > 0):\n        digest = message[1:curly]\n        if digest in _ALLOWED_DIGESTS:\n            payload = message[curly+1:]\n            return digest.decode('ascii'), payload\n    raise AuthenticationError(\n            'unsupported message length, missing digest prefix, '\n            f'or unsupported digest: {message=}')\n\n\ndef _create_response(authkey, message):\n    \"\"\"Create a MAC based on authkey and message\n\n    The MAC algorithm defaults to HMAC-MD5, unless MD5 is not available or\n    the message has a '{digest_name}' prefix. For legacy HMAC-MD5, the response\n    is the raw MAC, otherwise the response is prefixed with '{digest_name}',\n    e.g. b'{sha256}abcdefg...'\n\n    Note: The MAC protects the entire message including the digest_name prefix.\n    \"\"\"\n    import hmac\n    digest_name = _get_digest_name_and_payload(message)[0]\n    # The MAC protects the entire message: digest header and payload.\n    if not digest_name:\n        # Legacy server without a {digest} prefix on message.\n        # Generate a legacy non-prefixed HMAC-MD5 reply.\n        try:\n            return hmac.new(authkey, message, 'md5').digest()\n        except ValueError:\n            # HMAC-MD5 is not available (FIPS mode?), fall back to\n            # HMAC-SHA2-256 modern protocol. The legacy server probably\n            # doesn't support it and will reject us anyways. :shrug:\n            digest_name = 'sha256'\n    # Modern protocol, indicate the digest used in the reply.\n    response = hmac.new(authkey, message, digest_name).digest()\n    return b'{%s}%s' % (digest_name.encode('ascii'), response)\n\n\ndef _verify_challenge(authkey, message, response):\n    \"\"\"Verify MAC challenge\n\n    If our message did not include a digest_name prefix, the client is allowed\n    to select a stronger digest_name from _ALLOWED_DIGESTS.\n\n    In case our message is prefixed, a client cannot downgrade to a weaker\n    algorithm, because the MAC is calculated over the entire message\n    including the '{digest_name}' prefix.\n    \"\"\"\n    import hmac\n    response_digest, response_mac = _get_digest_name_and_payload(response)\n    response_digest = response_digest or 'md5'\n    try:\n        expected = hmac.new(authkey, message, response_digest).digest()\n    except ValueError:\n        raise AuthenticationError(f'{response_digest=} unsupported')\n    if len(expected) != len(response_mac):\n        raise AuthenticationError(\n                f'expected {response_digest!r} of length {len(expected)} '\n                f'got {len(response_mac)}')\n    if not hmac.compare_digest(expected, response_mac):\n        raise AuthenticationError('digest received was wrong')\n\n\ndef deliver_challenge(connection, authkey: bytes, digest_name='sha256'):\n    if not isinstance(authkey, bytes):\n        raise ValueError(\n            \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n    assert MESSAGE_LENGTH > _MD5ONLY_MESSAGE_LENGTH, \"protocol constraint\"\n    message = os.urandom(MESSAGE_LENGTH)\n    message = b'{%s}%s' % (digest_name.encode('ascii'), message)\n    # Even when sending a challenge to a legacy client that does not support\n    # digest prefixes, they'll take the entire thing as a challenge and\n    # respond to it with a raw HMAC-MD5.\n    connection.send_bytes(_CHALLENGE + message)\n    response = connection.recv_bytes(256)        # reject large message\n    try:\n        _verify_challenge(authkey, message, response)\n    except AuthenticationError:\n        connection.send_bytes(_FAILURE)\n        raise\n    else:\n        connection.send_bytes(_WELCOME)\n\n\ndef answer_challenge(connection, authkey: bytes):\n    if not isinstance(authkey, bytes):\n        raise ValueError(\n            \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n    message = connection.recv_bytes(256)         # reject large message\n    if not message.startswith(_CHALLENGE):\n        raise AuthenticationError(\n                f'Protocol error, expected challenge: {message=}')\n    message = message[len(_CHALLENGE):]\n    if len(message) < _MD5ONLY_MESSAGE_LENGTH:\n        raise AuthenticationError('challenge too short: {len(message)} bytes')\n    digest = _create_response(authkey, message)\n    connection.send_bytes(digest)\n    response = connection.recv_bytes(256)        # reject large message\n    if response != _WELCOME:\n        raise AuthenticationError('digest sent was rejected')\n\n#\n# Support for using xmlrpclib for serialization\n#\n\nclass ConnectionWrapper(object):\n    def __init__(self, conn, dumps, loads):\n        self._conn = conn\n        self._dumps = dumps\n        self._loads = loads\n        for attr in ('fileno', 'close', 'poll', 'recv_bytes', 'send_bytes'):\n            obj = getattr(conn, attr)\n            setattr(self, attr, obj)\n    def send(self, obj):\n        s = self._dumps(obj)\n        self._conn.send_bytes(s)\n    def recv(self):\n        s = self._conn.recv_bytes()\n        return self._loads(s)\n\ndef _xml_dumps(obj):\n    return xmlrpclib.dumps((obj,), None, None, None, 1).encode('utf-8')\n\ndef _xml_loads(s):\n    (obj,), method = xmlrpclib.loads(s.decode('utf-8'))\n    return obj\n\nclass XmlListener(Listener):\n    def accept(self):\n        global xmlrpclib\n        import xmlrpc.client as xmlrpclib\n        obj = Listener.accept(self)\n        return ConnectionWrapper(obj, _xml_dumps, _xml_loads)\n\ndef XmlClient(*args, **kwds):\n    global xmlrpclib\n    import xmlrpc.client as xmlrpclib\n    return ConnectionWrapper(Client(*args, **kwds), _xml_dumps, _xml_loads)\n\n#\n# Wait\n#\n\nif sys.platform == 'win32':\n\n    def _exhaustive_wait(handles, timeout):\n        # Return ALL handles which are currently signalled.  (Only\n        # returning the first signalled might create starvation issues.)\n        L = list(handles)\n        ready = []\n        # Windows limits WaitForMultipleObjects at 64 handles, and we use a\n        # few for synchronisation, so we switch to batched waits at 60.\n        if len(L) > 60:\n            try:\n                res = _winapi.BatchedWaitForMultipleObjects(L, False, timeout)\n            except TimeoutError:\n                return []\n            ready.extend(L[i] for i in res)\n            if res:\n                L = [h for i, h in enumerate(L) if i > res[0] & i not in res]\n            timeout = 0\n        while L:\n            short_L = L[:60] if len(L) > 60 else L\n            res = _winapi.WaitForMultipleObjects(short_L, False, timeout)\n            if res == WAIT_TIMEOUT:\n                break\n            elif WAIT_OBJECT_0 <= res < WAIT_OBJECT_0 + len(L):\n                res -= WAIT_OBJECT_0\n            elif WAIT_ABANDONED_0 <= res < WAIT_ABANDONED_0 + len(L):\n                res -= WAIT_ABANDONED_0\n            else:\n                raise RuntimeError('Should not get here')\n            ready.append(L[res])\n            L = L[res+1:]\n            timeout = 0\n        return ready\n\n    _ready_errors = {_winapi.ERROR_BROKEN_PIPE, _winapi.ERROR_NETNAME_DELETED}\n\n    def wait(object_list, timeout=None):\n        '''\n        Wait till an object in object_list is ready/readable.\n\n        Returns list of those objects in object_list which are ready/readable.\n        '''\n        if timeout is None:\n            timeout = INFINITE\n        elif timeout < 0:\n            timeout = 0\n        else:\n            timeout = int(timeout * 1000 + 0.5)\n\n        object_list = list(object_list)\n        waithandle_to_obj = {}\n        ov_list = []\n        ready_objects = set()\n        ready_handles = set()\n\n        try:\n            for o in object_list:\n                try:\n                    fileno = getattr(o, 'fileno')\n                except AttributeError:\n                    waithandle_to_obj[o.__index__()] = o\n                else:\n                    # start an overlapped read of length zero\n                    try:\n                        ov, err = _winapi.ReadFile(fileno(), 0, True)\n                    except OSError as e:\n                        ov, err = None, e.winerror\n                        if err not in _ready_errors:\n                            raise\n                    if err == _winapi.ERROR_IO_PENDING:\n                        ov_list.append(ov)\n                        waithandle_to_obj[ov.event] = o\n                    else:\n                        # If o.fileno() is an overlapped pipe handle and\n                        # err == 0 then there is a zero length message\n                        # in the pipe, but it HAS NOT been consumed...\n                        if ov and sys.getwindowsversion()[:2] >= (6, 2):\n                            # ... except on Windows 8 and later, where\n                            # the message HAS been consumed.\n                            try:\n                                _, err = ov.GetOverlappedResult(False)\n                            except OSError as e:\n                                err = e.winerror\n                            if not err and hasattr(o, '_got_empty_message'):\n                                o._got_empty_message = True\n                        ready_objects.add(o)\n                        timeout = 0\n\n            ready_handles = _exhaustive_wait(waithandle_to_obj.keys(), timeout)\n        finally:\n            # request that overlapped reads stop\n            for ov in ov_list:\n                ov.cancel()\n\n            # wait for all overlapped reads to stop\n            for ov in ov_list:\n                try:\n                    _, err = ov.GetOverlappedResult(True)\n                except OSError as e:\n                    err = e.winerror\n                    if err not in _ready_errors:\n                        raise\n                if err != _winapi.ERROR_OPERATION_ABORTED:\n                    o = waithandle_to_obj[ov.event]\n                    ready_objects.add(o)\n                    if err == 0:\n                        # If o.fileno() is an overlapped pipe handle then\n                        # a zero length message HAS been consumed.\n                        if hasattr(o, '_got_empty_message'):\n                            o._got_empty_message = True\n\n        ready_objects.update(waithandle_to_obj[h] for h in ready_handles)\n        return [o for o in object_list if o in ready_objects]\n\nelse:\n\n    import selectors\n\n    # poll/select have the advantage of not requiring any extra file\n    # descriptor, contrarily to epoll/kqueue (also, they require a single\n    # syscall).\n    if hasattr(selectors, 'PollSelector'):\n        _WaitSelector = selectors.PollSelector\n    else:\n        _WaitSelector = selectors.SelectSelector\n\n    def wait(object_list, timeout=None):\n        '''\n        Wait till an object in object_list is ready/readable.\n\n        Returns list of those objects in object_list which are ready/readable.\n        '''\n        with _WaitSelector() as selector:\n            for obj in object_list:\n                selector.register(obj, selectors.EVENT_READ)\n\n            if timeout is not None:\n                deadline = time.monotonic() + timeout\n\n            while True:\n                ready = selector.select(timeout)\n                if ready:\n                    return [key.fileobj for (key, events) in ready]\n                else:\n                    if timeout is not None:\n                        timeout = deadline - time.monotonic()\n                        if timeout < 0:\n                            return ready\n\n#\n# Make connection and socket objects shareable if possible\n#\n\nif sys.platform == 'win32':\n    def reduce_connection(conn):\n        handle = conn.fileno()\n        with socket.fromfd(handle, socket.AF_INET, socket.SOCK_STREAM) as s:\n            from . import resource_sharer\n            ds = resource_sharer.DupSocket(s)\n            return rebuild_connection, (ds, conn.readable, conn.writable)\n    def rebuild_connection(ds, readable, writable):\n        sock = ds.detach()\n        return Connection(sock.detach(), readable, writable)\n    reduction.register(Connection, reduce_connection)\n\n    def reduce_pipe_connection(conn):\n        access = ((_winapi.FILE_GENERIC_READ if conn.readable else 0) |\n                  (_winapi.FILE_GENERIC_WRITE if conn.writable else 0))\n        dh = reduction.DupHandle(conn.fileno(), access)\n        return rebuild_pipe_connection, (dh, conn.readable, conn.writable)\n    def rebuild_pipe_connection(dh, readable, writable):\n        handle = dh.detach()\n        return PipeConnection(handle, readable, writable)\n    reduction.register(PipeConnection, reduce_pipe_connection)\n\nelse:\n    def reduce_connection(conn):\n        df = reduction.DupFd(conn.fileno())\n        return rebuild_connection, (df, conn.readable, conn.writable)\n    def rebuild_connection(df, readable, writable):\n        fd = df.detach()\n        return Connection(fd, readable, writable)\n    reduction.register(Connection, reduce_connection)\n", 1190], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py": ["#\n# Module which deals with pickling of objects.\n#\n# multiprocessing/reduction.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\nfrom abc import ABCMeta\nimport copyreg\nimport functools\nimport io\nimport os\nimport pickle\nimport socket\nimport sys\n\nfrom . import context\n\n__all__ = ['send_handle', 'recv_handle', 'ForkingPickler', 'register', 'dump']\n\n\nHAVE_SEND_HANDLE = (sys.platform == 'win32' or\n                    (hasattr(socket, 'CMSG_LEN') and\n                     hasattr(socket, 'SCM_RIGHTS') and\n                     hasattr(socket.socket, 'sendmsg')))\n\n#\n# Pickler subclass\n#\n\nclass ForkingPickler(pickle.Pickler):\n    '''Pickler subclass used by multiprocessing.'''\n    _extra_reducers = {}\n    _copyreg_dispatch_table = copyreg.dispatch_table\n\n    def __init__(self, *args):\n        super().__init__(*args)\n        self.dispatch_table = self._copyreg_dispatch_table.copy()\n        self.dispatch_table.update(self._extra_reducers)\n\n    @classmethod\n    def register(cls, type, reduce):\n        '''Register a reduce function for a type.'''\n        cls._extra_reducers[type] = reduce\n\n    @classmethod\n    def dumps(cls, obj, protocol=None):\n        buf = io.BytesIO()\n        cls(buf, protocol).dump(obj)\n        return buf.getbuffer()\n\n    loads = pickle.loads\n\nregister = ForkingPickler.register\n\ndef dump(obj, file, protocol=None):\n    '''Replacement for pickle.dump() using ForkingPickler.'''\n    ForkingPickler(file, protocol).dump(obj)\n\n#\n# Platform specific definitions\n#\n\nif sys.platform == 'win32':\n    # Windows\n    __all__ += ['DupHandle', 'duplicate', 'steal_handle']\n    import _winapi\n\n    def duplicate(handle, target_process=None, inheritable=False,\n                  *, source_process=None):\n        '''Duplicate a handle.  (target_process is a handle not a pid!)'''\n        current_process = _winapi.GetCurrentProcess()\n        if source_process is None:\n            source_process = current_process\n        if target_process is None:\n            target_process = current_process\n        return _winapi.DuplicateHandle(\n            source_process, handle, target_process,\n            0, inheritable, _winapi.DUPLICATE_SAME_ACCESS)\n\n    def steal_handle(source_pid, handle):\n        '''Steal a handle from process identified by source_pid.'''\n        source_process_handle = _winapi.OpenProcess(\n            _winapi.PROCESS_DUP_HANDLE, False, source_pid)\n        try:\n            return _winapi.DuplicateHandle(\n                source_process_handle, handle,\n                _winapi.GetCurrentProcess(), 0, False,\n                _winapi.DUPLICATE_SAME_ACCESS | _winapi.DUPLICATE_CLOSE_SOURCE)\n        finally:\n            _winapi.CloseHandle(source_process_handle)\n\n    def send_handle(conn, handle, destination_pid):\n        '''Send a handle over a local connection.'''\n        dh = DupHandle(handle, _winapi.DUPLICATE_SAME_ACCESS, destination_pid)\n        conn.send(dh)\n\n    def recv_handle(conn):\n        '''Receive a handle over a local connection.'''\n        return conn.recv().detach()\n\n    class DupHandle(object):\n        '''Picklable wrapper for a handle.'''\n        def __init__(self, handle, access, pid=None):\n            if pid is None:\n                # We just duplicate the handle in the current process and\n                # let the receiving process steal the handle.\n                pid = os.getpid()\n            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False, pid)\n            try:\n                self._handle = _winapi.DuplicateHandle(\n                    _winapi.GetCurrentProcess(),\n                    handle, proc, access, False, 0)\n            finally:\n                _winapi.CloseHandle(proc)\n            self._access = access\n            self._pid = pid\n\n        def detach(self):\n            '''Get the handle.  This should only be called once.'''\n            # retrieve handle from process which currently owns it\n            if self._pid == os.getpid():\n                # The handle has already been duplicated for this process.\n                return self._handle\n            # We must steal the handle from the process whose pid is self._pid.\n            proc = _winapi.OpenProcess(_winapi.PROCESS_DUP_HANDLE, False,\n                                       self._pid)\n            try:\n                return _winapi.DuplicateHandle(\n                    proc, self._handle, _winapi.GetCurrentProcess(),\n                    self._access, False, _winapi.DUPLICATE_CLOSE_SOURCE)\n            finally:\n                _winapi.CloseHandle(proc)\n\nelse:\n    # Unix\n    __all__ += ['DupFd', 'sendfds', 'recvfds']\n    import array\n\n    # On MacOSX we should acknowledge receipt of fds -- see Issue14669\n    ACKNOWLEDGE = sys.platform == 'darwin'\n\n    def sendfds(sock, fds):\n        '''Send an array of fds over an AF_UNIX socket.'''\n        fds = array.array('i', fds)\n        msg = bytes([len(fds) % 256])\n        sock.sendmsg([msg], [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fds)])\n        if ACKNOWLEDGE and sock.recv(1) != b'A':\n            raise RuntimeError('did not receive acknowledgement of fd')\n\n    def recvfds(sock, size):\n        '''Receive an array of fds over an AF_UNIX socket.'''\n        a = array.array('i')\n        bytes_size = a.itemsize * size\n        msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_SPACE(bytes_size))\n        if not msg and not ancdata:\n            raise EOFError\n        try:\n            if ACKNOWLEDGE:\n                sock.send(b'A')\n            if len(ancdata) != 1:\n                raise RuntimeError('received %d items of ancdata' %\n                                   len(ancdata))\n            cmsg_level, cmsg_type, cmsg_data = ancdata[0]\n            if (cmsg_level == socket.SOL_SOCKET and\n                cmsg_type == socket.SCM_RIGHTS):\n                if len(cmsg_data) % a.itemsize != 0:\n                    raise ValueError\n                a.frombytes(cmsg_data)\n                if len(a) % 256 != msg[0]:\n                    raise AssertionError(\n                        \"Len is {0:n} but msg[0] is {1!r}\".format(\n                            len(a), msg[0]))\n                return list(a)\n        except (ValueError, IndexError):\n            pass\n        raise RuntimeError('Invalid data received')\n\n    def send_handle(conn, handle, destination_pid):\n        '''Send a handle over a local connection.'''\n        with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:\n            sendfds(s, [handle])\n\n    def recv_handle(conn):\n        '''Receive a handle over a local connection.'''\n        with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:\n            return recvfds(s, 1)[0]\n\n    def DupFd(fd):\n        '''Return a wrapper for an fd.'''\n        popen_obj = context.get_spawning_popen()\n        if popen_obj is not None:\n            return popen_obj.DupFd(popen_obj.duplicate_for_child(fd))\n        elif HAVE_SEND_HANDLE:\n            from . import resource_sharer\n            return resource_sharer.DupFd(fd)\n        else:\n            raise ValueError('SCM_RIGHTS appears not to be available')\n\n#\n# Try making some callable types picklable\n#\n\ndef _reduce_method(m):\n    if m.__self__ is None:\n        return getattr, (m.__class__, m.__func__.__name__)\n    else:\n        return getattr, (m.__self__, m.__func__.__name__)\nclass _C:\n    def f(self):\n        pass\nregister(type(_C().f), _reduce_method)\n\n\ndef _reduce_method_descriptor(m):\n    return getattr, (m.__objclass__, m.__name__)\nregister(type(list.append), _reduce_method_descriptor)\nregister(type(int.__add__), _reduce_method_descriptor)\n\n\ndef _reduce_partial(p):\n    return _rebuild_partial, (p.func, p.args, p.keywords or {})\ndef _rebuild_partial(func, args, keywords):\n    return functools.partial(func, *args, **keywords)\nregister(functools.partial, _reduce_partial)\n\n#\n# Make sockets picklable\n#\n\nif sys.platform == 'win32':\n    def _reduce_socket(s):\n        from .resource_sharer import DupSocket\n        return _rebuild_socket, (DupSocket(s),)\n    def _rebuild_socket(ds):\n        return ds.detach()\n    register(socket.socket, _reduce_socket)\n\nelse:\n    def _reduce_socket(s):\n        df = DupFd(s.fileno())\n        return _rebuild_socket, (df, s.family, s.type, s.proto)\n    def _rebuild_socket(df, family, type, proto):\n        fd = df.detach()\n        return socket.socket(family, type, proto, fileno=fd)\n    register(socket.socket, _reduce_socket)\n\n\nclass AbstractReducer(metaclass=ABCMeta):\n    '''Abstract base class for use in implementing a Reduction class\n    suitable for use in replacing the standard reduction mechanism\n    used in multiprocessing.'''\n    ForkingPickler = ForkingPickler\n    register = register\n    dump = dump\n    send_handle = send_handle\n    recv_handle = recv_handle\n\n    if sys.platform == 'win32':\n        steal_handle = steal_handle\n        duplicate = duplicate\n        DupHandle = DupHandle\n    else:\n        sendfds = sendfds\n        recvfds = recvfds\n        DupFd = DupFd\n\n    _reduce_method = _reduce_method\n    _reduce_method_descriptor = _reduce_method_descriptor\n    _rebuild_partial = _rebuild_partial\n    _reduce_socket = _reduce_socket\n    _rebuild_socket = _rebuild_socket\n\n    def __init__(self, *args):\n        register(type(_C().f), _reduce_method)\n        register(type(list.append), _reduce_method_descriptor)\n        register(type(int.__add__), _reduce_method_descriptor)\n        register(functools.partial, _reduce_partial)\n        register(socket.socket, _reduce_socket)\n", 281], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/streams.py": ["# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport contextlib\nimport io\nimport os\nimport sys\nimport threading\nfrom collections import deque\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Optional,\n    Protocol,\n)\n\nfrom marimo import _loggers\nfrom marimo._messaging.cell_output import CellChannel\nfrom marimo._messaging.console_output_worker import ConsoleMsg, buffered_writer\nfrom marimo._messaging.mimetypes import ConsoleMimeType\nfrom marimo._messaging.types import (\n    KernelMessage,\n    Stderr,\n    Stdin,\n    Stdout,\n    Stream,\n)\nfrom marimo._server.types import QueueType\nfrom marimo._types.ids import CellId_t\n\nif TYPE_CHECKING:\n    import queue\n    from collections.abc import Iterable, Iterator\n\nLOGGER = _loggers.marimo_logger()\n\n\n# Byte limits on outputs exist for two reasons\n#\n# 1. We use a multiprocessing.Connection object to send outputs from\n#    the kernel to the server (the server then sends the output to\n#    the frontend via a websocket). The Connection object has a limit\n#    of ~32MiB that it can send before it chokes\n#    (https://docs.python.org/3/library/multiprocessing.html#multiprocessing.connection.Connection.send).\n#\n#    TODO(akshayka): Get around this by breaking up the message sent\n#    over the Connection or plumbing the websocket into the kernel.\n#\n# 2. The frontend chokes when we send outputs that are too big, i.e.\n#    it freezes and sometimes even crashes. That can lead to lost work.\n#    It appears this is the bottleneck right now, compared to 1.\n#\n# Usually users only output gigantic things accidentally, so refusing\n# to show large outputs should in most cases not bother the user too much.\n# In any case, it's better than breaking the frontend/kernel.\n\n\ndef output_max_bytes() -> int:\n    from marimo._runtime.context import ContextNotInitializedError, get_context\n\n    try:\n        return get_context().marimo_config[\"runtime\"][\"output_max_bytes\"]\n    except ContextNotInitializedError:\n        return 5_000_000\n\n\ndef std_stream_max_bytes() -> int:\n    from marimo._runtime.context import ContextNotInitializedError, get_context\n\n    try:\n        return get_context().marimo_config[\"runtime\"][\"std_stream_max_bytes\"]\n    except ContextNotInitializedError:\n        return 1_000_000\n\n\nclass PipeProtocol(Protocol):\n    def send(self, obj: Any) -> None:\n        pass\n\n\nclass QueuePipe:\n    def __init__(self, queue: queue.Queue[KernelMessage]):\n        self._queue = queue\n\n    def send(self, obj: Any) -> None:\n        self._queue.put_nowait(obj)\n\n\nclass ThreadSafeStream(Stream):\n    \"\"\"A thread-safe wrapper around a pipe.\n\n    Does not own the pipe or queue.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipe: PipeProtocol,\n        input_queue: QueueType[str],\n        redirect_console: bool,\n        cell_id: Optional[CellId_t] = None,\n    ):\n        self.pipe = pipe\n        self.cell_id = cell_id\n        self.redirect_console = redirect_console\n        # A single stream is shared by the kernel and the code completion\n        # worker. The lock should almost always be uncontended.\n        self.stream_lock = threading.Lock()\n\n        if self.redirect_console:\n            # Console outputs are buffered\n            self.console_msg_cv = threading.Condition(threading.Lock())\n            self.console_msg_queue: deque[ConsoleMsg | None] = deque()\n            self.buffered_console_thread = threading.Thread(\n                target=buffered_writer,\n                args=(self.console_msg_queue, self, self.console_msg_cv),\n            )\n            self.buffered_console_thread.start()\n\n        # stdin messages are pulled from this queue\n        self.input_queue = input_queue\n\n    def write(self, op: str, data: dict[Any, Any]) -> None:\n        with self.stream_lock:\n            try:\n                self.pipe.send((op, data))\n            except OSError as e:\n                # Most likely a BrokenPipeError, caused by the\n                # server process shutting down\n                LOGGER.debug(\"Error when writing (op: %s) to pipe: %s\", op, e)\n\n    def stop(self) -> None:\n        \"\"\"Teardown resources created by the stream.\"\"\"\n        # Sending `None` through the queue signals the console thread to exit.\n        # We don't join the thread in case its processing outputs still; don't\n        # want to block the entire program.\n        if self.redirect_console:\n            self.console_msg_queue.append(None)\n            with self.console_msg_cv:\n                self.console_msg_cv.notify()\n\n\ndef _forward_os_stream(\n    standard_stream: Stdout | Stderr, fd: int, should_exit: threading.Event\n) -> None:\n    \"\"\"Watch a file descriptor and forward it to a stream object.\"\"\"\n\n    # This coarse try/except block silences exceptions; a raised exception\n    # at this point could cause bad errors, such as an infinite stream of data\n    # to be written to the fd/routed through the stream.\n    #\n    # TODO(akshayka): Make this loop bomb-proof, so that exceptions raised are\n    # exceptions we actually want to pay attention to; then store the exception\n    # and print it to the terminal later (outside an execution context).\n    try:\n        while not should_exit.is_set():\n            data = os.read(fd, 1024)\n            if not data:\n                break\n            standard_stream.write(data.decode())\n    except Exception:\n        ...\n\n\nclass Watcher:\n    \"\"\"Watches and redirects a standard stream.\"\"\"\n\n    def __init__(\n        self, standard_stream: ThreadSafeStdout | ThreadSafeStderr\n    ) -> None:\n        self.standard_stream = standard_stream\n        self.fd = self.standard_stream._original_fd\n        self.read_fd, self.write_fd = os.pipe()\n        self._should_exit = threading.Event()\n        self.thread = threading.Thread(\n            target=_forward_os_stream,\n            args=(self.standard_stream, self.read_fd, self._should_exit),\n            daemon=True,\n        )\n        self.thread.start()\n\n    def start(self) -> None:\n        # Save the file for the standard stream by opening a new file\n        # descriptor for it\n        self.fd_dup = os.dup(self.fd)\n        self.standard_stream._set_fileno(self.fd_dup)\n        # Change the original file descriptor for the standard stream\n        # to refer to the write end of the pipe\n        os.dup2(self.write_fd, self.fd)\n\n    def pause(self) -> None:\n        # Restore the original file descriptor to point to the standard\n        # stream file\n        os.dup2(self.fd_dup, self.fd)\n        os.close(self.fd_dup)\n        self.standard_stream._set_fileno(None)\n\n    def stop(self) -> None:\n        os.close(self.write_fd)\n        os.close(self.read_fd)\n        self._should_exit.set()\n\n\n# NB: Python doesn't provide a standard out class to inherit from, so\n# we inherit from TextIOBase.\nclass ThreadSafeStdout(Stdout):\n    encoding = sys.stdout.encoding\n    errors = sys.stdout.errors\n    _fileno: int | None = None\n\n    def __init__(self, stream: ThreadSafeStream):\n        self._stream = stream\n        self._original_fd = sys.stdout.fileno()\n        self._watcher = Watcher(self)\n\n    def stop(self) -> None:\n        self._watcher.stop()\n\n    def fileno(self) -> int:\n        if self._fileno is not None:\n            return self._fileno\n        raise io.UnsupportedOperation(\"Stream not redirected, no fileno.\")\n\n    def _set_fileno(self, fileno: int | None) -> None:\n        self._fileno = fileno\n\n    def writable(self) -> bool:\n        return True\n\n    def readable(self) -> bool:\n        return False\n\n    def seekable(self) -> bool:\n        return False\n\n    def flush(self) -> None:\n        # TODO(akshayka): maybe force the buffered writer to write\n        return\n\n    def _write_with_mimetype(\n        self, data: str, mimetype: ConsoleMimeType\n    ) -> int:\n        assert self._stream.cell_id is not None\n        if not isinstance(data, str):\n            raise TypeError(\n                f\"write() argument must be a str, not {type(data).__name__}\"\n            )\n        max_bytes = std_stream_max_bytes()\n        if sys.getsizeof(data) > max_bytes:\n            sys.stderr.write(\n                \"Warning: marimo truncated a very large console output.\\n\"\n            )\n            data = data[: int(max_bytes)] + \" ... \"\n        self._stream.console_msg_queue.append(\n            ConsoleMsg(\n                stream=CellChannel.STDOUT,\n                cell_id=self._stream.cell_id,\n                data=data,\n                mimetype=mimetype,\n            )\n        )\n        with self._stream.console_msg_cv:\n            self._stream.console_msg_cv.notify()\n        return len(data)\n\n    # Buffer type not available python < 3.12, hence type ignore\n    def writelines(self, sequence: Iterable[str]) -> None:  # type: ignore[override] # noqa: E501\n        for line in sequence:\n            self.write(line)\n\n\nclass ThreadSafeStderr(Stderr):\n    encoding = sys.stderr.encoding\n    errors = sys.stderr.errors\n    _fileno: int | None = None\n\n    def __init__(self, stream: ThreadSafeStream):\n        self._stream = stream\n        self._original_fd = sys.stderr.fileno()\n        self._watcher = Watcher(self)\n\n    def stop(self) -> None:\n        self._watcher.stop()\n\n    def fileno(self) -> int:\n        if self._fileno is not None:\n            return self._fileno\n        raise io.UnsupportedOperation(\"Stream not redirected, no fileno.\")\n\n    def _set_fileno(self, fileno: int | None) -> None:\n        self._fileno = fileno\n\n    def writable(self) -> bool:\n        return True\n\n    def readable(self) -> bool:\n        return False\n\n    def seekable(self) -> bool:\n        return False\n\n    def flush(self) -> None:\n        # TODO(akshayka): maybe force the buffered writer to write\n        return\n\n    def _write_with_mimetype(\n        self, data: str, mimetype: ConsoleMimeType\n    ) -> int:\n        assert self._stream.cell_id is not None\n        if not isinstance(data, str):\n            raise TypeError(\n                f\"write() argument must be a str, not {type(data).__name__}\"\n            )\n        max_bytes = std_stream_max_bytes()\n        if sys.getsizeof(data) > max_bytes:\n            data = (\n                \"Warning: marimo truncated a very large console output.\\n\"\n                + data[: int(max_bytes)]\n                + \" ... \"\n            )\n\n        with self._stream.console_msg_cv:\n            self._stream.console_msg_queue.append(\n                ConsoleMsg(\n                    stream=CellChannel.STDERR,\n                    cell_id=self._stream.cell_id,\n                    data=data,\n                    mimetype=mimetype,\n                )\n            )\n            self._stream.console_msg_cv.notify()\n        return len(data)\n\n    def writelines(self, sequence: Iterable[str]) -> None:  # type: ignore[override] # noqa: E501\n        for line in sequence:\n            self.write(line)\n\n\nclass ThreadSafeStdin(Stdin):\n    \"\"\"Implements a subset of stdin.\"\"\"\n\n    encoding = sys.stdin.encoding\n    errors = sys.stdin.errors\n\n    def __init__(self, stream: ThreadSafeStream):\n        self._stream = stream\n\n    def fileno(self) -> int:\n        raise io.UnsupportedOperation(\n            \"marimo's stdin is a pseudofile, which has no fileno.\"\n        )\n\n    def writable(self) -> bool:\n        return False\n\n    def readable(self) -> bool:\n        return True\n\n    def _readline_with_prompt(self, prompt: str = \"\") -> str:\n        \"\"\"Read input from the standard in stream, with an optional prompt.\"\"\"\n        assert self._stream.cell_id is not None\n        if not isinstance(prompt, str):\n            raise TypeError(\n                f\"prompt must be a str, not {type(prompt).__name__}\"\n            )\n\n        max_bytes = std_stream_max_bytes()\n        if sys.getsizeof(prompt) > max_bytes:\n            prompt = (\n                \"Warning: marimo truncated a very large console output.\\n\"\n                + prompt[: int(max_bytes)]\n                + \" ... \"\n            )\n\n        with self._stream.console_msg_cv:\n            # This sends a prompt request to the frontend.\n            self._stream.console_msg_queue.append(\n                ConsoleMsg(\n                    stream=CellChannel.STDIN,\n                    cell_id=self._stream.cell_id,\n                    data=prompt,\n                    mimetype=\"text/plain\",\n                )\n            )\n            self._stream.console_msg_cv.notify()\n\n        return self._stream.input_queue.get()\n\n    def readline(self, size: int | None = -1) -> str:  # type: ignore[override]  # noqa: E501\n        # size only included for compatibility with sys.stdin.readline API;\n        # we don't support it.\n        del size\n        return self._readline_with_prompt(prompt=\"\")\n\n    def readlines(self, hint: int | None = -1) -> list[str]:  # type: ignore[override]  # noqa: E501\n        # Just an alias for readline.\n        #\n        # hint only included for compatibility with sys.stdin.readlines API;\n        # we don't support it.\n        del hint\n        return self._readline_with_prompt(prompt=\"\").split(\"\\n\")\n\n\n@contextlib.contextmanager\ndef redirect(standard_stream: Stdout | Stderr) -> Iterator[None]:\n    \"\"\"Redirect a standard stream to the frontend.\"\"\"\n    try:\n        if isinstance(standard_stream, ThreadSafeStdout) or isinstance(\n            standard_stream, ThreadSafeStderr\n        ):\n            standard_stream._watcher.start()\n        yield\n    finally:\n        if isinstance(standard_stream, ThreadSafeStdout) or isinstance(\n            standard_stream, ThreadSafeStderr\n        ):\n            standard_stream._watcher.pause()\n", 416], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py": ["\"\"\"Selectors module.\n\nThis module allows high-level and efficient I/O multiplexing, built upon the\n`select` module primitives.\n\"\"\"\n\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nfrom collections.abc import Mapping\nimport math\nimport select\nimport sys\n\n\n# generic events, that must be mapped to implementation-specific ones\nEVENT_READ = (1 << 0)\nEVENT_WRITE = (1 << 1)\n\n\ndef _fileobj_to_fd(fileobj):\n    \"\"\"Return a file descriptor from a file object.\n\n    Parameters:\n    fileobj -- file object or file descriptor\n\n    Returns:\n    corresponding file descriptor\n\n    Raises:\n    ValueError if the object is invalid\n    \"\"\"\n    if isinstance(fileobj, int):\n        fd = fileobj\n    else:\n        try:\n            fd = int(fileobj.fileno())\n        except (AttributeError, TypeError, ValueError):\n            raise ValueError(\"Invalid file object: \"\n                             \"{!r}\".format(fileobj)) from None\n    if fd < 0:\n        raise ValueError(\"Invalid file descriptor: {}\".format(fd))\n    return fd\n\n\nSelectorKey = namedtuple('SelectorKey', ['fileobj', 'fd', 'events', 'data'])\n\nSelectorKey.__doc__ = \"\"\"SelectorKey(fileobj, fd, events, data)\n\n    Object used to associate a file object to its backing\n    file descriptor, selected event mask, and attached data.\n\"\"\"\nSelectorKey.fileobj.__doc__ = 'File object registered.'\nSelectorKey.fd.__doc__ = 'Underlying file descriptor.'\nSelectorKey.events.__doc__ = 'Events that must be waited for on this file object.'\nSelectorKey.data.__doc__ = ('''Optional opaque data associated to this file object.\nFor example, this could be used to store a per-client session ID.''')\n\n\nclass _SelectorMapping(Mapping):\n    \"\"\"Mapping of file objects to selector keys.\"\"\"\n\n    def __init__(self, selector):\n        self._selector = selector\n\n    def __len__(self):\n        return len(self._selector._fd_to_key)\n\n    def get(self, fileobj, default=None):\n        fd = self._selector._fileobj_lookup(fileobj)\n        return self._selector._fd_to_key.get(fd, default)\n\n    def __getitem__(self, fileobj):\n        fd = self._selector._fileobj_lookup(fileobj)\n        key = self._selector._fd_to_key.get(fd)\n        if key is None:\n            raise KeyError(\"{!r} is not registered\".format(fileobj))\n        return key\n\n    def __iter__(self):\n        return iter(self._selector._fd_to_key)\n\n\nclass BaseSelector(metaclass=ABCMeta):\n    \"\"\"Selector abstract base class.\n\n    A selector supports registering file objects to be monitored for specific\n    I/O events.\n\n    A file object is a file descriptor or any object with a `fileno()` method.\n    An arbitrary object can be attached to the file object, which can be used\n    for example to store context information, a callback, etc.\n\n    A selector can use various implementations (select(), poll(), epoll()...)\n    depending on the platform. The default `Selector` class uses the most\n    efficient implementation on the current platform.\n    \"\"\"\n\n    @abstractmethod\n    def register(self, fileobj, events, data=None):\n        \"\"\"Register a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        ValueError if events is invalid\n        KeyError if fileobj is already registered\n        OSError if fileobj is closed or otherwise is unacceptable to\n                the underlying system call (if a system call is made)\n\n        Note:\n        OSError may or may not be raised\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def unregister(self, fileobj):\n        \"\"\"Unregister a file object.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        KeyError if fileobj is not registered\n\n        Note:\n        If fileobj is registered but has since been closed this does\n        *not* raise OSError (even if the wrapped syscall does)\n        \"\"\"\n        raise NotImplementedError\n\n    def modify(self, fileobj, events, data=None):\n        \"\"\"Change a registered file object monitored events or attached data.\n\n        Parameters:\n        fileobj -- file object or file descriptor\n        events  -- events to monitor (bitwise mask of EVENT_READ|EVENT_WRITE)\n        data    -- attached data\n\n        Returns:\n        SelectorKey instance\n\n        Raises:\n        Anything that unregister() or register() raises\n        \"\"\"\n        self.unregister(fileobj)\n        return self.register(fileobj, events, data)\n\n    @abstractmethod\n    def select(self, timeout=None):\n        \"\"\"Perform the actual selection, until some monitored file objects are\n        ready or a timeout expires.\n\n        Parameters:\n        timeout -- if timeout > 0, this specifies the maximum wait time, in\n                   seconds\n                   if timeout <= 0, the select() call won't block, and will\n                   report the currently ready file objects\n                   if timeout is None, select() will block until a monitored\n                   file object becomes ready\n\n        Returns:\n        list of (key, events) for ready file objects\n        `events` is a bitwise mask of EVENT_READ|EVENT_WRITE\n        \"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the selector.\n\n        This must be called to make sure that any underlying resource is freed.\n        \"\"\"\n        pass\n\n    def get_key(self, fileobj):\n        \"\"\"Return the key associated to a registered file object.\n\n        Returns:\n        SelectorKey for this file object\n        \"\"\"\n        mapping = self.get_map()\n        if mapping is None:\n            raise RuntimeError('Selector is closed')\n        try:\n            return mapping[fileobj]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n\n    @abstractmethod\n    def get_map(self):\n        \"\"\"Return a mapping of file objects to selector keys.\"\"\"\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n\nclass _BaseSelectorImpl(BaseSelector):\n    \"\"\"Base selector implementation.\"\"\"\n\n    def __init__(self):\n        # this maps file descriptors to keys\n        self._fd_to_key = {}\n        # read-only mapping returned by get_map()\n        self._map = _SelectorMapping(self)\n\n    def _fileobj_lookup(self, fileobj):\n        \"\"\"Return a file descriptor from a file object.\n\n        This wraps _fileobj_to_fd() to do an exhaustive search in case\n        the object is invalid but we still have it in our map.  This\n        is used by unregister() so we can unregister an object that\n        was previously registered even if it is closed.  It is also\n        used by _SelectorMapping.\n        \"\"\"\n        try:\n            return _fileobj_to_fd(fileobj)\n        except ValueError:\n            # Do an exhaustive search.\n            for key in self._fd_to_key.values():\n                if key.fileobj is fileobj:\n                    return key.fd\n            # Raise ValueError after all.\n            raise\n\n    def register(self, fileobj, events, data=None):\n        if (not events) or (events & ~(EVENT_READ | EVENT_WRITE)):\n            raise ValueError(\"Invalid events: {!r}\".format(events))\n\n        key = SelectorKey(fileobj, self._fileobj_lookup(fileobj), events, data)\n\n        if key.fd in self._fd_to_key:\n            raise KeyError(\"{!r} (FD {}) is already registered\"\n                           .format(fileobj, key.fd))\n\n        self._fd_to_key[key.fd] = key\n        return key\n\n    def unregister(self, fileobj):\n        try:\n            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(\"{!r} is not registered\".format(fileobj)) from None\n        if events != key.events:\n            self.unregister(fileobj)\n            key = self.register(fileobj, events, data)\n        elif data != key.data:\n            # Use a shortcut to update the data.\n            key = key._replace(data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def close(self):\n        self._fd_to_key.clear()\n        self._map = None\n\n    def get_map(self):\n        return self._map\n\n\n\nclass SelectSelector(_BaseSelectorImpl):\n    \"\"\"Select-based selector.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._readers = set()\n        self._writers = set()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        if events & EVENT_READ:\n            self._readers.add(key.fd)\n        if events & EVENT_WRITE:\n            self._writers.add(key.fd)\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        self._readers.discard(key.fd)\n        self._writers.discard(key.fd)\n        return key\n\n    if sys.platform == 'win32':\n        def _select(self, r, w, _, timeout=None):\n            r, w, x = select.select(r, w, w, timeout)\n            return r, w + x, []\n    else:\n        _select = select.select\n\n    def select(self, timeout=None):\n        timeout = None if timeout is None else max(timeout, 0)\n        ready = []\n        try:\n            r, w, _ = self._select(self._readers, self._writers, [], timeout)\n        except InterruptedError:\n            return ready\n        r = frozenset(r)\n        w = frozenset(w)\n        rw = r | w\n        fd_to_key_get = self._fd_to_key.get\n        for fd in rw:\n            key = fd_to_key_get(fd)\n            if key:\n                events = ((fd in r and EVENT_READ)\n                          | (fd in w and EVENT_WRITE))\n                ready.append((key, events & key.events))\n        return ready\n\n\nclass _PollLikeSelector(_BaseSelectorImpl):\n    \"\"\"Base class shared between poll, epoll and devpoll selectors.\"\"\"\n    _selector_cls = None\n    _EVENT_READ = None\n    _EVENT_WRITE = None\n\n    def __init__(self):\n        super().__init__()\n        self._selector = self._selector_cls()\n\n    def register(self, fileobj, events, data=None):\n        key = super().register(fileobj, events, data)\n        poller_events = ((events & EVENT_READ and self._EVENT_READ)\n                         | (events & EVENT_WRITE and self._EVENT_WRITE) )\n        try:\n            self._selector.register(key.fd, poller_events)\n        except:\n            super().unregister(fileobj)\n            raise\n        return key\n\n    def unregister(self, fileobj):\n        key = super().unregister(fileobj)\n        try:\n            self._selector.unregister(key.fd)\n        except OSError:\n            # This can happen if the FD was closed since it\n            # was registered.\n            pass\n        return key\n\n    def modify(self, fileobj, events, data=None):\n        try:\n            key = self._fd_to_key[self._fileobj_lookup(fileobj)]\n        except KeyError:\n            raise KeyError(f\"{fileobj!r} is not registered\") from None\n\n        changed = False\n        if events != key.events:\n            selector_events = ((events & EVENT_READ and self._EVENT_READ)\n                               | (events & EVENT_WRITE and self._EVENT_WRITE))\n            try:\n                self._selector.modify(key.fd, selector_events)\n            except:\n                super().unregister(fileobj)\n                raise\n            changed = True\n        if data != key.data:\n            changed = True\n\n        if changed:\n            key = key._replace(events=events, data=data)\n            self._fd_to_key[key.fd] = key\n        return key\n\n    def select(self, timeout=None):\n        # This is shared between poll() and epoll().\n        # epoll() has a different signature and handling of timeout parameter.\n        if timeout is None:\n            timeout = None\n        elif timeout <= 0:\n            timeout = 0\n        else:\n            # poll() has a resolution of 1 millisecond, round away from\n            # zero to wait *at least* timeout seconds.\n            timeout = math.ceil(timeout * 1e3)\n        ready = []\n        try:\n            fd_event_list = self._selector.poll(timeout)\n        except InterruptedError:\n            return ready\n\n        fd_to_key_get = self._fd_to_key.get\n        for fd, event in fd_event_list:\n            key = fd_to_key_get(fd)\n            if key:\n                events = ((event & ~self._EVENT_READ and EVENT_WRITE)\n                           | (event & ~self._EVENT_WRITE and EVENT_READ))\n                ready.append((key, events & key.events))\n        return ready\n\n\nif hasattr(select, 'poll'):\n\n    class PollSelector(_PollLikeSelector):\n        \"\"\"Poll-based selector.\"\"\"\n        _selector_cls = select.poll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n\nif hasattr(select, 'epoll'):\n\n    _NOT_EPOLLIN = ~select.EPOLLIN\n    _NOT_EPOLLOUT = ~select.EPOLLOUT\n\n    class EpollSelector(_PollLikeSelector):\n        \"\"\"Epoll-based selector.\"\"\"\n        _selector_cls = select.epoll\n        _EVENT_READ = select.EPOLLIN\n        _EVENT_WRITE = select.EPOLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def select(self, timeout=None):\n            if timeout is None:\n                timeout = -1\n            elif timeout <= 0:\n                timeout = 0\n            else:\n                # epoll_wait() has a resolution of 1 millisecond, round away\n                # from zero to wait *at least* timeout seconds.\n                timeout = math.ceil(timeout * 1e3) * 1e-3\n\n            # epoll_wait() expects `maxevents` to be greater than zero;\n            # we want to make sure that `select()` can be called when no\n            # FD is registered.\n            max_ev = len(self._fd_to_key) or 1\n\n            ready = []\n            try:\n                fd_event_list = self._selector.poll(timeout, max_ev)\n            except InterruptedError:\n                return ready\n\n            fd_to_key = self._fd_to_key\n            for fd, event in fd_event_list:\n                key = fd_to_key.get(fd)\n                if key:\n                    events = ((event & _NOT_EPOLLIN and EVENT_WRITE)\n                              | (event & _NOT_EPOLLOUT and EVENT_READ))\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'devpoll'):\n\n    class DevpollSelector(_PollLikeSelector):\n        \"\"\"Solaris /dev/poll selector.\"\"\"\n        _selector_cls = select.devpoll\n        _EVENT_READ = select.POLLIN\n        _EVENT_WRITE = select.POLLOUT\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\nif hasattr(select, 'kqueue'):\n\n    class KqueueSelector(_BaseSelectorImpl):\n        \"\"\"Kqueue-based selector.\"\"\"\n\n        def __init__(self):\n            super().__init__()\n            self._selector = select.kqueue()\n            self._max_events = 0\n\n        def fileno(self):\n            return self._selector.fileno()\n\n        def register(self, fileobj, events, data=None):\n            key = super().register(fileobj, events, data)\n            try:\n                if events & EVENT_READ:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                    self._max_events += 1\n                if events & EVENT_WRITE:\n                    kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                        select.KQ_EV_ADD)\n                    self._selector.control([kev], 0, 0)\n                    self._max_events += 1\n            except:\n                super().unregister(fileobj)\n                raise\n            return key\n\n        def unregister(self, fileobj):\n            key = super().unregister(fileobj)\n            if key.events & EVENT_READ:\n                kev = select.kevent(key.fd, select.KQ_FILTER_READ,\n                                    select.KQ_EV_DELETE)\n                self._max_events -= 1\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # This can happen if the FD was closed since it\n                    # was registered.\n                    pass\n            if key.events & EVENT_WRITE:\n                kev = select.kevent(key.fd, select.KQ_FILTER_WRITE,\n                                    select.KQ_EV_DELETE)\n                self._max_events -= 1\n                try:\n                    self._selector.control([kev], 0, 0)\n                except OSError:\n                    # See comment above.\n                    pass\n            return key\n\n        def select(self, timeout=None):\n            timeout = None if timeout is None else max(timeout, 0)\n            # If max_ev is 0, kqueue will ignore the timeout. For consistent\n            # behavior with the other selector classes, we prevent that here\n            # (using max). See https://bugs.python.org/issue29255\n            max_ev = self._max_events or 1\n            ready = []\n            try:\n                kev_list = self._selector.control(None, max_ev, timeout)\n            except InterruptedError:\n                return ready\n\n            fd_to_key_get = self._fd_to_key.get\n            for kev in kev_list:\n                fd = kev.ident\n                flag = kev.filter\n                key = fd_to_key_get(fd)\n                if key:\n                    events = ((flag == select.KQ_FILTER_READ and EVENT_READ)\n                              | (flag == select.KQ_FILTER_WRITE and EVENT_WRITE))\n                    ready.append((key, events & key.events))\n            return ready\n\n        def close(self):\n            self._selector.close()\n            super().close()\n\n\ndef _can_use(method):\n    \"\"\"Check if we can use the selector depending upon the\n    operating system. \"\"\"\n    # Implementation based upon https://github.com/sethmlarson/selectors2/blob/master/selectors2.py\n    selector = getattr(select, method, None)\n    if selector is None:\n        # select module does not implement method\n        return False\n    # check if the OS and Kernel actually support the method. Call may fail with\n    # OSError: [Errno 38] Function not implemented\n    try:\n        selector_obj = selector()\n        if method == 'poll':\n            # check that poll actually works\n            selector_obj.poll(0)\n        else:\n            # close epoll, kqueue, and devpoll fd\n            selector_obj.close()\n        return True\n    except OSError:\n        return False\n\n\n# Choose the best implementation, roughly:\n#    epoll|kqueue|devpoll > poll > select.\n# select() also can't accept a FD > FD_SETSIZE (usually around 1024)\nif _can_use('kqueue'):\n    DefaultSelector = KqueueSelector\nelif _can_use('epoll'):\n    DefaultSelector = EpollSelector\nelif _can_use('devpoll'):\n    DefaultSelector = DevpollSelector\nelif _can_use('poll'):\n    DefaultSelector = PollSelector\nelse:\n    DefaultSelector = SelectSelector\n", 603], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py": ["\"\"\"Event loop using a selector and related classes.\n\nA selector is a \"notify-when-ready\" multiplexer.  For a subclass which\nalso includes support for signal handling, see the unix_events sub-module.\n\"\"\"\n\n__all__ = 'BaseSelectorEventLoop',\n\nimport collections\nimport errno\nimport functools\nimport itertools\nimport os\nimport selectors\nimport socket\nimport warnings\nimport weakref\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import base_events\nfrom . import constants\nfrom . import events\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n_HAS_SENDMSG = hasattr(socket.socket, 'sendmsg')\n\nif _HAS_SENDMSG:\n    try:\n        SC_IOV_MAX = os.sysconf('SC_IOV_MAX')\n    except OSError:\n        # Fallback to send\n        _HAS_SENDMSG = False\n\ndef _test_selector_event(selector, fd, event):\n    # Test if the selector is monitoring 'event' events\n    # for the file descriptor 'fd'.\n    try:\n        key = selector.get_key(fd)\n    except KeyError:\n        return False\n    else:\n        return bool(key.events & event)\n\n\nclass BaseSelectorEventLoop(base_events.BaseEventLoop):\n    \"\"\"Selector event loop.\n\n    See events.EventLoop for API specification.\n    \"\"\"\n\n    def __init__(self, selector=None):\n        super().__init__()\n\n        if selector is None:\n            selector = selectors.DefaultSelector()\n        logger.debug('Using selector: %s', selector.__class__.__name__)\n        self._selector = selector\n        self._make_self_pipe()\n        self._transports = weakref.WeakValueDictionary()\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        self._ensure_fd_no_transport(sock)\n        return _SelectorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT,\n    ):\n        self._ensure_fd_no_transport(rawsock)\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            ssl_shutdown_timeout=ssl_shutdown_timeout\n        )\n        _SelectorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        self._ensure_fd_no_transport(sock)\n        return _SelectorDatagramTransport(self, sock, protocol,\n                                          address, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n        self._close_self_pipe()\n        super().close()\n        if self._selector is not None:\n            self._selector.close()\n            self._selector = None\n\n    def _close_self_pipe(self):\n        self._remove_reader(self._ssock.fileno())\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n        self._add_reader(self._ssock.fileno(), self._read_from_self)\n\n    def _process_self_data(self, data):\n        pass\n\n    def _read_from_self(self):\n        while True:\n            try:\n                data = self._ssock.recv(4096)\n                if not data:\n                    break\n                self._process_self_data(data)\n            except InterruptedError:\n                continue\n            except BlockingIOError:\n                break\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n                       ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        self._add_reader(sock.fileno(), self._accept_connection,\n                         protocol_factory, sock, sslcontext, server, backlog,\n                         ssl_handshake_timeout, ssl_shutdown_timeout)\n\n    def _accept_connection(\n            self, protocol_factory, sock,\n            sslcontext=None, server=None, backlog=100,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        # This method is only called once for each event loop tick where the\n        # listening socket has triggered an EVENT_READ. There may be multiple\n        # connections waiting for an .accept() so it is called in a loop.\n        # See https://bugs.python.org/issue27906 for more details.\n        for _ in range(backlog):\n            try:\n                conn, addr = sock.accept()\n                if self._debug:\n                    logger.debug(\"%r got a new connection from %r: %r\",\n                                 server, addr, conn)\n                conn.setblocking(False)\n            except (BlockingIOError, InterruptedError, ConnectionAbortedError):\n                # Early exit because the socket accept buffer is empty.\n                return None\n            except OSError as exc:\n                # There's nowhere to send the error, so just log it.\n                if exc.errno in (errno.EMFILE, errno.ENFILE,\n                                 errno.ENOBUFS, errno.ENOMEM):\n                    # Some platforms (e.g. Linux keep reporting the FD as\n                    # ready, so we remove the read handler temporarily.\n                    # We'll try again in a while.\n                    self.call_exception_handler({\n                        'message': 'socket.accept() out of system resource',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    self._remove_reader(sock.fileno())\n                    self.call_later(constants.ACCEPT_RETRY_DELAY,\n                                    self._start_serving,\n                                    protocol_factory, sock, sslcontext, server,\n                                    backlog, ssl_handshake_timeout,\n                                    ssl_shutdown_timeout)\n                else:\n                    raise  # The event loop will catch, log and ignore it.\n            else:\n                extra = {'peername': addr}\n                accept = self._accept_connection2(\n                    protocol_factory, conn, extra, sslcontext, server,\n                    ssl_handshake_timeout, ssl_shutdown_timeout)\n                self.create_task(accept)\n\n    async def _accept_connection2(\n            self, protocol_factory, conn, extra,\n            sslcontext=None, server=None,\n            ssl_handshake_timeout=constants.SSL_HANDSHAKE_TIMEOUT,\n            ssl_shutdown_timeout=constants.SSL_SHUTDOWN_TIMEOUT):\n        protocol = None\n        transport = None\n        try:\n            protocol = protocol_factory()\n            waiter = self.create_future()\n            if sslcontext:\n                transport = self._make_ssl_transport(\n                    conn, protocol, sslcontext, waiter=waiter,\n                    server_side=True, extra=extra, server=server,\n                    ssl_handshake_timeout=ssl_handshake_timeout,\n                    ssl_shutdown_timeout=ssl_shutdown_timeout)\n            else:\n                transport = self._make_socket_transport(\n                    conn, protocol, waiter=waiter, extra=extra,\n                    server=server)\n\n            try:\n                await waiter\n            except BaseException:\n                transport.close()\n                # gh-109534: When an exception is raised by the SSLProtocol object the\n                # exception set in this future can keep the protocol object alive and\n                # cause a reference cycle.\n                waiter = None\n                raise\n                # It's now up to the protocol to handle the connection.\n\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if self._debug:\n                context = {\n                    'message':\n                        'Error on transport creation for incoming connection',\n                    'exception': exc,\n                }\n                if protocol is not None:\n                    context['protocol'] = protocol\n                if transport is not None:\n                    context['transport'] = transport\n                self.call_exception_handler(context)\n\n    def _ensure_fd_no_transport(self, fd):\n        fileno = fd\n        if not isinstance(fileno, int):\n            try:\n                fileno = int(fileno.fileno())\n            except (AttributeError, TypeError, ValueError):\n                # This code matches selectors._fileobj_to_fd function.\n                raise ValueError(f\"Invalid file object: {fd!r}\") from None\n        transport = self._transports.get(fileno)\n        if transport and not transport.is_closing():\n            raise RuntimeError(\n                f'File descriptor {fd!r} is used by transport '\n                f'{transport!r}')\n\n    def _add_reader(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            self._selector.register(fd, selectors.EVENT_READ,\n                                    (handle, None))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_READ,\n                                  (handle, writer))\n            if reader is not None:\n                reader.cancel()\n        return handle\n\n    def _remove_reader(self, fd):\n        if self.is_closed():\n            return False\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            return False\n        mask, (reader, writer) = key.events, key.data\n        mask &= ~selectors.EVENT_READ\n        if not mask:\n            self._selector.unregister(fd)\n        else:\n            self._selector.modify(fd, mask, (None, writer))\n\n        if reader is not None:\n            reader.cancel()\n            return True\n        else:\n            return False\n\n    def _add_writer(self, fd, callback, *args):\n        self._check_closed()\n        handle = events.Handle(callback, args, self, None)\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            self._selector.register(fd, selectors.EVENT_WRITE,\n                                    (None, handle))\n        else:\n            mask, (reader, writer) = key.events, key.data\n            self._selector.modify(fd, mask | selectors.EVENT_WRITE,\n                                  (reader, handle))\n            if writer is not None:\n                writer.cancel()\n        return handle\n\n    def _remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        if self.is_closed():\n            return False\n        key = self._selector.get_map().get(fd)\n        if key is None:\n            return False\n        mask, (reader, writer) = key.events, key.data\n        # Remove both writer and connector.\n        mask &= ~selectors.EVENT_WRITE\n        if not mask:\n            self._selector.unregister(fd)\n        else:\n            self._selector.modify(fd, mask, (reader, None))\n\n        if writer is not None:\n            writer.cancel()\n            return True\n        else:\n            return False\n\n    def add_reader(self, fd, callback, *args):\n        \"\"\"Add a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_reader(fd, callback, *args)\n\n    def remove_reader(self, fd):\n        \"\"\"Remove a reader callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_reader(fd)\n\n    def add_writer(self, fd, callback, *args):\n        \"\"\"Add a writer callback..\"\"\"\n        self._ensure_fd_no_transport(fd)\n        self._add_writer(fd, callback, *args)\n\n    def remove_writer(self, fd):\n        \"\"\"Remove a writer callback.\"\"\"\n        self._ensure_fd_no_transport(fd)\n        return self._remove_writer(fd)\n\n    async def sock_recv(self, sock, n):\n        \"\"\"Receive data from the socket.\n\n        The return value is a bytes object representing the data received.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv, fut, sock, n)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_read_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_reader(fd)\n\n    def _sock_recv(self, fut, sock, n):\n        # _sock_recv() can add itself as an I/O callback if the operation can't\n        # be done immediately. Don't use it directly, call sock_recv().\n        if fut.done():\n            return\n        try:\n            data = sock.recv(n)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(data)\n\n    async def sock_recv_into(self, sock, buf):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is the number of bytes written.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recv_into, fut, sock, buf)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recv_into(self, fut, sock, buf):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            nbytes = sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(nbytes)\n\n    async def sock_recvfrom(self, sock, bufsize):\n        \"\"\"Receive a datagram from a datagram socket.\n\n        The return value is a tuple of (bytes, address) representing the\n        datagram received and the address it came from.\n        The maximum amount of data to be received at once is specified by\n        nbytes.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom, fut, sock, bufsize)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom(self, fut, sock, bufsize):\n        # _sock_recvfrom() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recvfrom().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom(bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_recvfrom_into(self, sock, buf, nbytes=0):\n        \"\"\"Receive data from the socket.\n\n        The received data is written into *buf* (a writable buffer).\n        The return value is a tuple of (number of bytes written, address).\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        if not nbytes:\n            nbytes = len(buf)\n\n        try:\n            return sock.recvfrom_into(buf, nbytes)\n        except (BlockingIOError, InterruptedError):\n            pass\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        handle = self._add_reader(fd, self._sock_recvfrom_into, fut, sock, buf,\n                                  nbytes)\n        fut.add_done_callback(\n            functools.partial(self._sock_read_done, fd, handle=handle))\n        return await fut\n\n    def _sock_recvfrom_into(self, fut, sock, buf, bufsize):\n        # _sock_recv_into() can add itself as an I/O callback if the operation\n        # can't be done immediately. Don't use it directly, call\n        # sock_recv_into().\n        if fut.done():\n            return\n        try:\n            result = sock.recvfrom_into(buf, bufsize)\n        except (BlockingIOError, InterruptedError):\n            return  # try again next time\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(result)\n\n    async def sock_sendall(self, sock, data):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            n = sock.send(data)\n        except (BlockingIOError, InterruptedError):\n            n = 0\n\n        if n == len(data):\n            # all data sent\n            return\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendall, fut, sock,\n                                  memoryview(data), [n])\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendall(self, fut, sock, view, pos):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        start = pos[0]\n        try:\n            n = sock.send(view[start:])\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n            return\n\n        start += n\n\n        if start == len(view):\n            fut.set_result(None)\n        else:\n            pos[0] = start\n\n    async def sock_sendto(self, sock, data, address):\n        \"\"\"Send data to the socket.\n\n        The socket must be connected to a remote socket. This method continues\n        to send data from data until either all data has been sent or an\n        error occurs. None is returned on success. On error, an exception is\n        raised, and there is no way to determine how much data, if any, was\n        successfully processed by the receiving end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        try:\n            return sock.sendto(data, address)\n        except (BlockingIOError, InterruptedError):\n            pass\n\n        fut = self.create_future()\n        fd = sock.fileno()\n        self._ensure_fd_no_transport(fd)\n        # use a trick with a list in closure to store a mutable state\n        handle = self._add_writer(fd, self._sock_sendto, fut, sock, data,\n                                  address)\n        fut.add_done_callback(\n            functools.partial(self._sock_write_done, fd, handle=handle))\n        return await fut\n\n    def _sock_sendto(self, fut, sock, data, address):\n        if fut.done():\n            # Future cancellation can be scheduled on previous loop iteration\n            return\n        try:\n            n = sock.sendto(data, 0, address)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(n)\n\n    async def sock_connect(self, sock, address):\n        \"\"\"Connect to a remote socket at address.\n\n        This method is a coroutine.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n\n        if sock.family == socket.AF_INET or (\n                base_events._HAS_IPv6 and sock.family == socket.AF_INET6):\n            resolved = await self._ensure_resolved(\n                address, family=sock.family, type=sock.type, proto=sock.proto,\n                loop=self,\n            )\n            _, _, _, _, address = resolved[0]\n\n        fut = self.create_future()\n        self._sock_connect(fut, sock, address)\n        try:\n            return await fut\n        finally:\n            # Needed to break cycles when an exception occurs.\n            fut = None\n\n    def _sock_connect(self, fut, sock, address):\n        fd = sock.fileno()\n        try:\n            sock.connect(address)\n        except (BlockingIOError, InterruptedError):\n            # Issue #23618: When the C function connect() fails with EINTR, the\n            # connection runs in background. We have to wait until the socket\n            # becomes writable to be notified when the connection succeed or\n            # fails.\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_writer(\n                fd, self._sock_connect_cb, fut, sock, address)\n            fut.add_done_callback(\n                functools.partial(self._sock_write_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    def _sock_write_done(self, fd, fut, handle=None):\n        if handle is None or not handle.cancelled():\n            self.remove_writer(fd)\n\n    def _sock_connect_cb(self, fut, sock, address):\n        if fut.done():\n            return\n\n        try:\n            err = sock.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)\n            if err != 0:\n                # Jump to any except clause below.\n                raise OSError(err, f'Connect call failed {address}')\n        except (BlockingIOError, InterruptedError):\n            # socket is still registered, the callback will be retried later\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result(None)\n        finally:\n            fut = None\n\n    async def sock_accept(self, sock):\n        \"\"\"Accept a connection.\n\n        The socket must be bound to an address and listening for connections.\n        The return value is a pair (conn, address) where conn is a new socket\n        object usable to send and receive data on the connection, and address\n        is the address bound to the socket on the other end of the connection.\n        \"\"\"\n        base_events._check_ssl_socket(sock)\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        fut = self.create_future()\n        self._sock_accept(fut, sock)\n        return await fut\n\n    def _sock_accept(self, fut, sock):\n        fd = sock.fileno()\n        try:\n            conn, address = sock.accept()\n            conn.setblocking(False)\n        except (BlockingIOError, InterruptedError):\n            self._ensure_fd_no_transport(fd)\n            handle = self._add_reader(fd, self._sock_accept, fut, sock)\n            fut.add_done_callback(\n                functools.partial(self._sock_read_done, fd, handle=handle))\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            fut.set_exception(exc)\n        else:\n            fut.set_result((conn, address))\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        del self._transports[transp._sock_fd]\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n            self._transports[transp._sock_fd] = transp\n\n    def _process_events(self, event_list):\n        for key, mask in event_list:\n            fileobj, (reader, writer) = key.fileobj, key.data\n            if mask & selectors.EVENT_READ and reader is not None:\n                if reader._cancelled:\n                    self._remove_reader(fileobj)\n                else:\n                    self._add_callback(reader)\n            if mask & selectors.EVENT_WRITE and writer is not None:\n                if writer._cancelled:\n                    self._remove_writer(fileobj)\n                else:\n                    self._add_callback(writer)\n\n    def _stop_serving(self, sock):\n        self._remove_reader(sock.fileno())\n        sock.close()\n\n\nclass _SelectorTransport(transports._FlowControlMixin,\n                         transports.Transport):\n\n    max_size = 256 * 1024  # Buffer size passed to recv().\n\n    # Attribute used in the destructor: it must be set even if the constructor\n    # is not called (see _SelectorSslTransport which may start by raising an\n    # exception)\n    _sock = None\n\n    def __init__(self, loop, sock, protocol, extra=None, server=None):\n        super().__init__(extra, loop)\n        self._extra['socket'] = trsock.TransportSocket(sock)\n        try:\n            self._extra['sockname'] = sock.getsockname()\n        except OSError:\n            self._extra['sockname'] = None\n        if 'peername' not in self._extra:\n            try:\n                self._extra['peername'] = sock.getpeername()\n            except socket.error:\n                self._extra['peername'] = None\n        self._sock = sock\n        self._sock_fd = sock.fileno()\n\n        self._protocol_connected = False\n        self.set_protocol(protocol)\n\n        self._server = server\n        self._buffer = collections.deque()\n        self._conn_lost = 0  # Set when call to connection_lost scheduled.\n        self._closing = False  # Set when close() called.\n        self._paused = False  # Set when pause_reading() called\n\n        if self._server is not None:\n            self._server._attach(self)\n        loop._transports[self._sock_fd] = self\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        info.append(f'fd={self._sock_fd}')\n        # test if the transport was closed\n        if self._loop is not None and not self._loop.is_closed():\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd, selectors.EVENT_READ)\n            if polling:\n                info.append('read=polling')\n            else:\n                info.append('read=idle')\n\n            polling = _test_selector_event(self._loop._selector,\n                                           self._sock_fd,\n                                           selectors.EVENT_WRITE)\n            if polling:\n                state = 'polling'\n            else:\n                state = 'idle'\n\n            bufsize = self.get_write_buffer_size()\n            info.append(f'write=<{state}, bufsize={bufsize}>')\n        return '<{}>'.format(' '.join(info))\n\n    def abort(self):\n        self._force_close(None)\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n        self._protocol_connected = True\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def is_reading(self):\n        return not self.is_closing() and not self._paused\n\n    def pause_reading(self):\n        if not self.is_reading():\n            return\n        self._paused = True\n        self._loop._remove_reader(self._sock_fd)\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n        self._paused = False\n        self._add_reader(self._sock_fd, self._read_ready)\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._loop._remove_reader(self._sock_fd)\n        if not self._buffer:\n            self._conn_lost += 1\n            self._loop._remove_writer(self._sock_fd)\n            self._loop.call_soon(self._call_connection_lost, None)\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n            if self._server is not None:\n                self._server._detach(self)\n\n    def _fatal_error(self, exc, message='Fatal error on transport'):\n        # Should be called from exception handler only.\n        if isinstance(exc, OSError):\n            if self._loop.get_debug():\n                logger.debug(\"%r: %s\", self, message, exc_info=True)\n        else:\n            self._loop.call_exception_handler({\n                'message': message,\n                'exception': exc,\n                'transport': self,\n                'protocol': self._protocol,\n            })\n        self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._conn_lost:\n            return\n        if self._buffer:\n            self._buffer.clear()\n            self._loop._remove_writer(self._sock_fd)\n        if not self._closing:\n            self._closing = True\n            self._loop._remove_reader(self._sock_fd)\n        self._conn_lost += 1\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        try:\n            if self._protocol_connected:\n                self._protocol.connection_lost(exc)\n        finally:\n            self._sock.close()\n            self._sock = None\n            self._protocol = None\n            self._loop = None\n            server = self._server\n            if server is not None:\n                server._detach(self)\n                self._server = None\n\n    def get_write_buffer_size(self):\n        return sum(map(len, self._buffer))\n\n    def _add_reader(self, fd, callback, *args):\n        if not self.is_reading():\n            return\n        self._loop._add_reader(fd, callback, *args)\n\n\nclass _SelectorSocketTransport(_SelectorTransport):\n\n    _start_tls_compatible = True\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n\n        self._read_ready_cb = None\n        super().__init__(loop, sock, protocol, extra, server)\n        self._eof = False\n        self._empty_waiter = None\n        if _HAS_SENDMSG:\n            self._write_ready = self._write_sendmsg\n        else:\n            self._write_ready = self._write_send\n        # Disable the Nagle algorithm -- small writes will be\n        # sent without waiting for the TCP ACK.  This generally\n        # decreases the latency (in some cases significantly.)\n        base_events._set_nodelay(self._sock)\n\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def set_protocol(self, protocol):\n        if isinstance(protocol, protocols.BufferedProtocol):\n            self._read_ready_cb = self._read_ready__get_buffer\n        else:\n            self._read_ready_cb = self._read_ready__data_received\n\n        super().set_protocol(protocol)\n\n    def _read_ready(self):\n        self._read_ready_cb()\n\n    def _read_ready__get_buffer(self):\n        if self._conn_lost:\n            return\n\n        try:\n            buf = self._protocol.get_buffer(-1)\n            if not len(buf):\n                raise RuntimeError('get_buffer() returned an empty buffer')\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.get_buffer() call failed.')\n            return\n\n        try:\n            nbytes = self._sock.recv_into(buf)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not nbytes:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.buffer_updated(nbytes)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.buffer_updated() call failed.')\n\n    def _read_ready__data_received(self):\n        if self._conn_lost:\n            return\n        try:\n            data = self._sock.recv(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on socket transport')\n            return\n\n        if not data:\n            self._read_ready__on_eof()\n            return\n\n        try:\n            self._protocol.data_received(data)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.data_received() call failed.')\n\n    def _read_ready__on_eof(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if keep_open:\n            # We're keeping the connection open so the\n            # protocol can write more, but we still can't\n            # receive more, so remove the reader callback.\n            self._loop._remove_reader(self._sock_fd)\n        else:\n            self.close()\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n        if self._eof:\n            raise RuntimeError('Cannot call write() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Optimization: try to send now.\n            try:\n                n = self._sock.send(data)\n            except (BlockingIOError, InterruptedError):\n                pass\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc, 'Fatal write error on socket transport')\n                return\n            else:\n                data = memoryview(data)[n:]\n                if not data:\n                    return\n            # Not all was written; register write handler.\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n        # Add it to the buffer.\n        self._buffer.append(data)\n        self._maybe_pause_protocol()\n\n    def _get_sendmsg_buffer(self):\n        return itertools.islice(self._buffer, SC_IOV_MAX)\n\n    def _write_sendmsg(self):\n        assert self._buffer, 'Data should not be empty'\n        if self._conn_lost:\n            return\n        try:\n            nbytes = self._sock.sendmsg(self._get_sendmsg_buffer())\n            self._adjust_leftover_buffer(nbytes)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def _adjust_leftover_buffer(self, nbytes: int) -> None:\n        buffer = self._buffer\n        while nbytes:\n            b = buffer.popleft()\n            b_len = len(b)\n            if b_len <= nbytes:\n                nbytes -= b_len\n            else:\n                buffer.appendleft(b[nbytes:])\n                break\n\n    def _write_send(self):\n        assert self._buffer, 'Data should not be empty'\n        if self._conn_lost:\n            return\n        try:\n            buffer = self._buffer.popleft()\n            n = self._sock.send(buffer)\n            if n != len(buffer):\n                # Not all data was written\n                self._buffer.appendleft(buffer[n:])\n        except (BlockingIOError, InterruptedError):\n            pass\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._loop._remove_writer(self._sock_fd)\n            self._buffer.clear()\n            self._fatal_error(exc, 'Fatal write error on socket transport')\n            if self._empty_waiter is not None:\n                self._empty_waiter.set_exception(exc)\n        else:\n            self._maybe_resume_protocol()  # May append to buffer.\n            if not self._buffer:\n                self._loop._remove_writer(self._sock_fd)\n                if self._empty_waiter is not None:\n                    self._empty_waiter.set_result(None)\n                if self._closing:\n                    self._call_connection_lost(None)\n                elif self._eof:\n                    self._sock.shutdown(socket.SHUT_WR)\n\n    def write_eof(self):\n        if self._closing or self._eof:\n            return\n        self._eof = True\n        if not self._buffer:\n            self._sock.shutdown(socket.SHUT_WR)\n\n    def writelines(self, list_of_data):\n        if self._eof:\n            raise RuntimeError('Cannot call writelines() after write_eof()')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to writelines; sendfile is in progress')\n        if not list_of_data:\n            return\n        self._buffer.extend([memoryview(data) for data in list_of_data])\n        self._write_ready()\n        # If the entire buffer couldn't be written, register a write handler\n        if self._buffer:\n            self._loop._add_writer(self._sock_fd, self._write_ready)\n\n    def can_write_eof(self):\n        return True\n\n    def _call_connection_lost(self, exc):\n        super()._call_connection_lost(exc)\n        if self._empty_waiter is not None:\n            self._empty_waiter.set_exception(\n                ConnectionError(\"Connection is closed by peer\"))\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if not self._buffer:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n    def close(self):\n        self._read_ready_cb = None\n        self._write_ready = None\n        super().close()\n\n\nclass _SelectorDatagramTransport(_SelectorTransport, transports.DatagramTransport):\n\n    _buffer_factory = collections.deque\n\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        super().__init__(loop, sock, protocol, extra)\n        self._address = address\n        self._buffer_size = 0\n        self._loop.call_soon(self._protocol.connection_made, self)\n        # only start reading when connection_made() has been called\n        self._loop.call_soon(self._add_reader,\n                             self._sock_fd, self._read_ready)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def get_write_buffer_size(self):\n        return self._buffer_size\n\n    def _read_ready(self):\n        if self._conn_lost:\n            return\n        try:\n            data, addr = self._sock.recvfrom(self.max_size)\n        except (BlockingIOError, InterruptedError):\n            pass\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(exc, 'Fatal read error on datagram transport')\n        else:\n            self._protocol.datagram_received(data, addr)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(f'data argument must be a bytes-like object, '\n                            f'not {type(data).__name__!r}')\n\n        if self._address:\n            if addr not in (None, self._address):\n                raise ValueError(\n                    f'Invalid address: must be None or {self._address}')\n            addr = self._address\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        if not self._buffer:\n            # Attempt to send it right away first.\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n                return\n            except (BlockingIOError, InterruptedError):\n                self._loop._add_writer(self._sock_fd, self._sendto_ready)\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n        self._buffer_size += len(data) + 8  # include header bytes\n        self._maybe_pause_protocol()\n\n    def _sendto_ready(self):\n        while self._buffer:\n            data, addr = self._buffer.popleft()\n            self._buffer_size -= len(data)\n            try:\n                if self._extra['peername']:\n                    self._sock.send(data)\n                else:\n                    self._sock.sendto(data, addr)\n            except (BlockingIOError, InterruptedError):\n                self._buffer.appendleft((data, addr))  # Try again later.\n                self._buffer_size += len(data)\n                break\n            except OSError as exc:\n                self._protocol.error_received(exc)\n                return\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(\n                    exc, 'Fatal write error on datagram transport')\n                return\n\n        self._maybe_resume_protocol()  # May append to buffer.\n        if not self._buffer:\n            self._loop._remove_writer(self._sock_fd)\n            if self._closing:\n                self._call_connection_lost(None)\n", 1311], "/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events.get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        return base_futures._future_repr(self)\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancelled_exc is not None:\n            exc = self._cancelled_exc\n            self._cancelled_exc = None\n            return exc\n\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if isinstance(exception, StopIteration):\n            new_exc = RuntimeError(\"StopIteration interacts badly with \"\n                                   \"generators and cannot be raised into a \"\n                                   \"Future\")\n            new_exc.__cause__ = exception\n            new_exc.__context__ = exception\n            exception = new_exc\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args).with_traceback(exc.__traceback__)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args).with_traceback(exc.__traceback__)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events.get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 427], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py": ["# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport inspect\nimport re\nfrom abc import ABC, abstractmethod\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom marimo._ast.cell import CellImpl, _is_coroutine\nfrom marimo._ast.variables import is_mangled_local, unmangle_local\nfrom marimo._entrypoints.registry import EntryPointRegistry\nfrom marimo._runtime.copy import (\n    CloneError,\n    ShallowCopy,\n    ZeroCopy,\n    shallow_copy,\n)\nfrom marimo._runtime.exceptions import (\n    MarimoMissingRefError,\n    MarimoNameError,\n    MarimoRuntimeException,\n)\nfrom marimo._runtime.primitives import (\n    CLONE_PRIMITIVES,\n    build_ref_predicate_for_primitives,\n    from_unclonable_module,\n    is_unclonable_type,\n)\n\nif TYPE_CHECKING:\n    from marimo._runtime.dataflow import DirectedGraph\n\n_EXECUTOR_REGISTRY = EntryPointRegistry[type[\"Executor\"]](\n    \"marimo.cell.executor\",\n)\n\n\ndef get_executor(\n    config: ExecutionConfig,\n    registry: EntryPointRegistry[type[Executor]] = _EXECUTOR_REGISTRY,\n) -> Executor:\n    \"\"\"Get a code executor based on the execution configuration.\"\"\"\n    executors = registry.get_all()\n\n    base: Executor = DefaultExecutor()\n    if config.is_strict:\n        base = StrictExecutor(base)\n\n    for executor in executors:\n        base = executor(base)\n    return base\n\n\n@dataclass\nclass ExecutionConfig:\n    \"\"\"Configuration for cell execution.\"\"\"\n\n    is_strict: bool = False\n\n\ndef _raise_name_error(\n    graph: Optional[DirectedGraph], name_error: NameError\n) -> None:\n    if graph is None:\n        raise MarimoRuntimeException from name_error\n    (missing_name,) = re.findall(r\"'([^']*)'\", str(name_error))\n    _, private_cell_id = unmangle_local(missing_name)\n    if missing_name in graph.definitions or private_cell_id:\n        raise MarimoRuntimeException from MarimoMissingRefError(\n            missing_name, name_error\n        )\n    raise MarimoRuntimeException from name_error\n\n\nclass Executor(ABC):\n    def __init__(self, base: Optional[Executor] = None) -> None:\n        self.base = base\n\n    @abstractmethod\n    def execute_cell(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: DirectedGraph,\n    ) -> Any:\n        pass\n\n    @abstractmethod\n    async def execute_cell_async(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: DirectedGraph,\n    ) -> Any:\n        pass\n\n\nclass DefaultExecutor(Executor):\n    async def execute_cell_async(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: Optional[DirectedGraph] = None,\n    ) -> Any:\n        if cell.body is None:\n            return None\n        assert cell.last_expr is not None\n        try:\n            if _is_coroutine(cell.body):\n                await eval(cell.body, glbls)\n            else:\n                exec(cell.body, glbls)\n\n            if _is_coroutine(cell.last_expr):\n                return await eval(cell.last_expr, glbls)\n            else:\n                return eval(cell.last_expr, glbls)\n        except NameError as e:\n            _raise_name_error(graph, e)\n        except (BaseException, Exception) as e:\n            # Raising from a BaseException will fold in the stacktrace prior\n            # to execution\n            raise MarimoRuntimeException from e\n\n    def execute_cell(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: Optional[DirectedGraph] = None,\n    ) -> Any:\n        try:\n            if cell.body is None:\n                return None\n            assert cell.last_expr is not None\n\n            exec(cell.body, glbls)\n            return eval(cell.last_expr, glbls)\n        except NameError as e:\n            _raise_name_error(graph, e)\n        except (BaseException, Exception) as e:\n            raise MarimoRuntimeException from e\n\n\nclass StrictExecutor(Executor):\n    async def execute_cell_async(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: DirectedGraph,\n    ) -> Any:\n        assert self.base is not None, \"Invalid executor composition.\"\n\n        # Manage globals and references, but refers to the default beyond that.\n        refs = graph.get_transitive_references(\n            cell.refs,\n            predicate=build_ref_predicate_for_primitives(\n                glbls, CLONE_PRIMITIVES\n            ),\n        )\n        backup = self._sanitize_inputs(cell, refs, glbls)\n        try:\n            response = await self.base.execute_cell_async(cell, glbls, graph)\n        finally:\n            # Restore globals from backup and backfill outputs\n            self._update_outputs(cell, glbls, backup)\n        return response\n\n    def execute_cell(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        graph: DirectedGraph,\n    ) -> Any:\n        assert self.base is not None, \"Invalid executor composition.\"\n\n        refs = graph.get_transitive_references(\n            cell.refs,\n            predicate=build_ref_predicate_for_primitives(\n                glbls, CLONE_PRIMITIVES\n            ),\n        )\n        backup = self._sanitize_inputs(cell, refs, glbls)\n        try:\n            response = self.base.execute_cell(cell, glbls, graph)\n        finally:\n            self._update_outputs(cell, glbls, backup)\n        return response\n\n    def _sanitize_inputs(\n        self,\n        cell: CellImpl,\n        refs: set[str],\n        glbls: dict[str, Any],\n    ) -> dict[str, Any]:\n        # Some attributes should remain global\n        lcls = {\n            key: glbls[key]\n            for key in [\n                \"_MicropipFinder\",\n                \"_MicropipLoader\",\n                \"__builtin__\",\n                \"__doc__\",\n                \"__file__\",\n                \"__marimo__\",\n                \"__name__\",\n                \"__package__\",\n                \"__loader__\",\n                \"__spec__\",\n                \"input\",\n            ]\n            if key in glbls\n        }\n\n        for ref in refs:\n            if ref in glbls:\n                if (\n                    isinstance(\n                        glbls[ref],\n                        (ZeroCopy),\n                    )\n                    or inspect.ismodule(glbls[ref])\n                    or inspect.isfunction(glbls[ref])\n                    or from_unclonable_module(glbls[ref])\n                    or is_unclonable_type(glbls[ref])\n                ):\n                    lcls[ref] = glbls[ref]\n                elif isinstance(glbls[ref], ShallowCopy):\n                    lcls[ref] = shallow_copy(glbls[ref])\n                else:\n                    try:\n                        lcls[ref] = deepcopy(glbls[ref])\n                    except TypeError as e:\n                        raise CloneError(\n                            f\"Could not clone reference `{ref}` of type \"\n                            f\"{getattr(glbls[ref], '__module__', '<module>')}. \"\n                            f\"{glbls[ref].__class__.__name__} \"\n                            \"try wrapping the object in a `zero_copy` \"\n                            \"call. If this is a common object type, consider \"\n                            \"making an issue on the marimo GitHub \"\n                            \"repository to never deepcopy.\"\n                        ) from e\n            elif ref not in glbls[\"__builtins__\"]:\n                if ref in cell.defs:\n                    raise MarimoNameError(\n                        f\"name `{ref}` is referenced before definition.\", ref\n                    )\n                raise MarimoMissingRefError(ref)\n\n        # NOTE: Execution expects the globals dictionary by memory reference,\n        # so we need to clear it and update it with the sanitized locals,\n        # returning a backup of the original globals for later restoration.\n        # This must be performed at the end of the function to ensure valid\n        # state in case of failure.\n        backup = {**glbls}\n        glbls.clear()\n        glbls.update(lcls)\n        return backup\n\n    def _update_outputs(\n        self,\n        cell: CellImpl,\n        glbls: dict[str, Any],\n        backup: dict[str, Any],\n    ) -> None:\n        # NOTE: After execution, restore global state and update outputs.\n        lcls = {**glbls}\n        glbls.clear()\n        glbls.update(backup)\n\n        defs = cell.defs\n        for df in defs:\n            if df in lcls:\n                # Overwrite will delete the reference.\n                # Weak copy holds on with references.\n                glbls[df] = lcls[df]\n            # Captures the case where a variable was previously defined by the\n            # cell but this most recent run did not define it. The value is now\n            # stale and needs to be flushed.\n            elif df in glbls:\n                del glbls[df]\n\n        # Flush all private variables from memory\n        for df in backup:\n            if is_mangled_local(df, cell.cell_id):\n                del glbls[df]\n\n        # Now repopulate all private variables.\n        for df in lcls:\n            if is_mangled_local(df, cell.cell_id):\n                glbls[df] = lcls[df]\n", 292], "/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/print_override.py": ["# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport functools\nimport threading\nfrom typing import Any\n\nfrom marimo._messaging.cell_output import CellOutput\nfrom marimo._messaging.ops import CellOp\nfrom marimo._runtime.context.types import (\n    ContextNotInitializedError,\n    get_context,\n)\nfrom marimo._runtime.threads import THREADS\n\n_original_print = print\n\n\n@functools.wraps(print)\ndef print_override(*args: Any, **kwargs: Any) -> None:\n    \"\"\"Override print to be aware of marimo threads.\n\n    When running marimo without mo.Threads, this just forwards to the built-in\n    print. When running with mo.Threads, it gets the threads cell ID and\n    forwards the print message to the appropriate cell.\n\n    This method is only necessary because the file descriptors for standard\n    out and standard in are currently redirected, and sys.stdout is not\n    always patched / is not aware of mo.Threads.\n    \"\"\"\n    tid = threading.get_ident()\n    if tid not in THREADS:\n        _original_print(*args, **kwargs)\n        return\n\n    try:\n        ctx = get_context()\n    except ContextNotInitializedError:\n        _original_print(*args, **kwargs)\n        return\n\n    execution_context = ctx.execution_context\n    if execution_context is None:\n        _original_print(*args, **kwargs)\n        return\n\n    cell_id = execution_context.cell_id\n\n    sep = kwargs.get(\"sep\", \" \")\n    end = kwargs.get(\"end\", \"\\n\")\n    msg = sep.join([str(arg) for arg in args]) + end\n\n    CellOp(\n        cell_id=cell_id,\n        console=CellOutput.stdout(msg),\n    ).broadcast(ctx.stream)\n", 56]}, "functions": {"_GeneratorContextManagerBase.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:108)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 108], "contextmanager.<locals>.helper (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:303)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 303], "_RedirectStream.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:397)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 397], "_RedirectStream.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:402)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 402], "capture_stdout (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/capture.py:16)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/capture.py", 16], "_GeneratorContextManager.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:136)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 136], "BaseEventLoop.get_debug (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:2045)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 2045], "BaseEventLoop.create_future (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:458)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 458], "BaseEventLoop.time (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:768)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 768], "BaseEventLoop._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:550)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 550], "Handle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:36)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py", 36], "TimerHandle.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:113)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py", 113], "BaseEventLoop.call_at (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:801)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 801], "BaseEventLoop.call_later (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:777)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 777], "sleep (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:703)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py", 703], "Condition._acquire_restore (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:315)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 315], "_add_output_to_buffer (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:56)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py", 56], "_can_merge_outputs (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:52)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py", 52], "Condition._is_owned (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:318)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 318], "Condition._release_save (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:312)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 312], "Condition.wait (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:327)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 327], "Condition.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:306)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 306], "CellOutput.<lambda> (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/cell_output.py:36)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/cell_output.py", 36], "CellOp.<lambda> (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:139)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py", 139], "CellOp.__post_init__ (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:141)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py", 141], "_is_dataclass_instance (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1326)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py", 1326], "fields.<locals>.<genexpr> (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1323)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py", 1323], "fields (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1308)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py", 1308], "_asdict_inner (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1362)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py", 1362], "Enum.__deepcopy__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py:1303)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py", 1303], "deepcopy (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/copy.py:119)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/copy.py", 119], "asdict (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py:1338)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/dataclasses.py", 1338], "cast (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/typing.py:2366)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/typing.py", 2366], "serialize (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:63)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py", 63], "Op.serialize (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:106)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py", 106], "_ConnectionBase._check_closed (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:135)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py", 135], "_ConnectionBase._check_writable (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:143)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py", 143], "ForkingPickler.__init__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py:38)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py", 38], "Enum.__reduce_ex__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py:1300)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/enum.py", 1300], "ForkingPickler.dumps (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py:48)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/reduction.py", 48], "Connection._send (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:381)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py", 381], "Connection._send_bytes (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:406)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py", 406], "_ConnectionBase.send (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:202)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py", 202], "ThreadSafeStream.write (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/streams.py:122)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/streams.py", 122], "Op.broadcast (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py:86)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/ops.py", 86], "_write_console_output (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py:33)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/console_output_worker.py", 33], "Condition.__enter__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py:303)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/threading.py", 303], "EpollSelector.select (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py:435)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/selectors.py", 435], "BaseSelectorEventLoop._process_events (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py:740)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/selector_events.py", 740], "BaseEventLoop._call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:848)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 848], "BaseEventLoop.call_soon (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:819)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 819], "_set_result_unless_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:312)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py", 312], "Handle._run (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:87)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py", 87], "BaseEventLoop._run_once (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1947)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 1947], "BaseEventLoop._timer_handle_cancelled (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1942)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", 1942], "Handle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:73)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py", 73], "TimerHandle.cancel (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py:157)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/asyncio/events.py", 157], "DefaultExecutor.execute_cell_async (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py:101)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_runtime/executor.py", 101], "print_override (/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/print_override.py:19)": ["/home/ajp/projects/EDUCATION/pycon-co-2025/.venv/lib/python3.13/site-packages/marimo/_messaging/print_override.py", 19], "_RedirectStream.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:407)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 407], "_GeneratorContextManager.__exit__ (/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py:145)": ["/home/ajp/.local/share/uv/python/cpython-3.13.0-linux-x86_64-gnu/lib/python3.13/contextlib.py", 145]}}}